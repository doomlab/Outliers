@article{Beckman1983,
author = {Beckman, R. J. and Cook, R. D.},
doi = {10.2307/1268548},
issn = {00401706},
journal = {Technometrics},
month = {may},
number = {2},
pages = {161},
title = {{[Outlier..........s]: Response}},
url = {http://www.jstor.org/stable/1268548?origin=crossref},
volume = {25},
year = {1983}
}
@article{Etz2016,
author = {Etz, Alexander and Vandekerckhove, Joachim},
doi = {10.1371/journal.pone.0149794},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {2},
pages = {1--12},
pmid = {26919473},
title = {{A Bayesian perspective on the reproducibility project: Psychology}},
url = {http://dx.doi.org/10.1371/journal.pone.0149794},
volume = {11},
year = {2016}
}
@article{Zimmerman1994,
author = {Zimmerman, Donald W.},
journal = {The Journal of General Psychology},
number = {4},
pages = {391--401},
title = {{A note on the influence of outliers on parametric and nonparametric tests}},
volume = {121},
year = {1994}
}
@article{Hodge2004,
author = {Hodge, Victoria J and Austin, Jim},
doi = {10.1007/s10462-004-4304-y},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
month = {oct},
number = {2},
pages = {85--126},
title = {{A survey of outlier detection methodologies}},
url = {http://link.springer.com/10.1007/s10462-004-4304-y},
volume = {22},
year = {2004}
}
@article{Alexander2012,
abstract = {Reproducibility is a defining feature of science. However, because of strong incentives for innovation and weak incentives for confirmation, direct replication is rarely practiced or published. The Reproducibility Project is an open, large-scale, collaborative effort to systematically examine the rate and predictors of reproducibility in psychological science. So far, 72 volunteer researchers from 41 institutions have organized to openly and transparently replicate studies published in three prominent psychological journals in 2008. Multiple methods will be used to evaluate the findings, calculate an empirical rate of replication, and investigate factors that predict reproducibility. Whatever the result, a better understanding of reproducibility will ultimately improve confidence in scientific methodology and findings.},
author = {Alexander, Anita and Barnett-Cowan, Michael and Bartmess, Elizabeth and Bosco, Frank A. and Brandt, Mark and Carp, Joshua and Chandler, Jesse J. and Clay, Russ and Cleary, Hayley and Cohn, Michael and Costantini, Giulio and Decoster, Jamie and Dunn, Elizabeth and Eggleston, Casey and Estel, Vivien and Farach, Frank J. and Feather, Jenelle and Fiedler, Susann and Field, James G. and Foster, Joshua D. and Frank, Michael and Frazier, Rebecca S. and Fuchs, Heather M. and Galak, Jeff and Galliani, Elisa Maria and Garcia, Sara and Giammanco, Elise M. and Gilbert, Elizabeth A. and Giner-Sorolla, Roger and Goellner, Lars and Goh, Jin X. and {Justin Goss}, R. and Graham, Jesse and Grange, James A. and Gray, Jeremy R. and Gripshover, Sarah and Hartshorne, Joshua and Hayes, Timothy B. and Jahn, Georg and Johnson, Kate and Johnston, William and Joy-Gaba, Jennifer A. and Lai, Calvin K. and Lakens, Daniel and Lane, Kristin and Lebel, Etienne P. and Lee, Minha and Lemm, Kristi and Mackinnon, Sean and May, Michael and Moore, Katherine and Motyl, Matt and Muller, Stephanie M. and Munafo, Marcus and Nosek, Brian A. and Olsson, Catherine and Paunesku, Dave and Perugini, Marco and Pitts, Michael and Ratliff, Kate and Renkewitz, Frank and Rutchick, Abraham M. and Sandstrom, Gillian and Saxe, Rebecca and Selterman, Dylan and Simpson, William and Smith, Colin Tucker and Spies, Jeffrey R. and Strohminger, Nina and Talhelm, Thomas and {Van‘T Veer}, Anna and Vianello, Michelangelo},
doi = {10.1177/1745691612462588},
isbn = {1745-6916},
issn = {17456924},
journal = {Perspectives on Psychological Science},
keywords = {Methodology,Open,Psychological science,Replication,Reproducibility},
number = {6},
pages = {657--660},
pmid = {26168127},
title = {{An open, large-scale, collaborative effort to estimate the reproducibility of psychological science}},
volume = {7},
year = {2012}
}
@article{Doyen2012,
abstract = {The perspective that behavior is often driven by unconscious determinants has become widespread in social psychology. Bargh, Chen, and Burrows' (1996) famous study, in which participants unwittingly exposed to the stereotype of age walked slower when exiting the laboratory, was instrumental in defining this perspective. Here, we present two experiments aimed at replicating the original study. Despite the use of automated timing methods and a larger sample, our first experiment failed to show priming. Our second experiment was aimed at manipulating the beliefs of the experimenters: Half were led to think that participants would walk slower when primed congruently, and the other half was led to expect the opposite. Strikingly, we obtained a walking speed effect, but only when experimenters believed participants would indeed walk slower. This suggests that both priming and experimenters' expectations are instrumental in explaining the walking speed effect. Further, debriefing was suggestive of awareness of the primes. We conclude that unconscious behavioral priming is real, while real, involves mechanisms different from those typically assumed to cause the effect.},
author = {Doyen, St{\'{e}}phane and Klein, Olivier and Pichon, Cora-Lise and Cleeremans, Axel},
doi = {10.1371/journal.pone.0029081},
editor = {Lauwereyns, Jan},
isbn = {1932-6203},
issn = {1932-6203},
journal = {PLoS ONE},
month = {jan},
number = {1},
pages = {e29081},
pmid = {22279526},
title = {{Behavioral priming: It's all in the mind, but whose mind?}},
url = {http://dx.plos.org/10.1371/journal.pone.0029081},
volume = {7},
year = {2012}
}
@article{Valentine2017,
author = {Valentine, Kathrene D and Buchanan, Erin M and Scofield, John E and Beauchamp, Marshall},
doi = {10.17605/osf.io/9hp7y},
keywords = {Bayes Factors,Observation Oriented Modeling,Psychology,Quantitative Psychology,Social Statistics,Social and Behavioral Sciences,evidence,evidentiary value,null hypothesis testing,p values,statistics},
publisher = {Open Science Framework},
title = {{Beyond p-values: Utilizing multiple estimates to evaluate evidence}},
url = {https://osf.io/9hp7y/},
year = {2017}
}
@article{Lakens2013,
author = {Lakens, Dani{\"{e}}l},
doi = {10.3389/fpsyg.2013.00863},
issn = {1664-1078},
journal = {Frontiers in Psychology},
title = {{Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and ANOVAs}},
url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00863/abstract},
volume = {4},
year = {2013}
}
@article{Cook1980,
author = {Cook, R. Dennis and Weisberg, Sanford},
doi = {10.2307/1268187},
journal = {Technometrics},
number = {1},
pages = {495--508},
title = {{Characterizations of an empirical influence function for detecting influential cases in regression}},
volume = {22},
year = {1980}
}
@article{Huffcutt1995,
abstract = {This article describes the development of a new technique for identifying outlier coefficients in meta-analytic data sets. Denoted as the sample-adjusted meta-analytic deviancy statistic or SAMD, this technique takes into account the sample size on which each study is based when determining outlier status. An empirical test of the SAMD statistic with an actual meta-analytic data set resulted in a substantial reduction in residual variabilities and a corresponding increase in the percentage of variance accounted for by statistical artifacts after removal of outlier study coefficients. Moreover, removal of these coefficients helped to clarify what was a confusing and difficult-to-explain finding in this meta-analysis. It is suggested that analysis for outliers become a routine part of meta-analysis methodology. Limitations and directions for future research are discussed. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
author = {Huffcutt, Allen I and Arthur, Winfred},
doi = {10.1037//0021-9010.80.2.327},
issn = {0021-9010},
journal = {Journal of Applied Psychology},
number = {2},
pages = {327--334},
title = {{Development of a new outlier statistic for meta-analytic data}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0021-9010.80.2.327},
volume = {80},
year = {1995}
}
@article{Pashler2012a,
abstract = {lkj this special section had a lot of incisive articles. I read the paper version. Visit http://pps.sagepub.com/content/7/6.toc if wanting to see those articles again.},
author = {Pashler, Harold and Wagenmakers, Eric–Jan},
doi = {10.1177/1745691612465253},
isbn = {1745-6916$\backslash$n1745-6924},
issn = {1745-6916},
journal = {Perspectives on Psychological Science},
month = {nov},
number = {6},
pages = {528--530},
pmid = {26168108},
title = {{Editors' introduction to the special section on replicability in psychological science}},
url = {http://journals.sagepub.com/doi/10.1177/1745691612465253},
volume = {7},
year = {2012}
}
@article{Yuan2001,
abstract = {A small proportion of outliers can distort the results based on classical procedures in covariance structure analysis. We look at the quantitative effect of outliers on estimators and test statistics based on normal theory maximum likelihood and the asymptotically distribution-free procedures. Even if a proposed structure is correct for the majority of the data in a sample, a small proportion of outliers leads to biased estimators and significant test statistics. An especially unfortunate consequence is that the power to reject a model can be made arbitrarily--but misleadingly--large by inclusion of outliers in an analysis.},
author = {Yuan, Ke Hai and Bentler, Peter M.},
doi = {10.1348/000711001159366},
isbn = {2044-8317},
issn = {00071102},
journal = {British Journal of Mathematical and Statistical Psychology},
number = {1},
pages = {161--175},
pmid = {11393898},
title = {{Effect of outliers on estimators and tests in covariance structure analysis}},
volume = {54},
year = {2001}
}
@article{ScienceCollaboration2015,
author = {{Open Science Collaboration}},
doi = {10.1126/science.aac4716},
issn = {0036-8075},
journal = {Science},
month = {aug},
number = {6251},
pages = {aac4716},
title = {{Estimating the reproducibility of psychological science}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.aac4716},
volume = {349},
year = {2015}
}
@article{Simmons2011,
archivePrefix = {arXiv},
arxivId = {2021},
author = {Simmons, Joseph P and Nelson, Leif D and Simonsohn, Uri},
doi = {10.1177/0956797611417632},
eprint = {2021},
isbn = {1467-9280 (Electronic)$\backslash$n0956-7976 (Linking)},
issn = {0956-7976},
journal = {Psychological Science},
number = {11},
pages = {1359--1366},
pmid = {22006061},
title = {{False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant}},
volume = {22},
year = {2011}
}
@article{Klein2014c,
abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect – imagined contact reducing prejudice – showed weak support for replicability. And two effects – flag priming influencing conservatism and currency priming influencing system justification – did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahn{\'{i}}k, {\v{S}}t{\v{e}}p{\'{a}}n and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and Ijzerman, Hans and John, Melissa Sue and Joy-Gaba, Jennifer A. and Kappes, Heather Barry and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and {Van Swol}, Lyn M. and Thompson, Donna and {Van 'T Veer}, A. E. and Vaughn, Leigh Ann and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
doi = {10.1027/1864-9335/a000178},
isbn = {2050-9863},
issn = {21512590},
journal = {Social Psychology},
keywords = {Cross-cultural,Generalizability,Replication,Reproducibility,Variation},
number = {3},
pages = {142--152},
title = {{Investigating variation in replicability: A "many labs" replication project}},
volume = {45},
year = {2014}
}
@article{Maxwell2015,
author = {Maxwell, Scott E. and Lau, Michael Y. and Howard, George S.},
doi = {10.1037/a0039400},
issn = {1935-990X},
journal = {American Psychologist},
number = {6},
pages = {487--498},
title = {{Is psychology suffering from a replication crisis? What does “failure to replicate” really mean?}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0039400},
volume = {70},
year = {2015}
}
@misc{jamovi2018,
author = {{jamovi project}},
title = {{jamovi (Version 0.8)[Computer software]}},
url = {https://www.jamovi.org},
year = {2018}
}
@misc{JASP2018,
author = {{JASP Team}},
title = {{JASP (Version 0.8.6)[Computer software]}},
url = {https://jasp-stats.org/},
year = {2018}
}
@article{Simonsohn2013,
abstract = {Abstract I argue that requiring authors to post the raw data supporting their published results has the benefit, among many others, of making fraud much less likely to go undetected. I illustrate this point by describing two cases of suspected fraud I identified exclusively ...},
author = {Simonsohn, Uri},
doi = {10.1177/0956797613480366},
isbn = {0956-7976},
issn = {14679280},
journal = {Psychological Science},
keywords = {data posting,data sharing,decision making,fake data,judgment,scientific communication},
number = {10},
pages = {1875--1888},
pmid = {23982243},
title = {{Just post it: The lesson from two cases of fabricated data detected by statistics alone}},
volume = {24},
year = {2013}
}
@article{Lakens2018,
author = {Lakens, Daniel and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and {Van Calster}, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and Gonzalez-Marquez, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and van Harmelen, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavsk{\'{y}}, Jiř{\'{i}} and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and de Oliveira, Cilene Lino and de Xivry, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and {\'{S}}wi{\c{a}}tkowski, Wojciech and Vadillo, Miguel A. and {Van Assen}, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
doi = {10.1038/s41562-018-0311-x},
issn = {2397-3374},
journal = {Nature Human Behaviour},
month = {mar},
number = {3},
pages = {168--171},
title = {{Justify your alpha}},
url = {http://www.nature.com/articles/s41562-018-0311-x},
volume = {2},
year = {2018}
}
@article{VanElk2015,
author = {{van Elk}, Michiel and Matzke, Dora and Gronau, Quentin F and Guan, Maime and Vandekerckhove, Joachim and Wagenmakers, Eric-Jan},
doi = {10.3389/fpsyg.2015.01365},
isbn = {1664-1078},
issn = {1664-1078},
journal = {Frontiers in psychology},
pages = {1365},
pmid = {26441741},
title = {{Meta-analyses are no substitute for registered replications: A skeptical perspective on religious priming.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4569810{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {6},
year = {2015}
}
@article{Gigerenzer2004,
author = {Gigerenzer, Gerd},
doi = {10.1016/j.socec.2004.09.033},
journal = {The Journal of Socio-Economics},
keywords = {a fixed level of,and,case in the light,collective illusions,editors,he rather gives his,he rejects hypotheses,his ideas,in all circumstances,mind to each particular,no scientific worker has,of his evidence and,rituals,significance at which from,statistical significance,textbooks,year to year},
number = {5},
pages = {587--606},
title = {{Mindless statistics}},
volume = {33},
year = {2004}
}
@article{Gelman2006,
author = {Gelman, Andrew},
doi = {10.1198/004017005000000661},
issn = {0040-1706},
journal = {Technometrics},
month = {aug},
number = {3},
pages = {432--435},
title = {{Multilevel (hierarchical) modeling: What it can and cannot do}},
url = {http://www.tandfonline.com/doi/abs/10.1198/004017005000000661},
volume = {48},
year = {2006}
}
@misc{Pinheiro2017,
author = {Pinheiro, J and Bates, Douglas and Debroy, S and Sarkar, D and Team, R Core},
title = {{nlme: Linear and nonlinear mixed effects models}},
url = {https://cran.r-project.org/package=nlme},
year = {2017}
}
@article{Orr1991,
author = {Orr, JOHN M. and Sackett, PAUL R. and Dubois, CATHY L. Z.},
doi = {10.1111/j.1744-6570.1991.tb02401.x},
issn = {00315826},
journal = {Personnel Psychology},
month = {dec},
number = {3},
pages = {473--486},
title = {{Outlier detection and treatment in I / O psychology: A survey of researcher beliefs and empirical illustration}},
url = {http://doi.wiley.com/10.1111/j.1744-6570.1991.tb02401.x},
volume = {44},
year = {1991}
}
@article{Shaari2009,
abstract = {An outlier in a dataset is a point or a class of points that is considerably dissimilar to or inconsistent with the remainder of the data. Detection of outliers is important for many applications and has always attracted attention among data mining research community. In this paper, a new method in detecting outlier based on Rough Sets Theory is proposed. The main concept of using the Rough Sets for outlier detection is to discover Non-Reduct from the information system (IS). Non-Reduct is a set of attributes from IS that may contain outliers. It is discovered through the computation of Non-Reduct by defining Indiscernibility matrix modulo (iDMM D) and Indiscernibility function modulo (iDFM D). A measurement called RSetOF (Rough Set Outlier Factor Value) is hereby defined to identify and detect outlier objects. Extensive experiments were conducted where ten benchmark datasets were tested with the proposed method. To evaluate the effectiveness of performance of the proposed method, RSetAlg is compared to the Frequent Pattern (FindFPOF) method. The experimental result reveals that the approach utilised is a good outlier detection method compared to FindFPOF method. Thus, this proposed method has formed a novel and competitive method in outlier detection. (PsycINFO Database Record (c) 2010 APA, all rights reserved) (journal abstract)},
author = {Shaari, Faizah and Bakar, Azuraliza Abu and Hamdan, Abdul Razak},
doi = {10.3233/IDA-2009-0363},
isbn = {1088-467X},
issn = {1088467X},
journal = {Intelligent Data Analysis},
keywords = {Anomaly,Deviate,Mining rarity,Non-reduct,Outlier detection,Rare cases},
number = {2},
pages = {191--206},
title = {{Outlier detection based on rough sets theory}},
volume = {13},
year = {2009}
}
@article{Bakker2014,
abstract = {BACKGROUND: The removal of outliers to acquire a significant result is a questionable research practice that appears to be commonly used in psychology. In this study, we investigated whether the removal of outliers in psychology papers is related to weaker evidence (against the null hypothesis of no effect), a higher prevalence of reporting errors, and smaller sample sizes in these papers compared to papers in the same journals that did not report the exclusion of outliers from the analyses.$\backslash$n$\backslash$nMETHODS AND FINDINGS: We retrieved a total of 2667 statistical results of null hypothesis significance tests from 153 articles in main psychology journals, and compared results from articles in which outliers were removed (N = 92) with results from articles that reported no exclusion of outliers (N = 61). We preregistered our hypotheses and methods and analyzed the data at the level of articles. Results show no significant difference between the two types of articles in median p value, sample sizes, or prevalence of all reporting errors, large reporting errors, and reporting errors that concerned the statistical significance. However, we did find a discrepancy between the reported degrees of freedom of t tests and the reported sample size in 41{\%} of articles that did not report removal of any data values. This suggests common failure to report data exclusions (or missingness) in psychological articles.$\backslash$n$\backslash$nCONCLUSIONS: We failed to find that the removal of outliers from the analysis in psychological articles was related to weaker evidence (against the null hypothesis of no effect), sample size, or the prevalence of errors. However, our control sample might be contaminated due to nondisclosure of excluded values in articles that did not report exclusion of outliers. Results therefore highlight the importance of more transparent reporting of statistical analyses.},
author = {Bakker, Marjan and Wicherts, Jelte M.},
doi = {10.1371/journal.pone.0103360},
isbn = {1932-6203 (Electronic)$\backslash$r1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
number = {7},
pages = {1--9},
pmid = {25072606},
title = {{Outlier removal and the relation with reporting errors and quality of psychological research}},
volume = {9},
year = {2014}
}
@article{Stevens1984,
abstract = {Because the results of a regression analysis can be quite sensitive to outliers (either on y or in the space of the predictors), it is important to be able to detect such points. This article discusses and interrelates the following four diagnostics that are useful in identifying outliers: studentized residuals, the hat elements, Cook's distance, and Mahalanobis distance. Guidelines are given for interpretation of the diagnostics. Outliers will not necessarily be influential in affecting the regression coefficients. This important fact is illustrated and emphasized.},
author = {Stevens, James P.},
doi = {10.1037/0033-2909.95.2.334},
isbn = {0033-2909},
issn = {00332909},
journal = {Psychological Bulletin},
keywords = {methods of detection of errant data points in regr},
number = {2},
pages = {334--344},
title = {{Outliers and influential data points in regression analysis}},
volume = {95},
year = {1984}
}
@article{Young1980,
author = {Young, Walter R. and Barnett, Vic and Lewis, Toby},
doi = {10.2307/1268205},
issn = {00401706},
journal = {Technometrics},
month = {nov},
number = {4},
pages = {631},
title = {{Outliers in statistical data}},
url = {https://academic.oup.com/mind/article-lookup/doi/10.1093/mind/101.403.553 http://www.jstor.org/stable/1268205?origin=crossref},
volume = {22},
year = {1980}
}
@article{Munoz-Garcia1990,
author = {Mu{\~{n}}oz-Garcia, J. and Moreno-Rebollo, J. L. and Pascual-Acosta, A.},
doi = {10.2307/1403805},
issn = {03067734},
journal = {International Statistical Review / Revue Internationale de Statistique},
month = {dec},
number = {3},
pages = {215--226},
title = {{Outliers: A formal approach}},
volume = {58},
year = {1990}
}
@misc{Aust2017,
author = {Aust, F. and Barth, Marius},
title = {{papaja: Create APA manuscripts with R Markdown.}},
url = {https://github.com/crsh/papaja},
year = {2017}
}
@article{Nosek2015b,
author = {Nosek, B},
journal = {Science},
number = {6242},
pages = {1422--1425},
title = {{Promoting an open research culture}},
volume = {348},
year = {2015}
}
@article{Miguel2014,
abstract = {There is growing appreciation for the advantages of experimentation in the social sciences. Policy-relevant claims that in the past were backed by theoretical arguments and inconclusive correlations are now being investigated using more credible methods. Changes have been particularly pronounced in development economics, where hundreds of randomized trials have been carried out over the last decade. When experimentation is difficult or impossible, researchers are using quasi-experimental designs. Governments and advocacy groups display a growing appetite for evidence-based policy-making. In 2005, Mexico established an independent government agency to rigorously evaluate social programs, and in 2012, the U.S. Office of Management and Budget advised federal agencies to present evidence from randomized program evaluations in budget requests ( 1 , 2 ).},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Miguel, E and Camerer, C and Casey, K. and Cohen, J and Esterling, K M and Gerber, A. and Glennerster, R and Green, D P and Humphreys, M. and Imbens, G and Laitin, D and Madon, T and Nelson, L and Nosek, B. A. and Petersen, M and Sedlmayr, R and Simmons, J. P. and Simonsohn, U. and {van der Laan}, M},
doi = {10.1126/science.1245317},
eprint = {NIHMS150003},
isbn = {2122633255},
issn = {0036-8075},
journal = {Science},
month = {jan},
number = {6166},
pages = {30--31},
pmid = {24385620},
title = {{Promoting transparency in social science research}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24385620{\%}5Cnhttp://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4103621 http://www.sciencemag.org/lookup/doi/10.1126/science.1245317},
volume = {343},
year = {2014}
}
@article{LeBel2013,
abstract = {There is currently an unprecedented level of doubt regarding the reliability of research findings in psychology. Many recommendations have been made to improve the current situation. In this article, we report results from PsychDisclosure.org, a novel open-science initiative that provides a platform for authors of recently published articles to disclose four methodological design specification details that are not required to be disclosed under current reporting standards but that are critical for accurate interpretation and evaluation of reported findings. Grassroots sentiment—as manifested in the positive and appreciative response to our initiative—indicates that psychologists want to see changes made at the systemic level regarding disclosure of such methodological details. Almost 50{\%} of contacted researchers disclosed the requested design specifications for the four methodological categories (excluded subjects, nonreported conditions and measures, and sample size determination). Disclosed information provided by participating authors also revealed several instances of questionable editorial practices, which need to be thoroughly examined and redressed. On the basis of these results, we argue that the time is now for mandatory methods disclosure statements for all psychology journals, which would be an important step forward in improving the reliability of findings in psychology.},
author = {LeBel, Etienne P. and Borsboom, Denny and Giner-Sorolla, Roger and Hasselman, Fred and Peters, Kurt R. and Ratliff, Kate A. and Smith, Colin Tucker},
doi = {10.1177/1745691613491437},
isbn = {1745-6916 (Print) 1745-6916 (Linking)},
issn = {1745-6916},
journal = {Perspectives on Psychological Science},
keywords = {disclosure,methodological design specifications,methodology,reporting standards},
month = {jul},
number = {4},
pages = {424--432},
pmid = {26173121},
title = {{PsychDisclosure.org}},
url = {http://journals.sagepub.com/doi/10.1177/1745691613491437},
volume = {8},
year = {2013}
}
@article{Nelson2018,
author = {Nelson, Leif D and Simmons, Joseph and Simonsohn, Uri},
doi = {10.1146/annurev-psych-122216-011836},
issn = {0066-4308},
journal = {Annual Review of Psychology},
month = {jan},
number = {1},
pages = {511--534},
title = {{Psychology's renaissance}},
url = {http://www.annualreviews.org/doi/10.1146/annurev-psych-122216-011836},
volume = {69},
year = {2018}
}
@article{Ferguson2012a,
author = {Ferguson, Christopher J. and Brannick, Michael T.},
doi = {10.1037/a0024445},
issn = {1939-1463},
journal = {Psychological Methods},
number = {1},
pages = {120--128},
title = {{Publication bias in psychological science: Prevalence, methods for identifying and controlling, and implications for the use of meta-analyses.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0024445},
volume = {17},
year = {2012}
}
@article{Fiedler2016,
abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes.Weconclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
author = {Fiedler, Klaus and Schwarz, Norbert},
doi = {10.1177/1948550615612150},
isbn = {8224406113},
issn = {1948-5506},
journal = {Social Psychological and Personality Science},
keywords = {ethics/morality,language,research methods,research practices,survey methodology},
month = {jan},
number = {1},
pages = {45--52},
pmid = {25869851},
title = {{Questionable research practices revisited}},
url = {https://doi.org/10.1177/1948550615612150 http://journals.sagepub.com/doi/10.1177/1948550615612150},
volume = {7},
year = {2016}
}
@article{Edgington1978,
author = {Edgington, Eugene S. and Ezinga, Gerard},
doi = {10.1080/00223980.1978.9921468},
issn = {0022-3980},
journal = {The Journal of Psychology},
month = {jul},
number = {2},
pages = {259--262},
title = {{Randomization tests and outlier scores}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00223980.1978.9921468},
volume = {99},
year = {1978}
}
@article{Asendorpf2012,
abstract = {Abstract: Replicability of findings is at the heart of any empirical science. The aim of this article is to move the current replicability debate in psychology towards concrete recommendations for improvement. We focus on research practices but also offer guidelines for reviewers, editors, journal management, teachers, granting institutions, and university promotion committees, highlighting some of the emerging and existing practical solutions that can facilitate implementation of these recommendations. The challenges for improving replicability in psychological science are systemic. Improvement can occur only if changes are made at many levels of practice, evaluation, and reward},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Asendorpf, Jens B and Conner, Mark and {De Fruyt}, Filip and {De Houwer}, Jan and Denissen, Jaap J. A. and Fiedler, Klaus and Fiedler, Susann and Funder, David C. and Kliegl, Reinhold and Nosek, Brian A. and Perugini, Marco and Roberts, Brent W. and Schmitt, Manfred and van Aken, Marcel A. G. and Weber, Hannelore and Wicherts, Jelte M.},
doi = {10.1002/per.1919},
eprint = {arXiv:1011.1669v3},
isbn = {1099-0984},
issn = {08902070},
journal = {European Journal of Personality},
keywords = {confirmation bias,generalizability,publication bias,replicability,research transparency},
month = {mar},
number = {2},
pages = {108--119},
pmid = {25246403},
title = {{Recommendations for increasing replicability in psychology}},
url = {http://www.rap.ucr.edu/replication.pdf http://doi.wiley.com/10.1002/per.1919},
volume = {27},
year = {2013}
}
@article{Benjamin2018,
abstract = {We propose to change the default P-value threshold for statistical significance from 0.05 to 0.005 for claims of new discoveries.},
archivePrefix = {arXiv},
arxivId = {psyarxiv/mky9j},
author = {Benjamin, Daniel J. and Berger, James O. and Johannesson, Magnus and Nosek, Brian A. and Wagenmakers, Eric-Jan and Berk, Richard and Bollen, Kenneth A. and Brembs, Bj{\"{o}}rn and Brown, Lawrence and Camerer, Colin and Cesarini, David and Chambers, Christopher D. and Clyde, Merlise and Cook, Thomas D. and {De Boeck}, Paul and Dienes, Zoltan and Dreber, Anna and Easwaran, Kenny and Efferson, Charles and Fehr, Ernst and Fidler, Fiona and Field, Andy P. and Forster, Malcolm and George, Edward I. and Gonzalez, Richard and Goodman, Steven and Green, Edwin and Green, Donald P. and Greenwald, Anthony G. and Hadfield, Jarrod D. and Hedges, Larry V. and Held, Leonhard and {Hua Ho}, Teck and Hoijtink, Herbert and Hruschka, Daniel J. and Imai, Kosuke and Imbens, Guido and Ioannidis, John P.A. and Jeon, Minjeong and Jones, James Holland and Kirchler, Michael and Laibson, David and List, John and Little, Roderick and Lupia, Arthur and Machery, Edouard and Maxwell, Scott E. and McCarthy, Michael and Moore, Don A. and Morgan, Stephen L. and Munaf{\'{o}}, Marcus and Nakagawa, Shinichi and Nyhan, Brendan and Parker, Timothy H. and Pericchi, Luis and Perugini, Marco and Rouder, Jeff and Rousseau, Judith and Savalei, Victoria and Sch{\"{o}}nbrodt, Felix D. and Sellke, Thomas and Sinclair, Betsy and Tingley, Dustin and {Van Zandt}, Trisha and Vazire, Simine and Watts, Duncan J. and Winship, Christopher and Wolpert, Robert L. and Xie, Yu and Young, Cristobal and Zinman, Jonathan and Johnson, Valen E.},
doi = {10.1038/s41562-017-0189-z},
eprint = {mky9j},
isbn = {2397-3374},
issn = {23973374},
journal = {Nature Human Behaviour},
number = {1},
pages = {6--10},
primaryClass = {psyarxiv},
title = {{Redefine statistical significance}},
volume = {2},
year = {2018}
}
@article{Cumming2008,
author = {Cumming, G.},
doi = {10.1111/j.1745-6924.2008.00079.x},
journal = {Perspectives on Psychological Science},
number = {4},
pages = {286--300},
title = {{Replication and p intervals}},
volume = {3},
year = {2008}
}
@article{Lindsay2015,
author = {Lindsay, D. Stephen},
doi = {10.1177/0956797615616374},
isbn = {0956797615616},
issn = {0956-7976},
journal = {Psychological Science},
month = {dec},
number = {12},
pages = {1827--1832},
pmid = {26553013},
title = {{Replication in psychological science}},
url = {http://journals.sagepub.com/doi/10.1177/0956797615616374},
volume = {26},
year = {2015}
}
@article{Wainer1976,
author = {Wainer, Howard},
doi = {10.2307/1164985},
issn = {03629791},
journal = {Journal of Educational Statistics},
keywords = {influence function,robust statistics,sorized samples,stem-and-leaf diagram},
number = {4},
pages = {285},
title = {{Robust statistics: A survey and some prescriptions}},
url = {http://www.jstor.org/stable/1164985?origin=crossref},
volume = {1},
year = {1976}
}
@article{Nosek2012c,
abstract = {An academic scientist's professional success depends on publishing. Publishing norms emphasize novel, positive results. As such, disciplinary incentives encourage design, analysis, and reporting decisions that elicit positive results and ignore negative results. Prior reports demonstrate how these incentives inflate the rate of false effects in published science. When incentives favor novelty over replication, false results persist in the literature unchallenged, reducing efficiency in knowledge accumulation. Previous suggestions to address this problem are unlikely to be effective. For example, a journal of negative results publishes otherwise unpublishable reports. This enshrines the low status of the journal and its content. The persistence of false findings can be meliorated with strategies that make the fundamental but abstract accuracy motive—getting it right—competitive with the more tangible and concrete incentive—getting it published. This article develops strategies for improving scientific practices and knowledge accumulation that account for ordinary human motivations and biases.},
archivePrefix = {arXiv},
arxivId = {1205.4251},
author = {Nosek, Brian A. and Spies, Jeffrey R. and Motyl, Matt},
doi = {10.1177/1745691612459058},
eprint = {1205.4251},
isbn = {1745-6916, 1745-6924},
issn = {17456916},
journal = {Perspectives on Psychological Science},
keywords = {false positives,incentives,methodology,motivated reasoning,replication},
number = {6},
pages = {615--631},
pmid = {19565683},
title = {{Scientific utopia: II. Restructuring incentives and practices to promote truth over publishability}},
volume = {7},
year = {2012}
}
@article{Kruskal1960,
author = {Kruskal, William H.},
doi = {10.1080/00401706.1960.10489875},
issn = {15372723},
journal = {Technometrics},
number = {1},
pages = {1--3},
title = {{Some Remarks on Wild Observations}},
volume = {2},
year = {1960}
}
@article{Coin2008,
abstract = {Statistical models are often based on normal distributions and procedures for testing this distributional assumption are needed. Many goodness-of-fit tests suffer from the presence of outliers, in the sense that they may reject the null hypothesis even in the case of a single extreme observation. We show a possible extension of the Shapiro-Wilk test that is not affected by such a problem. The presented method is inspired by the forward search (FS), a new, recently proposed, diagnostic tool. An application to univariate observations shows how the procedure is able to capture the structure of the data, even in the presence of outliers. Other properties are also investigated. (PsycINFO Database Record (c) 2010 APA, all rights reserved) (journal abstract)},
author = {Coin, Daniele},
doi = {10.1007/s10260-007-0046-8},
isbn = {1618-2510$\backslash$r1613-981X},
issn = {16182510},
journal = {Statistical Methods and Applications},
keywords = {Forward search,Outlier,Robust approach,Shapiro-Wilk test},
number = {1},
pages = {3--12},
title = {{Testing normality in the presence of outliers}},
volume = {17},
year = {2008}
}
@article{Leggett,
author = {Leggett, Nathan C. and Thomas, Nicole A. and Loetscher, Tobias and Nicholls, Michael E. R.},
doi = {10.1080/17470218.2013.863371},
issn = {1747-0218},
journal = {Quarterly Journal of Experimental Psychology},
month = {dec},
number = {12},
pages = {2303--2309},
title = {{The life of p: "Just significant" results are on the rise}},
url = {http://journals.sagepub.com/doi/10.1080/17470218.2013.863371},
volume = {66},
year = {2013}
}
@article{Bernoulli1777,
author = {Bernoulli, DANIEL and Allen, C. G.},
doi = {10.1093/biomet/48.1-2.3},
issn = {0006-3444},
journal = {Biometrika},
number = {1-2},
pages = {3--18},
title = {{The most probable choice between several discrepant observations and the formation therefrom of the most likely induction}},
url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/48.1-2.3},
volume = {48},
year = {1961}
}
@article{Osborne2004,
author = {Osborne, Jason W. and Overbay, Amy},
journal = {Practical Assessment, Research {\&} Evaluation},
number = {6},
pages = {1--12},
title = {{The power of outliers (and why researchers should always check for them)}},
volume = {9},
year = {2004}
}
@book{Tabachnick2012,
address = {Boston, MA},
author = {Tabachnick, B. G. and Fidell, L. S.},
edition = {Sixth},
isbn = {978-0205849574},
issn = {0956-7976},
pmid = {1577},
publisher = {Pearson},
title = {{Using multivariate statistics}},
year = {2012}
}
@article{Ioannidis2005,
abstract = {There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
archivePrefix = {arXiv},
arxivId = {gr-qc/0208024},
author = {Ioannidis, John P A},
doi = {10.1371/journal.pmed.0020124},
eprint = {0208024},
isbn = {3540239081},
issn = {1549-1676},
journal = {PLoS Medicine},
number = {8},
pages = {e124},
pmid = {16060722},
primaryClass = {gr-qc},
title = {{Why most published research findings are false}},
url = {http://dx.plos.org/10.1371/journal.pmed.0020124},
volume = {2},
year = {2005}
}
@article{Wagenmakers2011a,
abstract = {Does psi exist? D. J. Bem (2011) conducted 9 studies with over 1,000 participants in an attempt to demonstrate that future events retroactively affect people's responses. Here we discuss several limitations of Bem's experiments on psi; in particular, we show that the data analysis was partly exploratory and that one-sided p values may overstate the statistical evidence against the null hypothesis. We reanalyze Bem's data with a default Bayesian t test and show that the evidence for psi is weak to nonexistent. We argue that in order to convince a skeptical audience of a controversial claim, one needs to conduct strictly confirmatory studies and analyze the results with statistical tests that are conservative rather than liberal. We conclude that Bem's p values do not indicate evidence in favor of precognition; instead, they indicate that experimental psychologists need to change the way they conduct their experiments and analyze their data.},
author = {Wagenmakers, Eric-Jan and Wetzels, Ruud and Borsboom, Denny and {van der Maas}, Han L. J.},
doi = {10.1037/a0022790},
isbn = {0022-3514},
issn = {1939-1315},
journal = {Journal of Personality and Social Psychology},
keywords = {Bayesian hypothesis test,Confirmatory experiments,ESP},
number = {3},
pages = {426--432},
pmid = {21280965},
title = {{Why psychologists must change the way they analyze their data: The case of psi: Comment on Bem (2011).}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0022790},
volume = {100},
year = {2011}
}
