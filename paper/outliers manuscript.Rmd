---
title             : "Outrageous observations: The redheaded stepchild of data analysis"
shorttitle        : "Outrageous observations"

author: 
  - name          : "K. D. Valentine"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "210 McAlester Hall, Columbia, MO, 65211"
    email         : "kdvdnf@mail.missouri.edu"
  - name          : "Erin M. Buchanan"
    affiliation   : "2"

affiliation:
  - id            : "1"
       institution   : "University of Missouri"
  - id            : "2"
      institution   : "Missouri State University"

author_note: |
 K. D. Valentine is a Ph.D. candidate at the University of Missouri.  Erin M. Buchanan is an Associate Professor of Quantitative Psychology at Missouri State University. 
  
abstract: |
  
  
keywords          : "outlier, influential observation, replication"

bibliography      : ["q_bib.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
relpace_ampersands: yes
csl               : apa6.csl
---

```{r load_packages, include = FALSE}
library("papaja")
library(knitr)
```

The focus on the lack of replication in psychology has raised concerns regarding the reproducibility and validity of current published reports (Etz & Vandekerckhove, 2016; Lindsay, 2015; Open Science Collaboration, 2015; van Elk et al., 2015). Researchers have attributed this lack of replication to many things, including publication bias (Brannick, 2012), the potential for the over-reliance, abuse, or misunderstanding of p-values (Gigerenzer, 2004; Ioannidis, 2005; Simmons, Nelson, & Simonsohn, 2011), and researcher degreed of freedom (Simmons, Nelson, & Simonsohn, 2011). 
In the years since the crisis began many recommendations have been made (Cumming, 2008; Simmons, Nelson, & Simonsohn, 2011; Asendorpf et al., 2013; Lakens, 2013; Benjamin et al., 2017; Lakens et al., 2018; Valentine, Buchanan, Scofield & Beauchamp, 2018; Maxwell, Lau, & Howard, 2015; Nosek, Spies, & Motyl, 2012), projects have been organized (Klein et al., 2014; Open Science Collaboration, 2015) and steps have been taken to assist in improving the reliability of psychological findings. While we have seen vast discussion of the problems and proposed solutions, research has yet to determine how this new culture of research has altered the specifics of reporting practices. Herein, we aim specifically to quantify the rates of reporting of outliers within psychology at two time points—in 2012, when the replication crisis was born (Pashler & Wagenmakers, 2012), and 5 years later, in 2017. 
#Outliers
Since Bernoulli first mentioned outliers in 1777, outliers, influential observations, or fringliers have been defined and redefined throughout the years (Beckman & Cook, 1983; Hodge & Austin, 2004; Munoz-Garcia, Moreno-Rebollo, Pascual-Acosta, 1990; Orr, Sackett, & Dubois, 1991; Osborne & Overbay, 2004), but for the purposes of this manuscript can be defined as, “an observation which being atypical and/or erroneous deviates decidedly from the general behavior of experimental data with respect to the criteria which is to be analyzed on it” (Munoz-Garcia, Moreno-Rebollo, Pascual-Acosta , 1990). And while there are a plethora of statistics available to researchers to identify and describe these datapoints (i.e. visual depictions of data and residuals, the 3 standard deviation rule, Mahalanobis or Cook’s distance, leverage, etc.; Tabachnick & Fidell, 2007), this discussion of outliers rarely makes it to the page. Consider, for example, Orr, Sackett, and Dubois (1991), who inspected 100 Industrial/Organizational Psychology personnel studies and found no mention of outliers. 
This is not to say that researchers are not taking these datapoints into consideration during their data cleaning and analysis plan, or even going so far as removing them, as work by LeBel and colleagues (2013) has reported that 11% of psychology researchers stated that they had not reported excluding participants for being outliers in their papers and additional work has suggested that more than a quarter decide whether to exclude data after looking at the impact of doing so (Fiedler & Schwarz, 2016). This, by definition, is a problem. The current zeitgeist of increased transparency and reproducibility applies not only to the manner in which data is collected, but also the various ways the data is transformed, cleaned, pared down, and analyzed. Therefore, it is just as important for a researcher to state how they identified outliers within their data, how the outliers were handled, and how this choice of handling impacted the estimates and conclusions of their analyses, as it is for them to report their sample size. 
Reporting out outlier practices are vital because by keeping outliers in a dataset, analyses are more likely to have increased error variance (depending on sample size, Orr et al., 1991) and biased estimates (Osborne, & Overbay, 2004) as well as reduced effect size and power (Orr, Sackett, & Dubois, 1991; Osborne, & Overbay 2004) which can alter the results of the analysis and lead to falsely supporting (Type I error), denying a claim (Type II error), or failing to replicate previous work. Additionally, incorrect estimates of effect lead to misleading meta-analyses or sample size estimates for study planning. Beyond these effects on analyses and conclusions investigation and exploration of these outliers can be beneficial to scientists as a way to further improve our research and research practices. Further, outliers can be informative to researchers and to their research models as they can encourage the diagnosis, change, and evolution of a research model (Beckman & Cook, 1983). Taken together, these issues caused by not noting outliers can lead to furthering unwarranted avenues of research, ignoring important information, and creating erroneous theories, all of which serve to weaken the sciences, while making clear the presence or absence of outliers, how they were assessed, and how they were handled, can improve our transparency and replicability, help to strengthen our science.  

# Method

## Fields
A list of psychological field areas was created to begin the search for appropriate journals to include.  The authors brainstormed the list of topics (shown in Table XX) by first listing major research areas in psychology (i.e. cognitive, clinical, social).  Second, a list of common courses offered at large universities was consulted to add to the list of fields.  Lastly, the American Psychological Association's list of divisions was examined for any potential missed fields. The topic list was created to capture large fields of psychology with small overlaps (i.e. cognition and neuropsychology) while avoiding specific subfields of topics (i.e. cognition, perception, and memory).

##Journals
Once these topic areas were decided, researchers used various search sources (Google, EBSCO host databases) to find journals that were dedicated to each broad topic. Journals were included if they appeared to publish a wide range of articles within the selected fields. A list of journals, publishers, and impact factors (as noted by the journal website) was created for each field.  Two journals from each field were selected based on the following criteria: 1) high impact factors, 2) impact factors over one a minimum, 3) a mix of publishers if possible, and 4) availability due to university resources.  These journals are shown in Table XX.

##Articles
Fifty articles from each journal were examined for data analysis. Data collection of articles started at the last volume publication from 2012 and progressed backwards until fifty articles had been found. We excluded online first publications and started in 2012 to ensure time for errata and retraction of articles. Articles were including if they met the following criteria: 1) included data analyses, 2) included multiple participants or data-points, and 3) analyses were based on human subjects or stimuli. Therefore, we excluded theory articles, animal populations, and single subject designs. Based on article review, three fields were excluded.  Applied behavior analysis articles predominantly included single-subject designs, evolutionary psychology articles were primarily theory articles, and statistics related journal articles were based on user created data with specific set characteristics.  Since none of these themes fit into our analysis of understanding data screening with human subject samples, we excluded those two fields from analyses.

##Data Processing
Each article was then reviewed for key components of data analyses.  Each experiment in an article was treated separately. For each experiment, the type of analysis, number of participants/stimuli, and if they indicated outliers were coded.  Types of analyses were broadly coded into basic statistics (descriptive statistics, z-scores, t-tests, and correlations), ANOVAs, regressions, chi-squares, nonparametric statistics, modeling, and Bayesian/other analyses. First, we coded if outliers were mentioned at all in article.  If so, we coded outliers into four types: 1) people, 2) data, 3) both, and 4) none found.  We found that a separate coding for data was important for analyses with response time studies where individual participants were not omitted but rather specific data trials were eliminated.  Then, the author decision on what to do with the outliers was coded into whether they removed participants/stimuli or left these outliers in the analysis, as well as if they tested the analyses with and without these outliers for determination of their effect on the study.  If they removed outliers, a new sample size was recorded. Lastly, we coded the reasoning for outlier detection as either participant based (i.e. 3-month old infants were too fussy to be included), experimenter based (i.e. the experimental session was interrupted), statistical based (i.e. three SD from the mean), and no listed reason.

Table XX
##List of Fields, Journals, and Impact Factors 2012
```{r table, echo = FALSE, results = 'asis'}
tableprint = matrix(NA, nrow = 31, ncol = 4)

tableprint[1, ] = c("Applied Behavior Analysis", "Journal of Experimental Analysis of Behavior", "Wiley", 1.39)

tableprint[2, ] = c("Applied Behavior Analysis", "Journal of Applied Behavior Analysis", "Wiley", 1.19)

tableprint[3, ] = c("Clinical", "Journal of Consulting and Clinical Psychology", "APA", 4.85)

tableprint[4, ] = c("Clinical", "Journal of Clinical Psychology", "Wiley", 2.12)

tableprint[5, ] = c("Cognitive", "Cognitive Psychology", "Elsevier", 4.27)

tableprint[6, ] = c("Cognitive", "Journal of Experimental Psychology: Learning, Memory, and Cognition", "APA", 2.85)

tableprint[7, ] = c("Counseling", "Journal of Counseling", "APA", 3.23)

tableprint[8, ] = c("Counseling", "The Counseling Psychologist", "Sage", 1.82)

tableprint[9, ] = c("Developmental", "Journal of Experimental Child Psychology", "Elsevier", 3.23)

tableprint[10, ] = c("Developmental", "Journal of Youth and Adolescence", "Springer", 2.72)

tableprint[11, ] = c("Educational", "Journal of Educational Psychology", "APA", 3.08)

tableprint[12, ] = c("Educational", "Contemporary Educational Psychology", "Elsevier", 2.20)

tableprint[13, ] = c("Environmental", "Journal of Environmental Psychology", "Elsevier", 2.93)

tableprint[14, ] = c("Environmental", "Environment and Behavior", "Sage", 1.27)

tableprint[15, ] = c("Evolutionary", "Evolution and Human Behavior", "Elsevier", 3.11)

tableprint[16, ] = c("Evolutionary", "Evolutionary Psychology", "Open Access", 1.06)

tableprint[17, ] = c("Forensics", "Psychology, Public Policy, and Law", "APA", 1.93)

tableprint[18, ] = c("Forensics", "Law and Human Bevhavior", "Spring", 2.16)

tableprint[19, ] = c("Industrial Organization", "Organizational Behavior and Human Decision Process", "Elsevier", 3.94)

tableprint[20, ] = c("Industrial Organization", "Personnel Psychology", "Wiley", 2.93)

tableprint[21, ] = c("Neurological/Physiological", "Neuropsychology", "APA", 3.82)

tableprint[22, ] = c("Neurological/Physiological", "Cognitive, Affective, and Behavioral Neuroscience", "Springer", 3.57)

tableprint[23, ] = c("Social", "Journal of Personality and Social Psychology", "APA", 5.08)

tableprint[24, ] = c("Social", "Journal of Experimental Social Psychology", "Elsevier", 2.31)

tableprint[25, ] = c("Sports", "Journal of Sport & Exercise Psychology", "Human Kinetics", 2.66)

tableprint[26, ] = c("Sports", "Sociology of Sport Journal", "Human Kinetics", 1.00)

tableprint[27, ] = c("Statistics", "Special Section of the Psychological Bulletin", "APA", 14.46)

tableprint[28, ] = c("Statistics", "Structural Equation Modeling", "Taylor & Francis", 4.71)

tableprint[29, ] = c("Overview", "Psychonomic Bulletin & Review", "Springer", 2.25)

tableprint[30, ] = c("Overview", "Psychonomic Science", "Sage", 4.43)

tableprint[31, ] = c("Overview", "Psychological Assessment", "APA", 2.99)

kable(tableprint, 
      digits = 3,
      col.names = c("Field", "Journal", "Publisher", "Impact Factor"))
```
*Note.* Impact factors as of tme of data collection (Spring 2013).
## Data analysis

# Results

# Discussion
Some may argue that use of the precious word limit dictated by journals to describe such acts as outlier identification and handling may be irrational. However, we contest that given the current availability and use of online supplements and appendixes, as well as the invent of the Open Science Framework (OSF; cite) which allows researchers the ability to upload any number of additional resources and supplements that can be easily referenced in their manuscripts in as little as 4 words: See supplement at osf.io/52mqw.



\newpage

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}


