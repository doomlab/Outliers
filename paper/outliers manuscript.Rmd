---
title             : "Outrageous observations: The redheaded stepchild of data analysis"
shorttitle        : "Outrageous observations"

author: 
  - name          : "K. D. Valentine"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "210 McAlester Hall, Columbia, MO, 65211"
    email         : "kdvdnf@mail.missouri.edu"
  - name          : "Erin M. Buchanan"
    affiliation   : "2"

affiliation:
  - id            : "1"
       institution   : "University of Missouri"
  - id            : "2"
      institution   : "Missouri State University"

author_note: |
 K. D. Valentine is a Ph.D. candidate at the University of Missouri.  Erin M. Buchanan is an Associate Professor of Quantitative Psychology at Missouri State University. 
  
abstract: |
  
  
keywords          : "outlier, influential observation, replication"

bibliography      : ["q_bib.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
relpace_ampersands: yes
csl               : apa6.csl
---

```{r load_packages, include = FALSE}
library("papaja")
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
master = read.csv("outliers complete.csv")
##experiment size
table(master$Journal, master$time.pulled)
```
The focus on the lack of replication in psychology has raised concerns regarding the reproducibility and validity of current published reports (Etz & Vandekerckhove, 2016; Lindsay, 2015; Open Science Collaboration, 2015; van Elk et al., 2015). Researchers have attributed this lack of replication to many things, including publication bias (Brannick, 2012), the potential for the over-reliance, abuse, or misunderstanding of p-values (Gigerenzer, 2004; Ioannidis, 2005; Simmons, Nelson, & Simonsohn, 2011), and researcher degreed of freedom (Simmons, Nelson, & Simonsohn, 2011), including selective removal or inclusion of datapoints. 
In the years since the crisis began many recommendations have been made (Cumming, 2008; Simmons, Nelson, & Simonsohn, 2011; Asendorpf et al., 2013; Lakens, 2013; Benjamin et al., 2017; Lakens et al., 2018; Valentine, Buchanan, Scofield & Beauchamp, 2018; Maxwell, Lau, & Howard, 2015; Nosek, Spies, & Motyl, 2012), projects have been organized (Klein et al., 2014; Open Science Collaboration, 2015) and steps have been taken to assist in improving the reliability of psychological findings. While we have seen vast discussion of the problems and proposed solutions, research has yet to determine how this new culture of research has altered the actual reporting practices of researchers. Herein, we aim specifically to quantify the rates of reporting of outliers within psychology at two time points—in 2012, when the replication crisis was born (Pashler & Wagenmakers, 2012), and 5 years later, in 2017. 
#Outliers
Since Bernoulli first mentioned outliers in 1777, outliers, influential observations, or fringliers have been defined and redefined throughout the years (Beckman & Cook, 1983; Hodge & Austin, 2004; Munoz-Garcia, Moreno-Rebollo, Pascual-Acosta, 1990; Orr, Sackett, & Dubois, 1991; Osborne & Overbay, 2004), but for the purposes of this manuscript can be defined as, “an observation which being atypical and/or erroneous deviates decidedly from the general behavior of experimental data with respect to the criteria which is to be analyzed on it” (Munoz-Garcia, Moreno-Rebollo, Pascual-Acosta , 1990). While there are a plethora of graphical and statistical options available to researchers to identify and describe these datapoints (i.e. visual depictions of data and residuals, the 3 standard deviation rule, Mahalanobis or Cook’s distance, leverage, etc.; Tabachnick & Fidell, 2007), this discussion of outliers rarely makes it to the page. Consider, for example, Orr, Sackett, and Dubois (1991), who inspected 100 Industrial/Organizational Psychology personnel studies and found no mention of outliers. 
This is not to say that researchers are not taking these datapoints into consideration during their data cleaning and analysis plan, or even going so far as removing them. Work by LeBel and colleagues (2013) has reported that 11% of psychology researchers stated that they had not reported excluding participants for being outliers in their papers, and Fiedler and Schwarz (2016) suggested that more than a quarter of researchers decide whether to exclude data after looking at the impact of doing so. This is problematic. Reporting out outlier practices are vital because by keeping outliers in a dataset, analyses are more likely to have increased error variance (depending on sample size, Orr et al., 1991) and biased estimates (Osborne, & Overbay, 2004) as well as reduced effect size and power (Orr, Sackett, & Dubois, 1991; Osborne, & Overbay 2004) which can alter the results of the analysis and lead to falsely supporting (Type I error), denying a claim (Type II error), or failing to replicate previous work. Additionally, incorrect estimates of effect lead to misleading meta-analyses or sample size estimates for study planning. Further, outliers can be informative to researchers and to their research models as they can encourage the diagnosis, change, and evolution of a research model (Beckman & Cook, 1983). Taken together, these issues caused by not mentioning outliers can lead to furthering unwarranted avenues of research, ignoring important information, and creating erroneous theories, all of which serve to weaken the sciences, while making clear the presence or absence of outliers, how they were assessed, and how they were handled, can improve our transparency and replicability, help to strengthen our science.  
The current zeitgeist of increased transparency and reproducibility applies not only to the manner in which data is collected, but also the various ways the data is transformed, cleaned, pared down, and analyzed. Therefore, it can be argued that it is just as important for a researcher to state how they identified outliers within their data, how the outliers were handled, and how this choice of handling impacted the estimates and conclusions of their analyses, as it is for them to report their sample size. 

# Method

## Fields
A list of psychological field areas was created to begin the search for appropriate journals to include.  The authors brainstormed the list of topics (shown in \@ref(tab:table)) by first listing major research areas in psychology (i.e. cognitive, clinical, social).  Second, a list of common courses offered at large universities was consulted to add to the list of fields.  Lastly, the American Psychological Association's list of divisions was examined for any potential missed fields. The topic list was created to capture large fields of psychology with small overlap (i.e. cognition and neuropsychology) while avoiding specific subfields of topics (i.e. cognition, perception, and memory). Sixteen fields were initiially identified, however only 13 were included in final analysis due to limitations noted below. 

##Journals
Once these topic areas were decided, researchers used various search sources (Google, EBSCO host databases) to find journals that were dedicated to each broad topic. Journals were included if they appeared to publish a wide range of articles within the selected fields. A list of journals, publishers, and impact factors (as noted by each journals website in Spring of 2013) were identified for each field.  Two journals from each field were selected based on the following criteria: 1) high impact factors, 2) impact factors over one at minimum, 3) a mix of publishers if possible, and 4) availability due to university resources.  These journals are shown in \@ref(tab:table).

##Articles
Fifty articles from each journal were examined for data analysis (25 articles were collected beginning in 2012 and 25 articles were collected beginning in 2017). Data collection of articles started at the last volume publication from 2012 and progressed backwards until fifty articles had been found. We excluded online first publications and started in 2012 to ensure time for errata and retraction of articles. Articles were including if they met the following criteria: 1) included data analyses, 2) included multiple participants or data-points, and 3) analyses were based on human subjects or stimuli. Therefore, we excluded theory articles, animal populations, and single subject designs. Based on article review, three fields were excluded.  Applied behavior analysis articles predominantly included single-subject designs, evolutionary psychology articles were primarily theory articles, and statistics related journal articles were based on user created data with specific set characteristics.  Since none of these themes fit into our analysis of understanding data screening with human subject samples, we excluded those three fields from analyses.

##Data Processing
Each article was then reviewed for key components of data analyses.  Each experiment in an article was treated separately. For each experiment, the type of analysis conducted, number of participants/stimuli analyzed, and if they made any mention of outliers were coded.  Types of analyses were broadly coded into basic statistics (descriptive statistics, z-scores, t-tests, and correlations), ANOVAs, regressions, chi-squares, nonparametric statistics, modeling, and Bayesian/other analyses. First, we coded if outliers were mentioned at all in article.  If so, we coded information about outliers into four types: 1) people, 2) data, 3) both, or 4) none found.  We found that a separate coding for data was important for analyses with response time studies where individual participants were not omitted but rather specific data trials were eliminated.  Then, the author's decision for how to handle the outliers was coded into whether they removed participants/stimuli, left these outliers in the analysis, or winsorized the daatapoints. Experiments were additionally coded for whether they tested the analyses with, without, or both with and without these outliers for determination of their effect on the study.  If they removed outliers, a new sample size was recorded. Lastly, we coded the reasoning for outlier detection as one or more of the following: 1) Statistical reason (i.e. they used numbers to define odd or deviant behavior in responding, such as z-score or Mahalanobis distance scores), 2) Participant error (i.e.failed attention checks, did not follow instructions, or poor data because of participant problems), 3) Unusable data (inside knowledge of the study or experimenter/technological problems), or 4) no listed reason.


##List of Fields, Journals, and Impact Factors 2012
```{r table, echo = FALSE, results = 'asis'}
tableprint = matrix(NA, nrow = 31, ncol = 4)

tableprint[1, ] = c("Applied Behavior Analysis", "Journal of Experimental Analysis of Behavior", "Wiley", 1.39)

tableprint[2, ] = c("Applied Behavior Analysis", "Journal of Applied Behavior Analysis", "Wiley", 1.19)

tableprint[3, ] = c("Clinical", "Journal of Consulting and Clinical Psychology", "APA", 4.85)

tableprint[4, ] = c("Clinical", "Journal of Clinical Psychology", "Wiley", 2.12)

tableprint[5, ] = c("Cognitive", "Cognitive Psychology", "Elsevier", 4.27)

tableprint[6, ] = c("Cognitive", "Journal of Experimental Psychology: Learning, Memory, and Cognition", "APA", 2.85)

tableprint[7, ] = c("Counseling", "Journal of Counseling", "APA", 3.23)

tableprint[8, ] = c("Counseling", "The Counseling Psychologist", "Sage", 1.82)

tableprint[9, ] = c("Developmental", "Journal of Experimental Child Psychology", "Elsevier", 3.23)

tableprint[10, ] = c("Developmental", "Journal of Youth and Adolescence", "Springer", 2.72)

tableprint[11, ] = c("Educational", "Journal of Educational Psychology", "APA", 3.08)

tableprint[12, ] = c("Educational", "Contemporary Educational Psychology", "Elsevier", 2.20)

tableprint[13, ] = c("Environmental", "Journal of Environmental Psychology", "Elsevier", 2.93)

tableprint[14, ] = c("Environmental", "Environment and Behavior", "Sage", 1.27)

tableprint[15, ] = c("Evolutionary", "Evolution and Human Behavior", "Elsevier", 3.11)

tableprint[16, ] = c("Evolutionary", "Evolutionary Psychology", "Open Access", 1.06)

tableprint[17, ] = c("Forensics", "Psychology, Public Policy, and Law", "APA", 1.93)

tableprint[18, ] = c("Forensics", "Law and Human Bevhavior", "Spring", 2.16)

tableprint[19, ] = c("Industrial Organization", "Organizational Behavior and Human Decision Process", "Elsevier", 3.94)

tableprint[20, ] = c("Industrial Organization", "Personnel Psychology", "Wiley", 2.93)

tableprint[21, ] = c("Neurological/Physiological", "Neuropsychology", "APA", 3.82)

tableprint[22, ] = c("Neurological/Physiological", "Cognitive, Affective, and Behavioral Neuroscience", "Springer", 3.57)

tableprint[23, ] = c("Social", "Journal of Personality and Social Psychology", "APA", 5.08)

tableprint[24, ] = c("Social", "Journal of Experimental Social Psychology", "Elsevier", 2.31)

tableprint[25, ] = c("Sports", "Journal of Sport & Exercise Psychology", "Human Kinetics", 2.66)

tableprint[26, ] = c("Sports", "Sociology of Sport Journal", "Human Kinetics", 1.00)

tableprint[27, ] = c("Statistics", "Special Section of the Psychological Bulletin", "APA", 14.46)

tableprint[28, ] = c("Statistics", "Structural Equation Modeling", "Taylor & Francis", 4.71)

tableprint[29, ] = c("Overview", "Psychonomic Bulletin & Review", "Springer", 2.25)

tableprint[30, ] = c("Overview", "Psychonomic Science", "Sage", 4.43)

tableprint[31, ] = c("Overview", "Psychological Assessment", "APA", 2.99)

kable(tableprint, 
      digits = 3,
      col.names = c("Field", "Journal", "Publisher", "Impact Factor"))
```
*Note.* Impact factors as of time of initial data collection (Spring 2013).


```{r proportions}

table(master$mention.outliers)
master$mention.outliers = factor(master$mention.outliers,
                      levels = c("no", "yes"),
                      labels = c("No", "Yes"))

##create a data frame of the percentages
outsummary = table(master$mention.outliers, master$time.pulled, master$Type)
outsummary = as.data.frame(outsummary)
colnames(outsummary) = c("mention.outliers", "time.pulled", "Type", "Freq")

##here we want to calculate if they mention outliers by year and type
##want to focus on the yeses 
library(dplyr)

outpercent = group_by(outsummary, time.pulled, Type) %>%
  mutate(percent = Freq/sum(Freq)*100)

#kable(outpercent)
##in the outpercent table, you see the percent based on year and Type, so each yes/no combination adds up to one hundred. We are only interested in the percent of yeses.
```


```{r journalgraph}
library(ggplot2)

cleanup = theme(panel.grid.major = element_blank(), 
              panel.grid.minor = element_blank(), 
              panel.background = element_blank(), 
              axis.line = element_line(colour = "black"), 
              legend.key = element_rect(fill = "white"),
              text = element_text(size = 15))

graphdata = subset(outpercent, mention.outliers == "Yes")

#calculate CIs to add to graph
graphdata$sample = as.data.frame(table(master$time.pulled, master$Type))$Freq
graphdata$SE = sqrt(graphdata$percent*(100-graphdata$percent)/graphdata$sample)

dotplot = ggplot(graphdata, aes(Type, percent, color = time.pulled))
finalgraph = dotplot +
  geom_pointrange(aes(ymin=percent-1.96*SE, ymax=percent+1.96*SE))+ 
  cleanup +
  xlab("Field of Journal") +
  ylab("Percent of Outlier Mentions") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  scale_color_manual(name = "Year", 
                     values = c("maroon", "gray")) + 
  coord_cartesian(ylim = c(0,75))

tiff(filename = "mention_graph.tiff", res = 300, width = 8, 
     height = 6, units = 'in', compression = "lzw")
plot(finalgraph)
dev.off()
```

```{r journalprop}
library(MOTE)
graphdata$prop = graphdata$percent/100
types = levels(graphdata$Type)
for (i in 1:length(types))
{
  saved = d.prop(graphdata$prop[graphdata$Type==types[i]][1],
         graphdata$prop[graphdata$Type==types[i]][2],
         graphdata$sample[ graphdata$Type==types[i]][1], 
         graphdata$sample[ graphdata$Type==types[i]][2],
         a = .05) 
  print(c(types[i],saved$d))
}

```

```{r reason}
master$reason.code
table(master$reason.code)
reasons = subset(master, mention.outliers == "Yes")

##create reason columns
reasons$part = 0
reasons$part[ grep("part", reasons$reason.code)] = 1
reasons$stat = 0
reasons$stat[ grep("stat", reasons$reason.code)] = 1
reasons$exp = 0
reasons$exp[ grep("unus", reasons$reason.code)] = 1

##create a data frame of the percentages
reasonssummary = table(reasons$part, reasons$time.pulled, reasons$Type)
reasonssummary = as.data.frame(reasonssummary)
colnames(reasonssummary) = c("yesno", "time.pulled", "Type", "part.freq")
reasonssummary$stat.freq = as.data.frame(table(reasons$stat, reasons$time.pulled, reasons$Type))$Freq
reasonssummary$exp.freq = as.data.frame(table(reasons$exp, reasons$time.pulled, reasons$Type))$Freq

reasonspercent = group_by(reasonssummary, time.pulled, Type) %>%
  mutate(part.percent = part.freq/sum(part.freq)*100) %>%
  mutate(stat.percent = stat.freq/sum(stat.freq)*100) %>%
  mutate(exp.percent = exp.freq/sum(exp.freq)*100) 

kable(reasonspercent)
#only include the percent of 1s

```

```{r Peopledata}

peep_data = subset(master, mention.outliers == "Yes")
table(peep_data$peopleor.data.points)

##create a data frame of the percentages
peep_datasummary = table(peep_data$peopleor.data.points, peep_data$time.pulled, peep_data$Type)
peep_datasummary = as.data.frame(peep_datasummary)
colnames(peep_datasummary) = c("code", "time.pulled", "Type", "Freq")

peep_datapercent = group_by(peep_datasummary, time.pulled, Type) %>%
  mutate(percent = Freq/sum(Freq)*100)
kable(peep_datapercent)
```
```{r BasicsBayes}

##if they are doing basic statistics so equals 1 how many times do they mention outliers yes/no by year 
library(reshape)
analyses = master[ , c(3, 4, 10, 20:26)]
View(analyses)
longanalyses = melt(analyses,
                    id = c("time.pulled", "Type", "mention.outliers"))
colnames(longanalyses)[4:5] = c("analysis.type", "used")

longanalyses = subset(longanalyses, used == 1)

##create a data frame of the percentages
analysessummary = table(longanalyses$mention.outliers, longanalyses$time.pulled, longanalyses$analysis.type)
analysessummary = as.data.frame(analysessummary)
colnames(analysessummary) = c("mention.outliers", "time.pulled",  "analysis.type", "Freq")

analysespercent = group_by(analysessummary, time.pulled, analysis.type) %>%
  mutate(percent = Freq/sum(Freq)*100)

graphdata = subset(analysespercent, mention.outliers == "Yes")

#calculate CIs to add to graph
graphdata$sample = as.data.frame(table(longanalyses$time.pulled, longanalyses$analysis.type))$Freq
graphdata$SE = sqrt(graphdata$percent*(100-graphdata$percent)/graphdata$sample)

dotplot2 = ggplot(graphdata, aes(analysis.type, percent, color = time.pulled))
finalgraph = dotplot2 +
  geom_pointrange(aes(ymin=percent-1.96*SE, ymax=percent+1.96*SE))+ 
  cleanup +
  xlab("Type of Analysis") +
  ylab("Percent of Outlier Mentions") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  scale_color_manual(name = "Year", 
                     values = c("maroon", "gray")) + 
  coord_cartesian(ylim = c(0,50))

tiff(filename = "analyses_graph.tiff", res = 300, width = 15, 
     height = 12, units = 'in', compression = "lzw")
plot(finalgraph)
dev.off()

```

```{r 2-waytable}
table1<-table(master$time.pulled, master$mention.outliers)
chisq.test(table1)
table1

install.packages("vcd")
library(vcd)
assocstats(table1)
```


```{r 3-waytable}
#let's try a 3-way table
# 3-Way Frequency Table 
mytable <- xtabs(~time.pulled+mention.outliers+Type, data=master)
model2<-loglm(~time.pulled+mention.outliers+Type, data=mytable)
model2
ftable(mytable)


#try for journal?
#mytable2 <- xtabs(~time.pulled+mention.outliers+Journal, data=master)
#loglm(~time.pulled+mention.outliers+Journal, data=mytable2)
#ftable(mytable2)
#can't talk about this becasue of data sparsity
```

```{r graphanalysisbyreason}
library(reshape)
out.testing = subset(master, mention.outliers == "Yes")

##create reason columns
out.testing$part = 0
out.testing$part[ grep("part", out.testing$reason.code)] = 1
out.testing$stat = 0
out.testing$stat[ grep("stat", out.testing$reason.code)] = 1
out.testing$exp = 0
out.testing$exp[ grep("unus", out.testing$reason.code)] = 1
names(out.testing)
long.testing=out.testing[,c("time.pulled", "Type", "peopleor.data.points", "take.out.or.leave.in", "run.with.or.without", 
                            "part", "stat", "exp", "Basics", "ANOVA" , "Regression", "ChiSquare", "Nonparametric", 
                            "Modeling", "BayesOther" )]

long.testing.data=melt(long.testing, 
                    id = c("time.pulled", "Type", "peopleor.data.points", "take.out.or.leave.in", "run.with.or.without", 
                           "part", "stat", "exp"),
                    measured = c("Basics", "ANOVA" , "Regression", "ChiSquare", "Nonparametric", 
                                 "Modeling", "BayesOther"))
#View(choice.data)          
colnames(long.testing.data)=c("time.pulled", "Type", "peopleor.data.points", "take.out.or.leave.in", "run.with.or.without", 
                           "part", "stat", "exp", "analysis", "used")
View(long.testing.data)
long.testing.data = subset(long.testing.data, used == 1)

View(long.testing.data)

long.reason.data=long.testing.data[,c("time.pulled", "Type", "peopleor.data.points", "take.out.or.leave.in", "run.with.or.without",  
                                       "analysis", "part", "stat", "exp")]
long.reason.data=melt(long.reason.data,
                      id=c("time.pulled", "Type", "peopleor.data.points", "take.out.or.leave.in", "run.with.or.without",  
                           "analysis"),
                      measured=c("part", "stat", "exp"))
colnames(long.reason.data)=c("time.pulled", "Type", "peopleor.data.points", "take.out.or.leave.in", "run.with.or.without",  
                             "analysis", "reasion.type", "used")
View(long.reason.data)
long.reason.data = subset(long.reason.data, used == 1)
long.reason.data<-na.omit(long.reason.data)
 View(long.reason.data)

long.reason.data$reason=factor(long.reason.data$reasion.type)
reason.table.test = table( long.reason.data$reason, long.reason.data$analysis)
View(reason.table.test)

barplot(reason.table.test, col=c("purple", "blue", "green"), space=1, legend = rownames(reason.table.test))

```


## Data analysis

# Results
1) Do we mention outliers at all?
2) Does reporting change by time?
3) Does this (difference in) reporting of outliers have any relationship to the specific discipline, Journal, Type of analysis, or Sample size?
Investigating reporting of outliers, we find that in 2012, X% of articles--X% of experiments--mentioned outliers, while in 2017 X% of articles--X% of experiments--mentioned outliers. This represents a significant increase in mentions of outliers *chi^2* (`r chisq.test(table1)$Table[1, 2]`) = `r chisq.test(table1)$Table[1,1]`, *p* = `r chisq.test(table1)$Table[1,3]`. Further exploration reveals that differences? in reporting between years arise between fields (*chi^2* (`r model2$Table[3, 3]`) = `r model2$Table[3,2]`, *p* = `r model2$Table[3,3]`), which can be seen in \@ref(tab:journalprop).  

I want a graph here x=type of analysis, y=proportion using each code, with side-by-side bars for each code.

4) When we do talk about them, what rational do we use to take them out most often, and how does this vary by time, discipline, Journal, Type of analysis, or Sample size?
5) Do we remove people or data points, and how does this vary by time, discipline, Journal, Type of analysis, or Sample size?
6) Do we test with and without them to determine if they should be taken out, and how does this vary by time, discipline, Journal, Type of analysis, or Sample size?

# Discussion
While modest improvments in reporting can be seen in some fields, overall the outlook is still bleak. 
Some may argue that use of the precious word limit dictated by journals to describe such acts as outlier identification and handling may be irrational. However, we contest that given the current availability and use of online supplements and appendixes, as well as the invent of the Open Science Framework (OSF; cite) which allows researchers the ability to upload any number of additional resources and supplements that can be easily referenced in their manuscripts in as little as 4 words: See supplement at osf.io/52mqw.



\newpage

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}


