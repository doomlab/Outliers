---
title             : "Have researchers increased reporting of outliers in response to the reproducability crisis?"
shorttitle        : "Outlier reporting"

author: 
  - name          : "K. D. Valentine"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "210 McAlester Hall, Columbia, MO, 65211"
    email         : "kdvdnf@mail.missouri.edu"
  - name          : "Erin M. Buchanan"
    affiliation   : "2"
  - name          : "Arielle Cunningham"
    affiliation   : "2"
  - name          : "Tabetha Hopke"
    affiliation   : "2"
  - name          : "Addie Wikowsky"
    affiliation   : "2"
  - name          : "Haley Wilson"
    affiliation   : "2"
    
affiliation:
  - id            : "1"
    institution   : "University of Missouri"
  - id            : "2"
    institution   : "Missouri State University"

author_note: |
 K. D. Valentine is a Ph.D. candidate at the University of Missouri. Erin M. Buchanan is an Associate Professor of Quantitative Psychology at Missouri State University. ADD HERE STUFF AND THINGS. 
  
abstract: |
  Psychology is currently experiencing a "renaissance" where replication and reproducibility of published reports is at the forefront of the field. While researchers have worked to discuss possible problems and solutions, work has yet to uncover how this new culture may alter reporting practices in Psychology. As outliers can bias both descriptive and inferential statistics, the examination for these data points is essential to any analysis using these parameters. We quantified the rates of reporting of outliers within psychology at two time points: 2012 when the replication crisis was born, and 2017, after the publication of reports concerning replication, questionable research practices, and transparency. A total of 2234 experiments were identified and analyzed, finding an increase in reporting of outliers from only 15.7% of experiments mentioning outliers in 2012 to 25.1% in 2017. We further delve into differences across years given the psychological field or statistical analysis that experiment employed. Further, we inspect whether outliers mentioned are people or data points, and what reasons authors gave for stating the observation was deviant. We conclude that while report rates are improving overall, there is still room for improvement in the reporting practices of psychological scientists which can only aid in strengthening our science.
  
keywords          : "outlier, influential observation, replication"

bibliography      : ["outliers.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
replace_ampersands: yes
csl               : apa6.csl
---

```{r load_packages, include = FALSE}
library("papaja")
library(knitr)
library(dplyr)
library(lme4)
library(ggplot2)
library(reshape)
knitr::opts_chunk$set(cache = TRUE)

##load the dataset
master = read.csv("outliers complete.csv")

##graph clean up code
cleanup = theme(panel.grid.major = element_blank(), 
              panel.grid.minor = element_blank(), 
              panel.background = element_blank(), 
              axis.line = element_line(colour = "black"), 
              legend.key = element_rect(fill = "white"),
              text = element_text(size = 15))
```

Psychology is undergoing a "renaissance" in which focus has shifted to the replication and reproducibility of current published reports [@Nelson2018; @Etz2016; @Lindsay2015; @ScienceCollaboration2015; @VanElk2015]. A main concern has been the difficulty in replicating phenomena, often attributed to publication bias [@Ferguson2012a], the use and misuse of *p*-values [@Gigerenzer2004; @Ioannidis2005], and researcher degrees of freedom [@Simmons2011]. In particular, this analysis focused on one facet of questionable research practices that affect potential replication (QPRs), specifically, the selective removal or inclusion of data points. 

As outlined by @Nelson2018, the social sciences turned inward to examine its practices due to the publication of unbelievable data [@Wagenmakers2011a], academic fraud [@Simonsohn2013], failures to replicate important findings [@Doyen2012], and the beginning of the Open Science Framework [@CenterforOpenScience]. These combined forces led to the current focus on QRPs and *p*-hacking and the investigation potential solutions to these problems. Recommendations included integrating effect sizes into results [@Cumming2008; @Lakens2013], implementing full disclosure and encouraging researchers to be transparent about their research practices, including not only the design and execution of their experiments, but especially the data preparation and resulting analyses [@Simmons2011], attempting and interpreting well thought out replication studies [@Asendorpf2012; @Maxwell2015], altering the way we think about *p*-values [@Benjamin2018; @Lakens2018; @Valentine], and restructuring incentives [@Nosek2012c]. Additionally, @Klein2014c developed the Many Labs project to aid in data collection for increased power, while the @ScienceCollaboration2015 published their findings from a combined many labs approach about the replication of phenomena in psychology. 

 While we have seen vast discussion of the problems and proposed solutions, research has yet to determine how this new culture may have impacted reporting practices of researchers. Herein, we aim specifically to quantify the rates of reporting of outliers within psychology at two time points: 2012 when the replication crisis was born [@Pashler2012a], and 2017, after the publication of reports concerning QPRs, replication, and transparency [@Miguel2014]. 

## Outliers

Bernoulli first mentioned outliers in 1777 starting the long history of examining for discrepant observations [@Bernoulli1777], which can bias both descriptive and inferential statistics [@Cook1980; @Stevens1984]. Therefore, the examination for these data points is essential to any analysis using these parameters, as outliers can impact study results. Outliers have been defined as influential observations or fringliers but specifically we use the definition of "an observation which being atypical and/or erroneous deviates decidedly from the general behavior of experimental data with respect to the criteria which is to be analyzed on it" [@Munoz-garcia1990, pg 217]. However, the definition of outliers can vary from researcher to researcher, as a wide range of graphical and statistical options are available for outlier detection [@Beckman1983; @Hodge2004; @Orr1991; @Osborne2004]. For example, @Tabachnick2012 outline several of the most popular detection methods including visual data inspection, residual statistics, a set number of standard deviations, Mahalanobis distance, Leverage, and Cook's distances. Before the serious focus on QRPs, the information regarding outlier detection as part of data screening was often excluded from publication, particularly if a journal page limit requirement needed to be considered. Consider, for example, THIS WAS THE WRONG STUDY/FIND THE RIGHT STUDY, who inspected 100 Industrial/Organizational Psychology personnel studies and found no mention of outliers. 

However, outlier detection and removal is likely part of a researchers data screening procedure, even if it does not make the research publication. @LeBel2013  that 11% of psychology researchers stated that they had not reported excluding participants for being outliers in their papers. @Fiedler2016 suggested that more than a quarter of researchers decide whether to exclude data only after looking at the impact of doing so. @Bakker2014 investigated the effects of outliers on published analyses, and while they did not find that they affected the surveyed results, they do report that these findings are likely biased by the non-reporting of data screening procedures, as sample sizes and degrees of freedom often did not match. The lack of transparency in data manipulation and reporting is problematic. 

By keeping outliers in a dataset, analyses are more likely to have increased error variance [depending on sample size, @Orr1991], biased estimates [Osborne2004], and reduced effect size and power [@Orr1991; @Osborne2004], which can alter the results of the analysis and lead to falsely supporting (Type I error), or denying a claim (Type II error). Inconsistencies in the treatment and publication of outliers could also lead to the failures to replicate previous work, as it would be difficult to replicate analyses that have been *p*-hacked into "just-significant" results [@Nelson2018; @Leggett]. The influence of this practice can be wide spread, as non-reporting of data manipulation can negatively affect meta-analyses, effect size, and sample size estimates for study planning. On the other hand, outliers do not always need to be seen as nuisance, as they will often be informative to researchers as they can encourage the diagnosis, change, and evolution of a research model [@Beckman1983]. Taken together, a lack of reporting of outlier practices can lead to furthering unwarranted avenues of research, ignoring important information, creating erroneous theories, and failure to replicate, all of which serve to weaken the sciences. Clarifying the presence or absence of outliers, how they were assessed, and how they were handled, can improve our transparency and replicability, and ultimately help to strengthen our science. 

The current zeitgeist of increased transparency and reproducibility applies not only to the manner in which data is collected, but also the various ways the data is transformed, cleaned, pared down, and analyzed. Therefore, it can be argued that it is just as important for a researcher to state how they identified outliers within their data, how the outliers were handled, and how this choice of handling impacted the estimates and conclusions of their analyses, as it is for them to report their sample size. Given the timing of the renaissance, we expected to find a positive change in reporting ratings for outliers in 2017, as compared to 2012. This report spans a wide range of psychological sub-domains, however, we also expected the impact of the @ScienceCollaboration2015 publication to affect social and cognitive psychology more than other fields. 

# Method

## Fields

A list of psychological sub-domains was created to begin the search for appropriate journals to include. The authors brainstormed the list of topics (shown in Table \@ref(tab:info-table)) by first listing major research areas in psychology (i.e., cognitive, clinical, social, etc.). Second, a list of common courses offered at large universities was consulted to add to the list of fields. Lastly, the American Psychological Association's list of divisions was examined for any potential missed fields. The topic list was created to capture large fields of psychology with small overlap (i.e., cognition and neuropsychology) while avoiding specific sub-fields of topics (i.e., cognition, perception, and memory). Sixteen fields were initially identified, however only thirteen were included in final analysis due to limitations noted below. 

## Journals

Once these fields were agreed upon, researchers used various search sources (Google, EBSCO host databases) to find journals that were dedicated to each broad topic. Journals were included if they appeared to publish a wide range of articles within the selected fields. A list of journals, publishers, and impact factors (as noted by each journals website in Spring of 2013 and 2018) were identified for each field. Two journals from each field were selected based on the following criteria: 1) high impact factors over one at minimum, 2) a mix of publishers, if possible, and 3) availability due to university resources. These journals are shown in the online supplemental materials at https://osf.io/52mqw/.

## Articles

Fifty articles from each journal were examined for data analysis: 25 articles were collected beginning in Spring 2013 for 2012 and in Fall 2017. Data collection of articles started at the last volume publication from the given year (2012 or 2017) and progressed backwards until 25 articles had been found. We excluded online first publications and started in 2012 to ensure time for errata and retraction of articles. Articles were included if they met the following criteria: 1) included data analyses, 2) included multiple participants or data-points, and 3) analyses were based on human subjects or stimuli. Therefore, we excluded theory articles, animal populations, and single subject designs. Based on review for the 2012 articles, three fields were excluded. Applied Behavior Analysis articles predominantly included single-subject designs, evolutionary psychology articles were primarily theory articles, and statistics related journal articles were based on user created data with specific set characteristics. Since none of these themes fit into our analysis of understanding data screening with human subject samples, we excluded those three fields from analyses.

## Data Processing

Each article was then reviewed for key components of data analysis. Each experiment in an article was coded separately. For each experiment, the type of analysis conducted, number of participants/stimuli analyzed, and whether or not they made any mention of outliers were coded. 

### Analysis types

Types of analyses were broadly defined as basic statistics (descriptive statistics, *z*-scores, *t*-tests, and correlations), ANOVAs, regressions, chi-squares, non-parametric statistics, modeling, and Bayesian/other analyses. 

### Outlier coding

For outliers, we used a dichotomous yes/no for if they were mentioned in an article. Outliers were not limited to simple statistical analysis of discrepant responses, but we also checked for specific exclusion criteria that were not related to missing data or study characteristics (i.e., we did not consider it an outlier if they were only looking for older adults). If so, we coded information about outliers into four types: 1) people, 2) data points, 3) both, or 4) none found. The distinction between people and data points was if individual trials were eliminated or if entire participant data was eliminated. We found that a unique code for data points was important for analyses with response time studies where individual participants were not omitted but rather specific data trials were eliminated. 

Then, for those articles that mentioned outliers, the author's decision for how to handle the outliers was coded into whether they removed participants/stimuli, left these outliers in the analysis, or winsorized the data points. Experiments were coded for whether they tested the analyses with, without, or both for determination of their effect on the study. If they removed outliers, a new sample size was recorded; although, this data was not analyzed, as we determined it was conflated with removal of other types of data unrelated to the interest of this paper (i.e., missing data). Lastly, we coded the reasoning for outlier detection as one or more of the following: 1) Statistical reason (i.e., used numbers to define odd or deviant behavior in responding, such as *z*-score or Mahalanobis distance scores), 2) Participant error (i.e., failed attention checks, did not follow instructions, or low quality data because of participant problems), and 3) Unusable data (i.e., inside knowledge of the study or experimenter/technological problems).

# Results

## Data Analytic Plan
Because each article constituted multiple data points within the dataset which were each nested within a particular journal, a multilevel model (MLM) was used to control for correlated error (Gelman, 2006). Pinheiro, Bates, Debroy, Sarkar, and Team's (2017) *nlme* package in *R* was used to calculate these analyses. A maximum likelihood logistic multilevel model was used to examine how the year in which the experiment was published predicted the likelihood of mentioning outliers (yes/no) while including a random intercept for journal. This model was run over all of the data, as well as broken down for each field, as well as each type of analysis in order to glean a more detailed account of the effect of year on outlier reporting. Additionally, 3 MLMs were analyzed attempting to individually predict each outlier reason (i.e. statistical reason yes/no; unusable data yes/no; participant reason yes/no) given the year while including a random intercept for journal. All code and data can be viewed at osf.io/52mqw. We futher explored whether these outliers were people or data points, how outliers were handled, and the reasons data were named outliers with descriptive statistics. 


## Overall Outliers

```{r overallout, include = FALSE}
##clean up the mention column 
master$mention.outliers = factor(master$mention.outliers,
                      levels = c("no", "yes"),
                      labels = c("No", "Yes"))

##create a data frame of the percentages by type
oaoutsummary = table(master$mention.outliers, master$time.pulled)
oaoutsummary = as.data.frame(oaoutsummary)
colnames(oaoutsummary) = c("mention.outliers", "time.pulled", "Freq")

overalloutpercent = as.data.frame(group_by(oaoutsummary, time.pulled) %>%
  mutate(percent = Freq/sum(Freq)*100))

##examine the model
m1 <- glmer(mention.outliers ~ year  + (1 | Journal),
            data=master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1)

```

Data processing resulted in a total of `r nrow(master)` experiments being coded, `r nrow(master[ master$time.pulled == "2012" , ])` of which were from 2012 or prior, with the additional `r nrow(master[ master$time.pulled == "2017" , ])` being from 2017 or prior. Investigating reporting of outliers, we found that in 2012, `r printnum(overalloutpercent$percent[2], digits = 1)`% of experiments mentioned outliers, while in 2017 `r printnum(overalloutpercent$percent[4], digits = 1)`% of experiments mentioned outliers. Actual publication year was used to predict outlier mention (yes/no) with a random intercept for journal, as described above. We found that publication year predicted outlier mentions, *Z* = `r printnum(summary(m1)$coefficients[2,3], digits = 2)`, *p* `r printp(summary(m1)$coefficients[2,4])`. Each year, experiments were `r printnum(exp(summary(m1)$coefficients[2,1])*100-100, digits = 1)`% more likely to report outliers as the previous year. 

## Fields

```{r fieldout, include = FALSE}

##create a data frame of the percentages by time and type
outsummary = table(master$mention.outliers, master$time.pulled, master$Type)
outsummary = as.data.frame(outsummary)
colnames(outsummary) = c("mention.outliers", "time.pulled", "Type", "Freq")

outpercent = as.data.frame(group_by(outsummary, time.pulled, Type) %>%
  mutate(percent = Freq/sum(Freq)*100))
```

```{r field-mlm, include = FALSE}
clin.master=master[which(master$Type=="Clinical"),]
cog.master=master[which(master$Type=="Cognitive"),]
coun.master=master[which(master$Type=="Counseling"),]
dev.master=master[which(master$Type=="Developmental"),]
edu.master=master[which(master$Type=="Educational"),]
envi.master=master[which(master$Type=="Environmental"),]
for.master=master[which(master$Type=="Forensics"),]
io.master=master[which(master$Type=="IO"),]
meth.master=master[which(master$Type=="Methods"),]
neu.master=master[which(master$Type=="Neuro"),]
over.master=master[which(master$Type=="Overview"),]
soc.master=master[which(master$Type=="Social"),]
sport.master=master[which(master$Type=="Sports"),]

m1cl <- glmer(mention.outliers ~ year  + (1 | Journal),
            data=clin.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1cl)

m1cog <- glmer(mention.outliers ~ year  + (1 | Journal),
            data=cog.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1cog)

m1cou <- glmer(mention.outliers ~ year  + (1 | Journal),
            data=coun.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1cou)

m1dev <- glmer(mention.outliers ~ year  + (1 | Journal),
            data=dev.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1dev)

m1edu <- glmer(mention.outliers ~ year  + (1 | Journal),
            data=edu.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1edu)

m1envi <- glmer(mention.outliers ~ year  + (1 | Journal),
            data=envi.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1envi)

m1for <- glmer(mention.outliers ~ year  + (1 | Journal),
            data=for.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1for)

m1io <- glmer(mention.outliers ~ year  + (1 | Journal),
            data=io.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1io)

m1meth <- glmer(mention.outliers ~ year  + (1 | Journal),
            data=meth.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1meth)

m1neu <- glmer(mention.outliers ~ year  + (1 | Journal),
            data=neu.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1neu)

m1over <- glmer(mention.outliers ~ year  + (1 | Journal),
            data=over.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1over)

m1soc <- glmer(mention.outliers ~ year  + (1 | Journal),
            data=soc.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1soc)

m1sport <- glmer(mention.outliers ~ year  + (1 | Journal),
            data=sport.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1sport)
```

Further exploration reveals that differences in reporting between years arise between fields which can be seen in Table \@ref(tab:info-table). Figure \@ref(fig:type-graph) displays the percentage of outlier mentions of each field colored by year examined. A MLM was analyzed for each field using journal as a random intercept to determine the influence of year of publication on outlier reporting rates. Specifically, if we look at the change in reporting for each field analyzed at the level of the experiment, we find the largest changes in forensic (`r printnum(exp(summary(m1for)$coefficients[2,1])*100-100, digits = 1)`% more likely to report), social (`r printnum(exp(summary(m1soc)$coefficients[2,1])*100-100, digits = 1)`%), and I/O (`r printnum(exp(summary(m1io)$coefficients[2,1])*100-100, digits = 1)`%), followed by developmental (`r printnum(exp(summary(m1dev)$coefficients[2,1])*100-100, digits = 1)`%), counseling (`r printnum(exp(summary(m1cou)$coefficients[2,1])*100-100, digits = 1)`%), and cognitive (`r printnum(exp(summary(m1cog)$coefficients[2,1])*100-100, digits = 1)`%). In support of our hypothesis, we found that social and cognitive fields showed increases in their outlier reporting; however, it was encouraging to see positive trends in other fields as well. These analyses show that in some fields, including our overall and neurological fields, we found a decrease in reporting across years, although these changes were not significant. 

The analyses shown below were exploratory based on the findings when coding each experiment for outlier data. We explored the relationship of outlier reporting to the type of analysis used to support research hypotheses, reasons for why outliers were excluded, as well as the type of outlier excluded from the study.

```{r type-graph, echo = FALSE, fig.cap = "Percent of outlier mentions by sub-domain field and year examined. Error bars represent 95% confidence interval.", fig.height = 6, fig.width = 8}
graphdata = subset(outpercent, mention.outliers == "Yes")

#calculate CIs to add to graph
graphdata$sample = as.data.frame(table(master$time.pulled, master$Type))$Freq
graphdata$SE = sqrt(graphdata$percent*(100-graphdata$percent)/graphdata$sample)

dotplot = ggplot(graphdata, aes(Type, percent, color = time.pulled))
dotplot = dotplot +
  geom_pointrange(aes(ymin=percent-1.96*SE, ymax=percent+1.96*SE))+ 
  cleanup +
  xlab("Field of Journal") +
  ylab("Percent of Outlier Mentions") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  scale_color_manual(name = "Year", 
                     values = c("black", "gray")) + 
  coord_cartesian(ylim = c(0,75))
dotplot
```

```{r info-table, echo = FALSE, results = 'asis'}
tableprint = matrix(NA, nrow = 13, ncol = 8)

tableprint[ ,1] = levels(master$Type)
tableprint[ ,2] = printnum(graphdata$percent[graphdata$time.pulled == "2012"], digits = 1)
tableprint[ ,3] = printnum(graphdata$sample[graphdata$time.pulled == "2012"], digits = 0)
tableprint[ ,4] = printnum(graphdata$percent[graphdata$time.pulled == "2017"], digits = 1)
tableprint[ ,5] = printnum(graphdata$sample[graphdata$time.pulled == "2017"], digits = 0)
tableprint[ , 6] = printnum(c(exp(summary(m1cl)$coefficients[2,1]), 
                     exp(summary(m1cog)$coefficients[2,1]),
                     exp(summary(m1cou)$coefficients[2,1]),
                     exp(summary(m1dev)$coefficients[2,1]),
                     exp(summary(m1edu)$coefficients[2,1]),
                     exp(summary(m1envi)$coefficients[2,1]),
                     exp(summary(m1for)$coefficients[2,1]),
                     exp(summary(m1io)$coefficients[2,1]),
                     exp(summary(m1meth)$coefficients[2,1]),
                     exp(summary(m1neu)$coefficients[2,1]),
                     exp(summary(m1over)$coefficients[2,1]),
                     exp(summary(m1soc)$coefficients[2,1]),
                     exp(summary(m1sport)$coefficients[2,1])), digits = 2)
tableprint[ , 7] = printnum(c(summary(m1cl)$coefficients[2,3], 
                     summary(m1cog)$coefficients[2,3],
                     summary(m1cou)$coefficients[2,3],
                     summary(m1dev)$coefficients[2,3],
                     summary(m1edu)$coefficients[2,3],
                     summary(m1envi)$coefficients[2,3],
                     summary(m1for)$coefficients[2,3],
                     summary(m1io)$coefficients[2,3],
                     summary(m1meth)$coefficients[2,3],
                     summary(m1neu)$coefficients[2,3],
                     summary(m1over)$coefficients[2,3],
                     summary(m1soc)$coefficients[2,3],
                     summary(m1sport)$coefficients[2,3]), digits = 2)
tableprint[ , 8] = printp(c(summary(m1cl)$coefficients[2,4], 
                     summary(m1cog)$coefficients[2,4],
                     summary(m1cou)$coefficients[2,4],
                     summary(m1dev)$coefficients[2,4],
                     summary(m1edu)$coefficients[2,4],
                     summary(m1envi)$coefficients[2,4],
                     summary(m1for)$coefficients[2,4],
                     summary(m1io)$coefficients[2,4],
                     summary(m1meth)$coefficients[2,4],
                     summary(m1neu)$coefficients[2,4],
                     summary(m1over)$coefficients[2,4],
                     summary(m1soc)$coefficients[2,4],
                     summary(m1sport)$coefficients[2,4]))

apa_table.latex(as.data.frame(tableprint),
                caption = "Outlier Reporting by Field Across Years",
                align = c("l", rep("c",7)),
                escape = FALSE,
                col.names = c("Field", "\\% 12", "$N$ 12", "\\% 17", 
                              "$N$ 17", "OR", "$Z$", "$p$"))
```

## Analyses Type

```{r analysisout, include=FALSE}
##create longdata set for analysis
long.test=master[,c("Journal","year", "time.pulled", "Type", "mention.outliers", "peopleor.data.points",
                    "take.out.or.leave.in", "run.with.or.without", 
                    "Basics", "ANOVA" , "Regression", "ChiSquare", "Nonparametric", 
                    "Modeling", "BayesOther" )]

long.test.data=melt(long.test, 
                    id = c("Journal","year", "time.pulled", "Type",
                           "mention.outliers", "peopleor.data.points", "take.out.or.leave.in", "run.with.or.without"),
                    measured = c("Basics", "ANOVA" , "Regression", "ChiSquare", "Nonparametric", 
                                 "Modeling", "BayesOther"))
colnames(long.test.data)=c("Journal","year", "time.pulled", 
                           "Type","mention.outliers", "peopleor.data.points", "take.out.or.leave.in",
                           "run.with.or.without", "analysis", "used")
##only use times that they used it or we are doubling up 
long.test.data = subset(long.test.data, used == 1)

##create a data frame of the percentages
analysessummary = table(long.test.data$mention.outliers, long.test.data$time.pulled, long.test.data$analysis)
analysessummary = as.data.frame(analysessummary)
colnames(analysessummary) = c("mention.outliers", "time.pulled",  "analysis.type", "Freq")

analysespercent = as.data.frame(group_by(analysessummary, time.pulled, analysis.type) %>%
  mutate(percent = Freq/sum(Freq)*100))
```

```{r analyses-graph, echo = FALSE, fig.cap = "Percent of outlier mentions by analysis type and year examined. Error bars represent 95% confidence interval.", fig.height = 6, fig.width = 8}
##only pull mention outliers
graphdata = subset(analysespercent, mention.outliers == "Yes")

#calculate CIs to add to graph
graphdata$sample = as.data.frame(table(long.test.data$time.pulled, long.test.data$analysis))$Freq
graphdata$SE = sqrt(graphdata$percent*(100-graphdata$percent)/graphdata$sample)

dotplot2 = ggplot(graphdata, aes(analysis.type, percent, color = time.pulled))
dotplot2 = dotplot2 +
  geom_pointrange(aes(ymin=percent-1.96*SE, ymax=percent+1.96*SE))+ 
  cleanup +
  xlab("Type of Analysis") +
  ylab("Percent of Outlier Mentions") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  scale_color_manual(name = "Year", 
                     values = c("black", "gray")) + 
  coord_cartesian(ylim = c(0,50))
dotplot2
```

```{r analysis-mlm, include=FALSE}
#now need to see if they vary by analysis
basic.data=long.test.data[which(long.test.data$analysis=="Basics"),]
anova.data=long.test.data[which(long.test.data$analysis=="ANOVA"),]
reg.data=long.test.data[which(long.test.data$analysis=="Regression"),]
chi.data=long.test.data[which(long.test.data$analysis=="ChiSquare"),]
nonpar.data=long.test.data[which(long.test.data$analysis=="Nonparametric"),]
model.data=long.test.data[which(long.test.data$analysis=="Modeling"),]
bo.data=long.test.data[which(long.test.data$analysis=="BayesOther"),]

m1basic <- glmer(mention.outliers ~ year + (1 | Journal),
            data=basic.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1basic)

m1anova <- glmer(mention.outliers ~ year + (1 | Journal),
            data=anova.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1anova)

m1reg <- glmer(mention.outliers ~ year + (1 | Journal),
            data=reg.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1reg)

m1chi <- glmer(mention.outliers ~ year + (1 | Journal),
            data=chi.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1chi)

m1non <- glmer(mention.outliers ~ year + (1 | Journal),
            data=nonpar.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1non)

m1model <- glmer(mention.outliers ~ year + (1 | Journal),
            data=model.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1model)

m1bo <- glmer(mention.outliers ~ year + (1 | Journal),
            data=bo.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1bo)
```

```{r analysis-table, echo = FALSE, results = 'asis'}
tableprint = matrix(NA, nrow = 7, ncol = 8)

tableprint[ , 1] = c("Basic Statistics", "ANOVA", "Regression", "Chi-Square",
                    "Non-Parametric", "Modeling", "Bayesian or Other")
tableprint[ ,2] = printnum(graphdata$percent[graphdata$time.pulled == "2012"], digits = 1)
tableprint[ ,3] = printnum(graphdata$sample[graphdata$time.pulled == "2012"], digits = 0)
tableprint[ ,4] = printnum(graphdata$percent[graphdata$time.pulled == "2017"], digits = 1)
tableprint[ ,5] = printnum(graphdata$sample[graphdata$time.pulled == "2017"], digits = 0)
tableprint[ , 6] = printnum(c(exp(summary(m1basic)$coefficients[2,1]), 
                     exp(summary(m1anova)$coefficients[2,1]),
                     exp(summary(m1reg)$coefficients[2,1]),
                     exp(summary(m1chi)$coefficients[2,1]),
                     exp(summary(m1non)$coefficients[2,1]),
                     exp(summary(m1model)$coefficients[2,1]),
                     exp(summary(m1bo)$coefficients[2,1])), digits = 2)
tableprint[ , 7] = printnum(c(summary(m1basic)$coefficients[2,3], 
                     summary(m1anova)$coefficients[2,3],
                     summary(m1reg)$coefficients[2,3],
                     summary(m1chi)$coefficients[2,3],
                     summary(m1non)$coefficients[2,3],
                     summary(m1model)$coefficients[2,3],
                     summary(m1bo)$coefficients[2,3]), digits = 2)
tableprint[ , 8] = printp(c(summary(m1basic)$coefficients[2,4], 
                     summary(m1anova)$coefficients[2,4],
                     summary(m1reg)$coefficients[2,4],
                     summary(m1chi)$coefficients[2,4],
                     summary(m1non)$coefficients[2,4],
                     summary(m1model)$coefficients[2,4],
                     summary(m1bo)$coefficients[2,4]))
apa_table.latex(as.data.frame(tableprint),
                caption = "Outlier Reporting by Analysis Type Across Years",
                align = c("l", rep("c",7)),
                escape = F,
                col.names = c("Analysis", "\\% 12", "$N$ 12", "\\% 17", 
                              "$N$ 17", "OR", "$Z$", "$p$"))
```

Table \@ref(tab:analysis-table) indicates the types of analyses across years that mention outliers, and Figure \@ref(fig:analyses-graph) visually depicts these findings. An increase in reporting was found for non-parametric statistics (`r printnum(exp(summary(m1non)$coefficients[2,1])*100-100, digits = 1)`%), basic statistics (`r printnum(exp(summary(m1basic)$coefficients[2,1])*100-100, digits = 1)`%), regression (`r printnum(exp(summary(m1reg)$coefficients[2,1])*100-100, digits = 1)`%), ANOVA (`r printnum(exp(summary(m1anova)$coefficients[2,1])*100-100, digits = 1)`%), and modeling (`r printnum(exp(summary(m1model)$coefficients[2,1])*100-100, digits = 1)`%). Bayesian and other statistics additionally showed a comparable increase, `r printnum(exp(summary(m1bo)$coefficients[2,1])*100-100, digits = 1)`%, which was not a significant change over years. 


## Type of Outlier

```{r typeout, include=FALSE}
peep_data = subset(master, mention.outliers == "Yes")

##create a data frame of the percentages
peep_datasummary = table(peep_data$peopleor.data.points)

peep_datasummary = as.data.frame(peep_datasummary)
colnames(peep_datasummary) = c("code", "Freq")

peep_datapercent = as.data.frame(group_by(peep_datasummary) %>%
  mutate(percent = Freq/sum(Freq)*100))
```

```{r type-mlm, include=FALSE}
##people versus no outliers
people = subset(master, peopleor.data.points == "people" | peopleor.data.points == "")
datapoints = subset(master, peopleor.data.points == "data points" | peopleor.data.points == "")

m1people <- glmer(mention.outliers ~ year + (1 | Journal),
            data=people, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1people)

m1dp <- glmer(mention.outliers ~ year + (1 | Journal),
            data=datapoints, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1dp)

whatdid = table(master$run.with.or.without[master$run.with.or.without !=""])/sum(table(master$run.with.or.without[master$run.with.or.without !=""]))*100
```

In our review, the majority of outliers mentioned referred to people (`r printnum(peep_datapercent$percent[5], digits = 1)`%) as opposed to data points (`r printnum(peep_datapercent$percent[3], digits = 1)`%), or both people and data points (`r printnum(peep_datapercent$percent[2], digits = 1)`%), and a final (`r printnum(peep_datapercent$percent[4], digits = 1)`%) of experiments mentioned outliers but did not specify a type, just that they found none. The trends across years were examined for mentioning outliers (yes/no) for both people and data points, dropping the both and none found categories due to small size. Therefore, the dependent variable was outlier mention where the "yes" category indicated either the people or data point categories separately. The mentions of excluding participants increased across years, `r printnum(abs(exp(summary(m1people)$coefficients[2,1])*100-100), digits = 1)`%, *Z* = `r printnum(summary(m1people)$coefficients[2,3], digits = 2)`, *p* `r printp(summary(m1people)$coefficients[2,4])`, while the mention of data point exclusion was consistent across years, `r printnum(abs(exp(summary(m1dp)$coefficients[2,1])*100-100), digits = 1)`%, *Z* = `r printnum(summary(m1dp)$coefficients[2,3], digits = 2)`, *p* = `r printp(summary(m1dp)$coefficients[2,4])`. When handling these data, some experiments chose to winzorize the data (`r printnum(whatdid["winsorized"], digits = 1)`%), most analyzed the data without the observations (`r printnum(whatdid["without"], digits = 1)`%), some analyzed the data with the observations (`r printnum(whatdid["with"], digits = 1)`%), and some conducted analyses both with and without the observations (`r printnum(whatdid["both"], digits = 1)`%). 

## Reasons

```{r reasonsout, include=FALSE}
table(master$reason.code)
reasons = subset(master, mention.outliers == "Yes")

##create reason columns
reasons$part = 0
reasons$part[ grep("part", reasons$reason.code)] = 1
reasons$stat = 0
reasons$stat[ grep("stat", reasons$reason.code)] = 1
reasons$unus = 0
reasons$unus[ grep("unus", reasons$reason.code)] = 1

##create a data frame of the percentages
reasonssummary = table(reasons$part)
reasonssummary = as.data.frame(reasonssummary)
colnames(reasonssummary) = c("yesno", "part.freq")
reasonssummary$stat.freq = as.data.frame(table(reasons$stat))$Freq
reasonssummary$unus.freq = as.data.frame(table(reasons$unus))$Freq

reasonspercent = as.data.frame(group_by(reasonssummary) %>%
  mutate(part.percent = part.freq/sum(part.freq)*100) %>%
  mutate(stat.percent = stat.freq/sum(stat.freq)*100) %>%
  mutate(unus.percent = unus.freq/sum(unus.freq)*100))
```

```{r reason-mlm, include=FALSE}
reasondata = master[ master$mention.outliers == "Yes", c("Journal", "year", "reason.code")]

##create reason columns
reasondata$part = 0
reasondata$part[ grep("part", reasondata$reason.code)] = 1
reasondata$stat = 0
reasondata$stat[ grep("stat", reasondata$reason.code)] = 1
reasondata$unus = 0
reasondata$unus[ grep("unus", reasondata$reason.code)] = 1

m1part <- glmer(part ~ year + (1 | Journal),
            data=reasondata, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1part)

m1stat <- glmer(stat ~ year + (1 | Journal),
            data=reasondata, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1stat)

m1unus <- glmer(unus ~ year + (1 | Journal),
            data=reasondata, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1unus)
```

We found that researchers often used multiple criterion checks for outlier coding, as one study might exclude participants for exceeding a standard deviation cut-off, while also excluding participants for low effort data. Therefore, reason coding was not unique for each experiment, and each experiment could have one to three reasons for data exclusion. Statistical reasoning was the largest reported exclusion criteria of papers that mentioned outliers at `r printnum(reasonspercent$stat.percent[2], digits = 1)`%. Next, participant reasons followed with `r printnum(reasonspercent$part.percent[2], digits = 1)`% of outlier mentions, and unusable data was coded in `r printnum(reasonspercent$unus.percent[2], digits = 1)`% of experiments that mentioned outliers. To examine the trend over time, we used a similar MLM analysis as described in the our data analytic plan, with Journal as a random intercept, year as the independent variable, and the mention of type of outlier (yes/no for participant, statistical, and unusable data) as the dependent variables separately. Statistical reasons decreased by `r printnum(abs(exp(summary(m1stat)$coefficients[2,1])*100-100), digits = 1)`%, *Z* = `r printnum(summary(m1stat)$coefficients[2,3], digits = 2)`, *p* = `r printp(summary(m1stat)$coefficients[2,4])`. Participant reasons increased over time by `r printnum(abs(exp(summary(m1part)$coefficients[2,1])*100-100), digits = 1)`%, *Z* = `r printnum(summary(m1part)$coefficients[2,3], digits = 2)`, *p* = `r printp(summary(m1part)$coefficients[2,4])`. Unusable data increased by about `r printnum(abs(exp(summary(m1unus)$coefficients[2,1])*100-100), digits = 1)`%, *Z* = `r printnum(summary(m1unus)$coefficients[2,3], digits = 2)`, *p* = `r printp(summary(m1unus)$coefficients[2,4])`. 

# Discussion

We hypothesized that report rates for outliers would increase overall in experiments from 2012 to 2017, and we largely we found support for this hypothesis. We additionally hypothesized larger increases in report rates of outliers for the domains of social and cognitive psychology. This, too, bore out within our results. While modest improvements in reporting can be seen in almost all fields, it is worthwhile to note that in 2017 the average proportion of experiments reporting outliers was still only x%, with some fields as low as x%. While the effort of many fields should not be overlooked, we suggest that there is still a large amount of room for improvement overall.

Further investigation into the rates of reporting over time broken down by analysis revealed that all analytic techniques showed increased reporting over time, ranging from 11.7% for modeling to 38.2% for nonparametric statistics. Of all outliers reported, we found that the majority discussed were people (65.9%), and that while exclusion of people as outliers increased from 2012 to 2017, exclusion of outlying data points remained consistent across time. Interestingly, most experiments cited outliers as those found through statistical means (e.g. mahalanobis distance, leverage, or the 3 standard deviation rule) and/or participant reasons (e.g. failed attention checks or failure to follow instructions), but a small subset were cited as unusable data (e.g. individuals who believed the procedure was staged or participants whose position in a room was never recorded by the experimenter). These findings suggest that not only is discussion of outliers important for the study at hand, but also for future studies. Given insight into ways data can become unusable, a researcher is better equipped to prepare for and deter unusable data from arising in future studies through knowledge of past failures that can improve their research design. 

While there may be many reasons an individual experiment does not speak to outliers (ERIN PUT AN EXAMPLE HERE), we believe that given the frequentist nature of most psychological work there are many more reasons that an experiment should take the time and word length to express the presence or absence of these data. Some may argue that use of the precious word limit dictated by journals to describe such choices as identification and handling of outliers may be irrational. However, we contest that given the current availability and use of online supplements and appendixes, as well as the invent of the Open Science Framework [@CenterforOpenScience] researchers now have the ability to upload any number of additional resources and supplements that can be easily referenced in manuscripts. This should assist researchers in moving detail-laden data cleaning procedures into a supplemental document, allowing for a simple statement that data cleaning was completed (leaving the details in the referenced documentation), and reporting the resultant sample size. This can help to alleviate a problem noted by @Bakker2014, that in many articles where no data removal was reported, the reported sample size was inconsistent with the reported degrees of freedom. 

We implore researchers not to overlook the importance of visualizing your data and identifying data—statically or otherwise—that may not fall within the expected range or pattern of the sample. Currently, there are a plethora of online tools that can assist even the most junior researcher in the cleaning of data, including outlier detection and handling, for almost any type of analysis. From online courses (cite) to YouTube videos that walk you through the process step-by-step (cite), we believe that if the failure to report these outliers is due to a lack of learning, that this could quickly be fixed on a case-by-case basis. Further, we implore those who are reviewers and editors to think critically about this idea. If no mention is made of outlier inspection within an article, perhaps it is worthwhile to ask the researchers why this was so. 

While these findings assist in showing the improvements made by this renaissance in psychology, we believe there is still much work to be done in this area. Future studies should inquire about other poor researcher behaviors (such as QRPs) and neglected hallmarks of psychology (such as replication). We believe that quantifying the changes within our science is worthwhile to show researchers that we are in a renaissance and are actively reshaping our practices for the better. More importantly, we believe that spotlighting those fields and researchers that are improving and taking new recommendations into consideration is one way to encourage future researchers to make strong, ethical research decisions. 


\newpage

# References


\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}


