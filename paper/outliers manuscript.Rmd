---
title             : "Have reserachers increased reporting of outliers in response to the reproducability crisis?"
shorttitle        : "Outlier reporting"

author: 
  - name          : "K. D. Valentine"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "210 McAlester Hall, Columbia, MO, 65211"
    email         : "kdvdnf@mail.missouri.edu"
  - name          : "Erin M. Buchanan"
    affiliation   : "2"

affiliation:
  - id            : "1"
       institution   : "University of Missouri"
  - id            : "2"
      institution   : "Missouri State University"

author_note: |
 K. D. Valentine is a Ph.D. candidate at the University of Missouri.  Erin M. Buchanan is an Associate Professor of Quantitative Psychology at Missouri State University. 
  
abstract: |
  
  
keywords          : "outlier, influential observation, replication"

bibliography      : ["q_bib.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
output            : papaja::apa6_pdf
relpace_ampersands: yes
csl               : apa6.csl
---

```{r load_packages, include = FALSE}
library("papaja")
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
master = read.csv("outliers complete.csv")
##experiment size
table(master$Journal, master$time.pulled)
table(master$Journal, master$year)

```

The focus on the lack of replication in psychology has raised concerns regarding the reproducibility and validity of current published reports (Etz & Vandekerckhove, 2016; Lindsay, 2015; Open Science Collaboration, 2015; van Elk et al., 2015). Researchers have attributed this lack of replication to many things, including publication bias (Brannick, 2012), the potential for the over-reliance, abuse, or misunderstanding of p-values (Gigerenzer, 2004; Ioannidis, 2005; Simmons, Nelson, & Simonsohn, 2011), and researcher degreed of freedom (Simmons, Nelson, & Simonsohn, 2011), including selective removal or inclusion of datapoints. 

In the years since the crisis began many recommendations have been made (Cumming, 2008; Simmons, Nelson, & Simonsohn, 2011; Asendorpf et al., 2013; Lakens, 2013; Benjamin et al., 2017; Lakens et al., 2018; Valentine, Buchanan, Scofield & Beauchamp, 2018; Maxwell, Lau, & Howard, 2015; Nosek, Spies, & Motyl, 2012), projects have been organized (Klein et al., 2014; Open Science Collaboration, 2015) and steps have been taken to assist in improving the reliability of psychological findings. These include an increased (encouragement/requirement) for researchers to be transparent about their research practices, including not only the design and execution of their experiments, but especially the data preparation and resulting analyses. While we have seen vast discussion of the problems and proposed solutions, research has yet to determine how this new culture of research has altered the actual reporting practices of researchers. Herein, we aim specifically to quantify the rates of reporting of outliers within psychology at two time points—in 2012, when the replication crisis was born (Pashler & Wagenmakers, 2012), and 5 years later, in 2017. 

# Outliers

Since Bernoulli first mentioned outliers in 1777, outliers, influential observations, or fringliers have been defined and redefined throughout the years (Beckman & Cook, 1983; Hodge & Austin, 2004; Munoz-Garcia, Moreno-Rebollo, Pascual-Acosta, 1990; Orr, Sackett, & Dubois, 1991; Osborne & Overbay, 2004), but for the purposes of this manuscript can be defined as, “an observation which being atypical and/or erroneous deviates decidedly from the general behavior of experimental data with respect to the criteria which is to be analyzed on it” (Munoz-Garcia, Moreno-Rebollo, Pascual-Acosta , 1990). While there are a plethora of graphical and statistical options available to researchers to identify and describe these datapoints (i.e. visual depictions of data and residuals, the 3 standard deviation rule, Mahalanobis or Cook’s distance, leverage, etc.; Tabachnick & Fidell, 2007), this discussion of outliers rarely makes it to the page. Consider, for example, Orr, Sackett, and Dubois (1991), who inspected 100 Industrial/Organizational Psychology personnel studies and found no mention of outliers. 

This is not to say that researchers are not taking these datapoints into consideration during their data cleaning and analysis plan, or even going so far as removing them. Work by LeBel and colleagues (2013) reported that 11% of psychology researchers stated that they had not reported excluding participants for being outliers in their papers, and Fiedler and Schwarz (2016) suggested that more than a quarter of researchers decide whether to exclude data only after looking at the impact of doing so. This is problematic. Reporting outlier removal or inclusion practices are vital. By keeping outliers in a dataset, analyses are more likely to have increased error variance (depending on sample size, Orr et al., 1991), biased estimates (Osborne, & Overbay, 2004), and reduced effect size and power (Orr, Sackett, & Dubois, 1991; Osborne, & Overbay 2004), which can alter the results of the analysis and lead to falsely supporting (Type I error), denying a claim (Type II error). This is especially concerning as inconsistencies in treatment of outliers can thus lead to failures to replicate previous work. Additionally, incorrect estimates of effect lead to misleading meta-analyses or sample size estimates for study planning. On the other hand, outliers do not always need to be seen as nuisance, as they will often be informative to researchers as they can encourage the diagnosis, change, and evolution of a research model (Beckman & Cook, 1983). Taken together, a lack of reporting of outlier practices can lead to furthering unwarranted avenues of research, ignoring important information, creating erroneous theories, and failure to replicate, all of which serve to weaken the sciences. Clarifying the presence or absence of outliers, how they were assessed, and how they were handled, can improve our transparency and replicability, and ultimately help to strengthen our science.  

The current zeitgeist of increased transparency and reproducibility applies not only to the manner in which data is collected, but also the various ways the data is transformed, cleaned, pared down, and analyzed. Therefore, it can be argued that it is just as important for a researcher to state how they identified outliers within their data, how the outliers were handled, and how this choice of handling impacted the estimates and conclusions of their analyses, as it is for them to report their sample size. 

# Method

## Fields

A list of psychological field areas was created to begin the search for appropriate journals to include.  The authors brainstormed the list of topics (shown in \@ref(tab:table)) by first listing major research areas in psychology (i.e. cognitive, clinical, social).  Second, a list of common courses offered at large universities was consulted to add to the list of fields.  Lastly, the American Psychological Association's list of divisions was examined for any potential missed fields. The topic list was created to capture large fields of psychology with small overlap (i.e. cognition and neuropsychology) while avoiding specific subfields of topics (i.e. cognition, perception, and memory). Sixteen fields were initially identified, however only 13 were included in final analysis due to limitations noted below. 

## Journals

Once these fields were agreed upon, researchers used various search sources (Google, EBSCO host databases) to find journals that were dedicated to each broad topic. Journals were included if they appeared to publish a wide range of articles within the selected fields. A list of journals, publishers, and impact factors (as noted by each journals website in Spring of 2013) were identified for each field.  Two journals from each field were selected based on the following criteria: 1) high impact factors, 2) impact factors over one at minimum, 3) a mix of publishers if possible, and 4) availability due to university resources.  These journals are shown in \@ref(tab:table).

##Articles
Fifty articles from each journal were examined for data analysis (25 articles were collected beginning in 2012 and 25 articles were collected beginning in 2017). Data collection of articles started at the last volume publication from the given year (2012 or 2017) and progressed backwards until 25 articles had been found. We excluded online first publications and started in 2012 to ensure time for errata and retraction of articles. Articles were including if they met the following criteria: 1) included data analyses, 2) included multiple participants or data-points, and 3) analyses were based on human subjects or stimuli. Therefore, we excluded theory articles, animal populations, and single subject designs. Based on review for the 2012 articles, three fields were excluded.  Applied behavior analysis articles predominantly included single-subject designs, evolutionary psychology articles were primarily theory articles, and statistics related journal articles were based on user created data with specific set characteristics.  Since none of these themes fit into our analysis of understanding data screening with human subject samples, we excluded those three fields from analyses.

## Data Processing

Each article was then reviewed for key components of data analysis.  Each experiment in an article was coded separately. For each experiment, the type of analysis conducted, number of participants/stimuli analyzed, and whether or not they made any mention of outliers were coded.  Types of analyses were broadly coded into basic statistics (descriptive statistics, z-scores, t-tests, and correlations), ANOVAs, regressions, chi-squares, nonparametric statistics, modeling, and Bayesian/other analyses. First, we coded if outliers were mentioned at all in article.  If so, we coded information about outliers into four types: 1) people, 2) data, 3) both, or 4) none found.  We found that a unique code for data was important for analyses with response time studies where individual participants were not omitted but rather specific data trials were eliminated.  Then, for those articles that mentioned outliers, the author's decision for how to handle the outliers was coded into whether they removed participants/stimuli, left these outliers in the analysis, or winsorized the datapoints. Experiments were additionally coded for whether they tested the analyses with, without, or both with and without these outliers for determination of their effect on the study.  If they removed outliers, a new sample size was recorded. Lastly, we coded the reasoning for outlier detection as one or more of the following: 1) Statistical reason (i.e. they used numbers to define odd or deviant behavior in responding, such as z-score or Mahalanobis distance scores), 2) Participant error (i.e. failed attention checks, did not follow instructions, or poor data because of participant problems), 3) Unusable data (inside knowledge of the study or experimenter/technological problems), or 4) no listed reason.



## List of Fields, Journals, and Impact Factors 2012

```{r table, echo = FALSE, results = 'asis'}
tableprint = matrix(NA, nrow = 31, ncol = 4)

tableprint[1, ] = c("Applied Behavior Analysis", "Journal of Experimental Analysis of Behavior", "Wiley", 1.39)

tableprint[2, ] = c("Applied Behavior Analysis", "Journal of Applied Behavior Analysis", "Wiley", 1.19)

tableprint[3, ] = c("Clinical", "Journal of Consulting and Clinical Psychology", "APA", 4.85)

tableprint[4, ] = c("Clinical", "Journal of Clinical Psychology", "Wiley", 2.12)

tableprint[5, ] = c("Cognitive", "Cognitive Psychology", "Elsevier", 4.27)

tableprint[6, ] = c("Cognitive", "Journal of Experimental Psychology: Learning, Memory, and Cognition", "APA", 2.85)

tableprint[7, ] = c("Counseling", "Journal of Counseling", "APA", 3.23)

tableprint[8, ] = c("Counseling", "The Counseling Psychologist", "Sage", 1.82)

tableprint[9, ] = c("Developmental", "Journal of Experimental Child Psychology", "Elsevier", 3.23)

tableprint[10, ] = c("Developmental", "Journal of Youth and Adolescence", "Springer", 2.72)

tableprint[11, ] = c("Educational", "Journal of Educational Psychology", "APA", 3.08)

tableprint[12, ] = c("Educational", "Contemporary Educational Psychology", "Elsevier", 2.20)

tableprint[13, ] = c("Environmental", "Journal of Environmental Psychology", "Elsevier", 2.93)

tableprint[14, ] = c("Environmental", "Environment and Behavior", "Sage", 1.27)

tableprint[15, ] = c("Evolutionary", "Evolution and Human Behavior", "Elsevier", 3.11)

tableprint[16, ] = c("Evolutionary", "Evolutionary Psychology", "Open Access", 1.06)

tableprint[17, ] = c("Forensics", "Psychology, Public Policy, and Law", "APA", 1.93)

tableprint[18, ] = c("Forensics", "Law and Human Bevhavior", "Spring", 2.16)

tableprint[19, ] = c("Industrial Organization", "Organizational Behavior and Human Decision Process", "Elsevier", 3.94)

tableprint[20, ] = c("Industrial Organization", "Personnel Psychology", "Wiley", 2.93)

tableprint[21, ] = c("Neurological/Physiological", "Neuropsychology", "APA", 3.82)

tableprint[22, ] = c("Neurological/Physiological", "Cognitive, Affective, and Behavioral Neuroscience", "Springer", 3.57)

tableprint[23, ] = c("Social", "Journal of Personality and Social Psychology", "APA", 5.08)

tableprint[24, ] = c("Social", "Journal of Experimental Social Psychology", "Elsevier", 2.31)

tableprint[25, ] = c("Sports", "Journal of Sport & Exercise Psychology", "Human Kinetics", 2.66)

tableprint[26, ] = c("Sports", "Sociology of Sport Journal", "Human Kinetics", 1.00)

tableprint[27, ] = c("Statistics", "Special Section of the Psychological Bulletin", "APA", 14.46)

tableprint[28, ] = c("Statistics", "Structural Equation Modeling", "Taylor & Francis", 4.71)

tableprint[29, ] = c("Overview", "Psychonomic Bulletin & Review", "Springer", 2.25)

tableprint[30, ] = c("Overview", "Psychonomic Science", "Sage", 4.43)

tableprint[31, ] = c("Overview", "Psychological Assessment", "APA", 2.99)

kable(tableprint, 
      digits = 3,
      col.names = c("Field", "Journal", "Publisher", "Impact Factor"))
```
*Note.* Impact factors as of time of initial data collection (Spring 2013).


```{r proportions}

table(master$mention.outliers)
master$mention.outliers = factor(master$mention.outliers,
                      levels = c("no", "yes"),
                      labels = c("No", "Yes"))

##create a data frame of the percentages
outsummary = table(master$mention.outliers, master$time.pulled, master$Type)
outsummary = as.data.frame(outsummary)
colnames(outsummary) = c("mention.outliers", "time.pulled", "Type", "Freq")

##here we want to calculate if they mention outliers by year and type
##want to focus on the yeses 
library(dplyr)
oaoutsummary = table(master$mention.outliers, master$time.pulled)
oaoutsummary = as.data.frame(oaoutsummary)
colnames(oaoutsummary) = c("mention.outliers", "time.pulled", "Freq")
overalloutpercent = group_by(oaoutsummary, time.pulled) %>%
  mutate(percent = Freq/sum(Freq)*100)

outpercent = group_by(outsummary, time.pulled, Type) %>%
  mutate(percent = Freq/sum(Freq)*100)

#kable(outpercent)
##in the outpercent table, you see the percent based on year and Type, so each yes/no combination adds up to one hundred. We are only interested in the percent of yeses.
```

```{r journalgraph}
library(ggplot2)

cleanup = theme(panel.grid.major = element_blank(), 
              panel.grid.minor = element_blank(), 
              panel.background = element_blank(), 
              axis.line = element_line(colour = "black"), 
              legend.key = element_rect(fill = "white"),
              text = element_text(size = 15))

graphdata = subset(outpercent, mention.outliers == "Yes")

#calculate CIs to add to graph
graphdata$sample = as.data.frame(table(master$time.pulled, master$Type))$Freq
graphdata$SE = sqrt(graphdata$percent*(100-graphdata$percent)/graphdata$sample)

dotplot = ggplot(graphdata, aes(Type, percent, color = time.pulled))
finalgraph = dotplot +
  geom_pointrange(aes(ymin=percent-1.96*SE, ymax=percent+1.96*SE))+ 
  cleanup +
  xlab("Field of Journal") +
  ylab("Percent of Outlier Mentions") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  scale_color_manual(name = "Year", 
                     values = c("maroon", "gray")) + 
  coord_cartesian(ylim = c(0,75))

tiff(filename = "mention_graph.tiff", res = 300, width = 8, 
     height = 6, units = 'in', compression = "lzw")
plot(finalgraph)
dev.off()
```

```{r journalprop}
library(MOTE)
graphdata$prop = graphdata$percent/100
types = levels(graphdata$Type)
for (i in 1:length(types))
{
  saved = d.prop(graphdata$prop[graphdata$Type==types[i]][1],
         graphdata$prop[graphdata$Type==types[i]][2],
         graphdata$sample[ graphdata$Type==types[i]][1], 
         graphdata$sample[ graphdata$Type==types[i]][2],
         a = .05) 
  print(c(types[i],saved$d))
}

```

```{r reason}
master$reason.code
table(master$reason.code)
reasons = subset(master, mention.outliers == "Yes")

##create reason columns
reasons$part = 0
reasons$part[ grep("part", reasons$reason.code)] = 1
reasons$stat = 0
reasons$stat[ grep("stat", reasons$reason.code)] = 1
reasons$exp = 0
reasons$exp[ grep("unus", reasons$reason.code)] = 1

##create a data frame of the percentages
reasonssummary = table(reasons$part, reasons$time.pulled, reasons$Type)
reasonssummary = as.data.frame(reasonssummary)
colnames(reasonssummary) = c("yesno", "time.pulled", "Type", "part.freq")
reasonssummary$stat.freq = as.data.frame(table(reasons$stat, reasons$time.pulled, reasons$Type))$Freq
reasonssummary$exp.freq = as.data.frame(table(reasons$exp, reasons$time.pulled, reasons$Type))$Freq

reasonspercent = group_by(reasonssummary, time.pulled, Type) %>%
  mutate(part.percent = part.freq/sum(part.freq)*100) %>%
  mutate(stat.percent = stat.freq/sum(stat.freq)*100) %>%
  mutate(exp.percent = exp.freq/sum(exp.freq)*100) 

#kable(reasonspercent)
#only include the percent of 1s

```

```{r Peopledata}

peep_data = subset(master, mention.outliers == "Yes")
table(peep_data$peopleor.data.points)

##create a data frame of the percentages
peep_datasummary = table(peep_data$peopleor.data.points, peep_data$time.pulled, peep_data$Type)
peep_datasummary = as.data.frame(peep_datasummary)
colnames(peep_datasummary) = c("code", "time.pulled", "Type", "Freq")

peep_datapercent = group_by(peep_datasummary, time.pulled, Type) %>%
  mutate(percent = Freq/sum(Freq)*100)
kable(peep_datapercent)
```

```{r BasicsBayes}

##if they are doing basic statistics so equals 1 how many times do they mention outliers yes/no by year 
library(reshape)
analyses = master[ , c(3, 4, 10, 20:26)]
#View(analyses)
longanalyses = melt(analyses,
                    id = c("time.pulled", "Type", "mention.outliers"))
colnames(longanalyses)[4:5] = c("analysis.type", "used")

longanalyses = subset(longanalyses, used == 1)

##create a data frame of the percentages
analysessummary = table(longanalyses$mention.outliers, longanalyses$time.pulled, longanalyses$analysis.type)
analysessummary = as.data.frame(analysessummary)
colnames(analysessummary) = c("mention.outliers", "time.pulled",  "analysis.type", "Freq")

analysespercent = group_by(analysessummary, time.pulled, analysis.type) %>%
  mutate(percent = Freq/sum(Freq)*100)

graphdata = subset(analysespercent, mention.outliers == "Yes")

#calculate CIs to add to graph
graphdata$sample = as.data.frame(table(longanalyses$time.pulled, longanalyses$analysis.type))$Freq
graphdata$SE = sqrt(graphdata$percent*(100-graphdata$percent)/graphdata$sample)

dotplot2 = ggplot(graphdata, aes(analysis.type, percent, color = time.pulled))
finalgraph = dotplot2 +
  geom_pointrange(aes(ymin=percent-1.96*SE, ymax=percent+1.96*SE))+ 
  cleanup +
  xlab("Type of Analysis") +
  ylab("Percent of Outlier Mentions") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  scale_color_manual(name = "Year", 
                     values = c("maroon", "gray")) + 
  coord_cartesian(ylim = c(0,50))

tiff(filename = "analyses_graph.tiff", res = 300, width = 15, 
     height = 12, units = 'in', compression = "lzw")
plot(finalgraph)
dev.off()

```

```{r 2-waytable}
table1<-table(master$time.pulled, master$mention.outliers)
stuff<-chisq.test(table1)
table1

```


```{r 3-waytable}
#let's try a 3-way table
# 3-Way Frequency Table 
#
library(MASS)
mytable <- xtabs(~time.pulled+mention.outliers+Type, data=master)
model2<-loglm(~time.pulled+mention.outliers+Type, data=mytable)
t.m.f=model2
ftable(mytable)



#try for journal?
#mytable2 <- xtabs(~time.pulled+mention.outliers+Journal, data=master)
#loglm(~time.pulled+mention.outliers+Journal, data=mytable2)
#ftable(mytable2)
#can't talk about this becasue of data sparsity


```

```{r graphanalysisbyreason}
out.testing = subset(master, mention.outliers == "Yes")
library(car)
master$mention.outliers2<-as.numeric(master$mention.outliers)
table(master$mention.outliers2)
master$mention.outliers2=recode(master$mention.outliers2, "1=0; 2=1")
table(master$mention.outliers2)
#0=no, 1=yes
#need to go back and put this var into these datasets so can analyze mlm**********************************
##create reason columns
out.testing$part = 0
out.testing$part[ grep("part", out.testing$reason.code)] = 1
out.testing$stat = 0
out.testing$stat[ grep("stat", out.testing$reason.code)] = 1
out.testing$exp = 0
out.testing$exp[ grep("unus", out.testing$reason.code)] = 1
names(out.testing)
long.testing=out.testing[,c("time.pulled", "Type", "peopleor.data.points", "take.out.or.leave.in", "run.with.or.without", 
                            "part", "stat", "exp", "Basics", "ANOVA" , "Regression", "ChiSquare", "Nonparametric", 
                            "Modeling", "BayesOther" )]

long.testing.data=melt(long.testing, 
                    id = c("time.pulled", "Type", "peopleor.data.points", "take.out.or.leave.in", "run.with.or.without", 
                           "part", "stat", "exp"),
                    measured = c("Basics", "ANOVA" , "Regression", "ChiSquare", "Nonparametric", 
                                 "Modeling", "BayesOther"))
#View(choice.data)          
colnames(long.testing.data)=c("time.pulled", "Type", "peopleor.data.points", "take.out.or.leave.in", "run.with.or.without", 
                           "part", "stat", "exp", "analysis", "used")
#View(long.testing.data)
long.testing.data = subset(long.testing.data, used == 1)

#View(long.testing.data)


long.reason.data=long.testing.data[,c("time.pulled", "Type", "peopleor.data.points", "take.out.or.leave.in", "run.with.or.without",  
                                       "analysis", "part", "stat", "exp")]
long.reason.data=melt(long.reason.data,
                      id=c("time.pulled", "Type", "peopleor.data.points", "take.out.or.leave.in", "run.with.or.without",  
                           "analysis"),
                      measured=c("part", "stat", "exp"))
colnames(long.reason.data)=c("time.pulled", "Type", "peopleor.data.points", "take.out.or.leave.in", "run.with.or.without",  
                             "analysis", "reasion.type", "used")
#View(long.reason.data)
long.reason.data = subset(long.reason.data, used == 1)
long.reason.data<-na.omit(long.reason.data)
# View(long.reason.data)

long.reason.data$reason=factor(long.reason.data$reasion.type)
reason.table.test = table( long.reason.data$reason, long.reason.data$analysis)
#View(reason.table.test)
p.t.m.a=chisq.test(reason.table.test) #thre a "approximation may be incorrect" error, so switched to the Fisher test.
f.t.m.a=fisher.test(reason.table.test, simulate.p.value=TRUE,B=1e7)

#so probably don't need to graph it so much. 
#barplot(reason.table.test, col=c("purple", "blue", "green"), space=1, legend = rownames(reason.table.test))

```


## Data analysis
Chi-Square tests were used to analyze the associations between variables. When a chi-square analysis suggested that the approximation may be incorrect a Fisher's Exact test will be analyzed as well and both will be reported. Frequencies were used to describe the outliers mentioned. 
```{r chisquares}


long.test=master[,c("Journal","year", "time.pulled", "Type", "mention.outliers", "peopleor.data.points", "take.out.or.leave.in", "run.with.or.without", 
                             "Basics", "ANOVA" , "Regression", "ChiSquare", "Nonparametric", 
                            "Modeling", "BayesOther" )]

long.test.data=melt(long.test, 
                    id = c("Journal","year", "time.pulled", "Type","mention.outliers", "peopleor.data.points", "take.out.or.leave.in", "run.with.or.without"),
                    measured = c("Basics", "ANOVA" , "Regression", "ChiSquare", "Nonparametric", 
                                 "Modeling", "BayesOther"))
#View(choice.data)          
colnames(long.test.data)=c("Journal","year", "time.pulled", "Type","mention.outliers", "peopleor.data.points", "take.out.or.leave.in", "run.with.or.without", 
                            "analysis", "used")
#View(long.test.data)
long.test.data = subset(long.test.data, used == 1)
long.test.data$mention.outliers2<-as.numeric(long.test.data$mention.outliers)
table(long.test.data$mention.outliers2)
long.test.data$mention.outliers2=recode(long.test.data$mention.outliers2, "1=0; 2=1")
table(long.test.data$mention.outliers2)

#View(long.test.data)

mytable3 <- xtabs(~time.pulled+mention.outliers+analysis, data=long.test.data)
model3<-loglm(~time.pulled+mention.outliers+analysis, data=mytable3)
t.m.a=model3
ftable(mytable3)
#find significant association between analysis type and reproting of outliers.  every analyssi shows an increase in mentions of outliers. 
model3b=table(long.testing.data$analysis, long.testing.data$time.pulled)
chisq.test(model3b)
model3b
#rationalextime
#View(long.reason.data)
long.reason.data.na=na.omit(long.reason.data)
length(long.reason.data.na$time.pulled)
t.r.time<-table(long.reason.data.na$time.pulled, long.reason.data.na$reason)
t.r.time
chisq.test(t.r.time)
#View(t.r.time.test$observed)
#associated: more overall in 2017, but most increase in reporting of participant error (increasing 3-fold), experimental error (2-fold), and statistical reasons (<2fold)


#rationalexdiscipline
#t.r.field<-table(long.reason.data.na$Type, long.reason.data.na$reason)
#View(t.r.field)

#can't discuss this, too sparse of data



out.testing$peopleor.data.points.f=factor(out.testing$peopleor.data.points)
time.people<-table(out.testing$time.pulled, out.testing$peopleor.data.points)
time.people<-time.people[,c(2:5)]
time.people

chisq.test(time.people)#thre a "approximation may be incorrect" error, so switched to the Fisher test.
#fisher.test(time.people, simulate.p.value=TRUE,B=1e7) # same as chisq basically p= 0.01594
#so, there is a sign. difference, mainly driven by the fact that taking out people has doubled. 


#long.reason.data.2=long.reason.data[,c(1, 3:9)]
#people.reason=table(long.reason.data$peopleor.data.points, long.reason.data$reason)
#people.reason
#so can't run this one b/c data sparsity

#time.out=table(long.reason.data$take.out.or.leave.in, long.reason.data$time.pulled)
#time.out
#can't run that

#reason.out=table(long.reason.data$reason, long.reason.data$take.out.or.leave.in)
#reason.out
#can't run that either

#out.people=table(out.testing$take.out.or.leave.in, out.testing$peopleor.data.points)
#out.people
#this data is super sparse, let's keep this part simple and just describe the data we have. 

table(out.testing$peopleor.data.points)     
table(out.testing$take.out.or.leave.in)     
table(out.testing$run.with.or.without)
table(out.testing$difference.between.analyses.with.and.without)
table(out.testing$report.results.on.if.did.both)
table(out.testing$report.results.on.if.did.both, out.testing$difference.between.analyses.with.and.without)
```

```{r correlation}
cor.data<-master[,c(9, 28)]
cor.test(cor.data$original.sample.size, cor.data$mention.outliers2, use="complete.obs", method="spearman") 
#super small rho, but it's significant. 

cor.test(master$original.sample.size, master$time.pulled, use="complete.obs", method="spearman") 
#we are increasing in sample size over time, slightly, rho=.136, p<.001
plot(master$year, master$original.sample.size) #wtf is up with year here?

#printnum(stuff$p.value, gt1=FALSE, zero=FALSE)
#?apa_print

```

```{r mlms}
library(lme4)
m1 <- glmer(mention.outliers2 ~ year  + (1 | Journal),
            data=master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1)

clin.master=master[which(master$Type=="Clinical"),]
cog.master=master[which(master$Type=="Cognitive"),]
coun.master=master[which(master$Type=="Counseling"),]
dev.master=master[which(master$Type=="Developmental"),]
edu.master=master[which(master$Type=="Educational"),]
envi.master=master[which(master$Type=="Environmental"),]
for.master=master[which(master$Type=="Forensics"),]
io.master=master[which(master$Type=="IO"),]
meth.master=master[which(master$Type=="Methods"),]
neu.master=master[which(master$Type=="Neuro"),]
over.master=master[which(master$Type=="Overview"),]
soc.master=master[which(master$Type=="Social"),]
sport.master=master[which(master$Type=="Sports"),]

m1cl <- glmer(mention.outliers2 ~ year  + (1 | Journal),
            data=clin.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1cl)

m1cog <- glmer(mention.outliers2 ~ year  + (1 | Journal),
            data=cog.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1cog)

m1cou <- glmer(mention.outliers2 ~ year  + (1 | Journal),
            data=coun.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1cou)

m1dev <- glmer(mention.outliers2 ~ year  + (1 | Journal),
            data=dev.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1dev)

m1edu <- glmer(mention.outliers2 ~ year  + (1 | Journal),
            data=edu.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1edu)

m1envi <- glmer(mention.outliers2 ~ year  + (1 | Journal),
            data=envi.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1envi)

m1for <- glmer(mention.outliers2 ~ year  + (1 | Journal),
            data=for.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1for)

m1io <- glmer(mention.outliers2 ~ year  + (1 | Journal),
            data=io.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1io)

m1meth <- glmer(mention.outliers2 ~ year  + (1 | Journal),
            data=meth.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1meth)

m1neu <- glmer(mention.outliers2 ~ year  + (1 | Journal),
            data=neu.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1neu)

m1over <- glmer(mention.outliers2 ~ year  + (1 | Journal),
            data=over.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1over)

m1soc <- glmer(mention.outliers2 ~ year  + (1 | Journal),
            data=soc.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1soc)

m1sport <- glmer(mention.outliers2 ~ year  + (1 | Journal),
            data=sport.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1sport)
#show largest change in forensic (45% more likely to report), followed by social at 34% and io at 33% followed by developmental at 20% and cognitive at 15%  

#now need to see if they vary by analysis
basic.data=long.test.data[which(long.test.data$analysis=="Basics"),]
anova.data=long.test.data[which(long.test.data$analysis=="ANOVA"),]
reg.data=long.test.data[which(long.test.data$analysis=="Regression"),]
chi.data=long.test.data[which(long.test.data$analysis=="ChiSquare"),]
nonpar.data=long.test.data[which(long.test.data$analysis=="Nonparametric"),]
model.data=long.test.data[which(long.test.data$analysis=="Modeling"),]
bo.data=long.test.data[which(long.test.data$analysis=="BayesOther"),]



m1basic <- glmer(mention.outliers2 ~ year + (1 | Journal),
            data=basic.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1basic)

m1anova <- glmer(mention.outliers2 ~ year + (1 | Journal),
            data=anova.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1anova)

m1reg <- glmer(mention.outliers2 ~ year + (1 | Journal),
            data=reg.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1reg)

m1chi <- glmer(mention.outliers2 ~ year + (1 | Journal),
            data=chi.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1chi)

m1non <- glmer(mention.outliers2 ~ year + (1 | Journal),
            data=nonpar.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1non)

m1model <- glmer(mention.outliers2 ~ year + (1 | Journal),
            data=model.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1model)

m1bo <- glmer(mention.outliers2 ~ year + (1 | Journal),
            data=bo.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
summary(m1bo)

#change in reporting by year significant for nonparamertic (38% more likely to report), basic (23%), regression (15%), anova (14%), and modeling (.11). Interesting changes in variance due to journal. 




#want to see if reporting people or data points changes over time
#subset data removing both and none
names(out.testing)
table(out.testing$peopleor.data.points.f)
out.testing.people = subset(out.testing, peopleor.data.points.f == c("data points", "people"))
out.testing.people <- subset(out.testing, peopleor.data.points.f=='data points' | peopleor.data.points.f=='people') 
table(out.testing.people$peopleor.data.points.f)
#make the variable binary
out.testing.people$peopleor.data.points.f<-as.numeric(out.testing.people$peopleor.data.points.f)
table(out.testing.people$peopleor.data.points.f)
out.testing.people$peopleor.data.points.bi=recode(out.testing.people$peopleor.data.points.f, "2=0; 4=1")
table(out.testing.people$peopleor.data.points.bi)
#datapoints=0, people=1
m2 <- glmer(peopleor.data.points.bi ~ year  + (1 | Journal),
          data=out.testing.people, family=binomial(link="logit"),
          control = glmerControl(optimizer = "bobyqa"),
          nAGQ = 0)
summary(m2)

#no difference in reporting by time
#this is important because it contradicts what the chi-sq of time pulled says




```

# Results
Investigating reporting of outliers, we found that in 2012, `r overalloutpercent$percent[2]`% of experiments mentioned outliers, while in 2017 `r overalloutpercent$percent[4]`% of experiments mentioned outliers. This represents a significant increase in mentions of outliers, $\chi^2$ (`r stuff$parameter`) = `r stuff$statistics`, *p*< .001. Further exploration reveals that differences in reporting between years arise between fields ($\chi^2$ (`r t.m.f$df`) = `r t.m.f$pearson`, *p*< .001) which can be seen in \@ref(tab:journalprop). Specifically, if we look at the change in reporting for each field analyzed at the level of the experiment (HEYO, CAN I GET A TABLE IN HERE LIKE ON THE POSTER WITH THE EFFECT SIZES?) we find the largest differences appear for Social, Cognitive, I/O, and Forensic Psychology, with the smallest changes appearing for the Environmental, Overview, and Clinical Psychology. 

When viewing the reporting of outliers broken down by analysis across time, we see there is a significant association between time, analysis, and outlier reporting ($\chi^2$ (`r t.m.a$df`) = `r t.m.a$pearson`, *p*< .001). As can be seen in \@ref(????) Every analysis shows an increase in mentions of outliers over time, and when looking solely the relationship between analysis and time of those experiments actually reporting outliers, we see the largest increases in reporting for the Bayesian/Other category, followed by the modeling category, and the nonparametric category.  
Of those outliers that were reported, the majority of the time this was reportedly due to statistical reasons (x%) and/or participant errors (x%), with few experiments reporting outliers due to unusable data. When looking at the association of these reasons over time, we again clearly see all reasons increasing in usage from 2012 to 2017, but see the most increase in reporting of participant errors, followed by unusable data, and finally statistical reasons. It is therefore unsurprising that there is a significant association between time and whether it was a person or datapoint were discusses as outliers (insert chi-sq here), such that all instances increased in reporting over time, however, removal of people doubled from 2012 to 2017. 
In our review, the majority of outliers mentioned referred to people (x%), removed the observation (x%), and ran the analyses without the observation (x%). Only 9 experiments (x%) discussed the differences between results both with and without the outlying observations, and of those, 5 found no differences in their analyses and 4 found differences in their analyses. Of the 5 that found no differences,  3 reported their final analyses without the outliers, 1 reported their final analyses with the outlier included, and 1 reported both with and without the outliers. Of the 4 that found differences, 3 reported the results without the outliers and 1 reported the results both with and without the outliers. (CAN WE PUT A TABLE FOR THIS PARAGRAPH THAT HAS A ROW FOR EACH OUTCOME (take out, report diff, etc.), I.E. WITH/WITHOUT/WINSORIZED, AND A COLUMN FOR THE %(N) FALLING INTO THAT CATEGORY?)

# Discussion

While modest improvments in reporting can be seen in some fields, overall the outlook is still bleak. 
Some may argue that use of the precious word limit dictated by journals to describe such acts as outlier identification and handling may be irrational. However, we contest that given the current availability and use of online supplements and appendixes, as well as the invent of the Open Science Framework (OSF; cite) which allows researchers the ability to upload any number of additional resources and supplements that can be easily referenced in their manuscripts in as little as 4 words: See supplement at osf.io/52mqw.



\newpage

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}


