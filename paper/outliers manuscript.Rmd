---
title             : "Have psychologists increased reporting of outliers in response to the reproducibility crisis?"
shorttitle        : "Outlier reporting"

author: 
  - name          : "K. D. Valentine"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "100 Cambridge St., Boston, MA 02114"
    email         : "kvalentine2@mgh.harvard.edu"
  - name          : "Erin M. Buchanan"
    affiliation   : "2"
  - name          : "Arielle Cunningham"
    affiliation   : "3"
  - name          : "Tabetha Hopke"
    affiliation   : "3"
  - name          : "Addie Wikowsky"
    affiliation   : "3"
  - name          : "Haley Wilson"
    affiliation   : "3"
    
affiliation:
  - id            : "1"
    institution   : "Massachusetts General Hospital"
  - id            : "2"
    institution   : "Harrisburg University of Science and Technology"
  - id            : "3"
    institution   : "Missouri State University"

author_note: |
 K. D. Valentine is a Postdoctoral Research Fellow at the Health Decision Sciences Center at Massachusetts General Hospital. Erin M. Buchanan is an Professor of Cognitive Analytics at Harrisburg University of Science and Technology. Arielle Cunningham, Tabetha Hopke, Addie Wikowsky, and Haley Wilson are master's candidates at Missouri State University. 
  
abstract: |
  Psychology is currently experiencing a "renaissance" where the replication and reproducibility of published reports are at the forefront of conversations in the field. While researchers have worked to discuss possible problems and solutions, work has yet to uncover how this new culture may have altered reporting practices in the social sciences. As outliers and other errant data points can bias both descriptive and inferential statistics, the search for these data points is essential to any analysis using these parameters. We quantified the rates of reporting of outliers and other data within psychology at two time points: 2012 when the replication crisis was born, and 2017, after the publication of reports concerning replication, questionable research practices, and transparency. A total of 2235 experiments were identified and analyzed, finding an increase in reporting from only 15.7% of experiments in 2012 to 25.0% in 2017. We investigated differences across years given the psychological field or statistical analysis that experiment employed. Further, we inspected whether data exclusions mentioned were whole participant observations or data points, and what reasons authors gave for stating the observation was deviant. We conclude that while report rates are improving overall, there is still room for improvement in the reporting practices of psychological scientists which can only aid in strengthening our science.
  
keywords          : "outlier, influential observation, replication"

bibliography      : ["outliers.bib"]

figsintext        : no
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "english"
class             : "man"
mask              : no
output            : papaja::apa6_pdf
replace_ampersands: yes
csl               : apa6.csl
---

```{r load_packages, include = FALSE}
library("papaja")
library(knitr)
library(dplyr)
library(lme4)
library(ggplot2)
library(reshape)
knitr::opts_chunk$set(cache = TRUE)

##load the dataset
master = read.csv("outliers complete.csv")

##graph clean up code
cleanup = theme(panel.grid.major = element_blank(), 
              panel.grid.minor = element_blank(), 
              panel.background = element_blank(), 
              axis.line = element_line(colour = "black"), 
              legend.key = element_rect(fill = "white"),
              text = element_text(size = 15))
```

Psychology is undergoing a "renaissance" in which focus has shifted to the replication and reproducibility of current published reports [@Nelson2018; @Etz2016; @Lindsay2015; @ScienceCollaboration2015; @VanElk2015]. A main concern has been the difficulty in replicating phenomena, often attributed to publication bias [@Ferguson2012a], the use and misuse of *p*-values [@Gigerenzer2004; @Ioannidis2005], and researcher degrees of freedom [@Simmons2011]. In particular, this analysis focused on one facet of questionable research practices (QRPs) that affect potential replication; the selective removal or inclusion of data points. 

As outlined by @Nelson2018, the social sciences turned inward to examine their practices due to the publication of unbelievable data [@Wagenmakers2011a], academic fraud [@Simonsohn2013], failures to replicate important findings [@Doyen2012], and the beginning of the Open Science Framework [@Nosek2015b]. These combined forces led to the current focus on QRPs and *p*-hacking and the investigation into potential solutions to these problems. Recommendations included integrating effect sizes into results [@Cumming2008; @Lakens2013], encouraging researchers to be transparent about their research practices, including not only the design and execution of their experiments, but especially the data preparation and resulting analyses [@Simmons2011], attempting and interpreting well thought out replication studies [@Asendorpf2012; @Maxwell2015], altering the way we think about *p*-values [@Benjamin2018; @Lakens2018; @Valentine2017], and restructuring incentives [@Nosek2012c]. Additionally, @Klein2014c developed the Many Labs project to aid in data collection for increased power, while the @ScienceCollaboration2015 utilized a many labs approach to publish combined findings to speak to the replication of phenomena in psychology. 

While we have seen vast discussion of the problems and proposed solutions, research has yet to determine how this new culture may have impacted reporting practices of researchers. Herein, we aim specifically to quantify the rates of reporting of outliers within psychology at two time points: 2012, when the replication crisis was outlined [@Pashler2012a], and 2017, after the publication of reports concerning QRPs, replication, and transparency [@Miguel2014]. Because of the slow editorial, revision, and publication process, publications with the year 2012 were likely completed in 2011 or earlier, thus, a good starting time point for gathering data at or around the start of the "crisis". 

## Outliers

Bernoulli first mentioned outliers in 1777 starting the long history of examining for discrepant observations [@Bernoulli1777], which can bias both descriptive and inferential statistics [@Cook1980; @Stevens1984; @Yuan2001; @Zimmerman1994]. Therefore, the examination for these data points is essential to any analysis using these parameters, as outliers can impact study results. Outliers have been defined as influential observations or fringliers, but herein, we specifically use the definition of "an observation which being atypical and/or erroneous deviates decidedly from the general behavior of experimental data with respect to the criteria which is to be analyzed on it" [@Munoz-Garcia1990, pg. 217]. This definition was used to capture a wide range of what one might consider "deviant": participant errors in an experiment, unusable data, and data that may be found at the tail ends of a distribution. The removal of any data point for discrepant reasons should be transparently conveyed in a study, and therefore, we used a more broad definition to include these different scenarios. Additionally, the definition of outliers can vary from researcher to researcher, and a wide range of graphical and statistical options are available for deviant data detection [@Beckman1983; @Hodge2004; @Orr1991; @Osborne2004]. For example, @Tabachnick2012 outline several of the most popular detection methods including visual data inspection, residual statistics, a set number of standard deviations, Mahalanobis distance, Leverage, and Cook's distances. Participants who do not complete the study correctly and/or unusable data are often found with these types of detection techniques, and therefore, a broad definition of outliers is necessary to capture researcher behavior. 

Researchers have separated outliers into categories in many ways over the years [@Beckman1983; @Hodge2004; @Munoz-Garcia1990; @Orr1991; @Osborne2004]. Some of the most pervasive categories include experimenter error (e.g., an error in the way the data was collected, coded, or prepared), participant behaviors (e.g., intentional or motivated misreporting), and natural variability (including legitimate data that are interesting because they do not fit the expected scheme). Just as there are different categories of outliers, there are different ways to handle outliers. For instance, an outlier that is a legitimate data point that does not fit into the expected scheme should not necessarily be removed. However, an outlying data point that arose due to a coding error should be corrected, not necessarily removed from an analysis. 

Therefore, it is important to understand how outliers were detected, what type of outlier they may be, and a justification for how the outliers were handled. Before the serious focus on QRPs, the information regarding outlier detection as part of data screening was often excluded from publication, particularly if a journal page limit requirement needed to be followed. Consider, for example, @Orr1991, who inspected 100 Industrial/Organizational Psychology personnel studies and found no mention of outliers whatsoever. 

However, while outliers may not be publicized, outlier detection and removal is likely part of a researcher's data screening procedure. @LeBel2013 found that 11% of psychology researchers stated that they had not reported excluding participants for being outliers in their papers. @Fiedler2016 suggested that more than a quarter of researchers decide whether to exclude data only after looking at the impact of doing so. @Bakker2014 investigated the effects of outliers on published analyses, and while they did not find that they affected the surveyed results, they did report that these findings are likely biased by the non-reporting of data screening procedures in some articles, as sample sizes and degrees of freedom often did not match. These studies indicate that a lack of transparency in data manipulation and reporting is problematic. 

By keeping outliers in a dataset, analyses are more likely to have increased error variance [depending on sample size, @Orr1991], biased estimates [@Osborne2004], and reduced effect size and power [@Orr1991; @Osborne2004], which can alter the results of the analysis and lead to falsely supporting (Type I error), or denying a claim (Type II error). Inconsistencies in the treatment and publication of outliers could also lead to failures to replicate previous work, as it would be difficult to replicate analyses that have been *p*-hacked into "just-significant" results [@Nelson2018; @Leggett]. The influence of this practice can be wide spread, as non-reporting of data manipulation can negatively affect meta-analyses, effect sizes, and sample size estimates for study planning. On the other hand, outliers do not always need to be seen as nuisance, as they will often be informative to researchers because they can encourage the diagnosis, change, and evolution of a research model [@Beckman1983]. Taken together, a lack of reporting of outlier practices can lead to furthering unwarranted avenues of research, ignoring important information, creating erroneous theories, and failure to replicate, all of which serve to weaken the sciences. Clarifying the presence or absence of outliers, how they were assessed, and how they were handled can improve our transparency and replicability, and ultimately help to strengthen our science. 

The current zeitgeist of increased transparency and reproducibility applies not only to the manner in which data is collected, but also the various ways the data is transformed, cleaned, pared down, and analyzed. Therefore, it can be argued that it is just as important for a researcher to state how they identified outliers within their data, how the outliers were handled, and how this choice of handling impacted the estimates and conclusions of their analyses, as it is for them to report their sample size. Given the timing of the renaissance, we expected to find a positive change in reporting rates for outliers in 2017, as compared to 2012. This report spans a wide range of psychological sub-domains; however, we also expected the impact of the @ScienceCollaboration2015 publication to affect social and cognitive psychology more than other fields. 

# Method

## Fields

A list of psychological sub-domains was created to begin the search for appropriate journals to include. The authors brainstormed the list of topics (shown in Table \@ref(tab:info-table)) by first listing major research areas in psychology (i.e., cognitive, clinical, social, etc.). Second, a list of common courses offered at large universities was consulted to add to the list of fields. Last, the American Psychological Association's list of divisions was examined for any potential missed fields. The topic list was created to capture large fields of psychology with small overlap (i.e., cognition and neuropsychology) while avoiding specific sub-fields of topics (i.e., cognition overall versus perception and memory only journals). Sixteen fields were initially identified; however, only thirteen were included in final analysis due to limitations noted below. 

## Journals

Once these fields were agreed upon, researchers used various search sources (Google, EBSCO host databases) to find journals that were dedicated to each broad topic. Journals were included if they appeared to publish a wide range of articles within the selected fields. A list of journals, publishers, and impact factors (as noted by each journal's website in Spring of 2013 and 2018) were identified for each field. Two journals from each field were selected based on the following criteria: 1) impact factors over one at minimum, 2) a mix of publishers, if possible, and 3) availability due to university resources. These journals, impact factors, and publishers are shown in the online supplemental materials at https://osf.io/52mqw/.

## Articles

Fifty articles from each journal were manually examined for data analysis: In the Spring of 2013, 25 articles were collected from each journal from 2012 backward, then, in the Fall of 2017, 25 articles were collected from 2017 backward. Data collection of articles started at the last volume publication from the given year (2012 or 2017) and progressed backwards until 25 articles had been found. Thus, while some journals may only include articles from 2012, other journals will include articles from previous years in order to fulfill the 25 article goal. Articles were included if they met the following criteria: 1) included data analyses, 2) included multiple participants or data-points, and 3) analyses were based on human subjects or stimuli. Therefore, we excluded theory articles, animal populations, and single subject designs. Based on review of the 2012 articles, three fields were excluded. Applied Behavior Analysis articles predominantly included single-subject designs, evolutionary psychology articles were primarily theory articles, and statistics related journal articles were based on user simulated data with a specific set of characteristics. Since none of these themes fit into our analysis of understanding data screening with human subject samples, we excluded those three fields from analyses.

## Data Processing

Each article was manually reviewed for key components of data analysis. Each experiment in an article was coded separately. For each experiment, the type of analysis conducted, number of participants/stimuli analyzed, and whether or not they made any mention of outliers were coded by hand by research assistants. 

### Analysis types

Types of analyses were broadly defined as basic statistics (descriptive statistics, *z*-scores, *t*-tests, and correlations), ANOVAs, regressions, chi-squares, non-parametric statistics, modeling, and Bayesian/other analyses. 

### Outlier coding

For reporting of outliers, the project team used a dichotomous yes/no coding regarding whether or not they were mentioned in an article. Outliers were not limited to simple statistical analysis of discrepant responses, but we also coded for specific exclusion criteria that were not related to missing data or study characteristics (i.e., we did not consider it an outlier if they were only looking for older adults). If outliers were mentioned, we coded information about outliers into four types: 1) people, 2) data points, 3) both, or 4) none found. Data that were coded as data points refer to the identification of individual trials being outlying while those coded as people referred to identification of the participant's entire row of data being outlying. We found that a unique code for data points was important for analyses with response time studies where individual participants were not omitted but rather specific data trials were eliminated. 

Then, for those articles that mentioned outliers, the author's decision for how to handle the outliers was hand coded into whether they removed participants/data points, left these outliers in the analysis, or winsorized the data points. Experiments were coded for whether they tested the analyses with, without, or both for determination of their effect on the study. If they removed outliers, a new sample size was recorded. However, this data was not analyzed, as we determined it was conflated with removal of other types of data unrelated to the interest of this paper (e.g., missing data). Lastly, we coded the reasoning for outlier detection as one or more of the following: 1) Statistical reason (e.g., used numbers to define odd or deviant behavior in responding, such as *z*-score or Mahalanobis distance scores), 2) Participant error (e.g., failed attention checks, did not follow instructions, or low quality data because of participant problems), and 3) Unusable data (e.g., inside knowledge of the study or experimenter/technological problems).

# Results

## Data Analytic Plan

Because each article constituted multiple data points within the dataset which were each nested within a particular journal and article, a multilevel model (MLM) was used to control for correlated error [@Gelman2006]. The @Pinheiro2017 *nlme* package in *R* was used to calculate these analyses. A maximum likelihood logistic multilevel model was used to examine how the year in which the experiment was published predicted the likelihood of mentioning outliers (yes/no) while including a random intercept for journal and article. This model was analyzed over all of the data, as well as broken down by sub-fields or analyses in order to glean a more detailed account of the effect of year on outlier reporting. Additionally, three MLMs were analyzed attempting to individually predict each outlier reason (i.e., statistical reason yes/no; unusable data yes/no; participant reason yes/no) given the year while including a random intercept for journal and article. We did not use publication year as a dichotomous variable, as not all articles were from 2012 or 2017 because of publication rates (i.e., number of articles and issues per year) and article exclusions. Publication year ranged from 2001 to 2013 for articles collected in 2012, and 2015 to 2018 for articles collected in 2017 (several articles were considered online first with publication dates officially in 2018, and the official data was used for each article). Therefore, we treated this variable as continuous to capture the differences in years present across each subfield and time point collected. Data is presented in tables dichotomously to preserve space.  We further explored whether these outliers were people or data points, how outliers were handled, and the reasons data were named outliers with descriptive statistics. All code and data can be viewed inline with the manuscript, which was written with the *papaja* package [@Aust2017]. 

## Overall Outliers

```{r overallout, include = FALSE}
##clean up the mention column 
master$mention.outliers = factor(master$mention.outliers,
                      levels = c("no", "yes"),
                      labels = c("No", "Yes"))

##create a data frame of the percentages by type
oaoutsummary = table(master$mention.outliers, master$time.pulled)
oaoutsummary = as.data.frame(oaoutsummary)
colnames(oaoutsummary) = c("mention.outliers", "time.pulled", "Freq")

overalloutpercent = as.data.frame(group_by(oaoutsummary, time.pulled) %>%
  mutate(percent = Freq/sum(Freq)*100))

##examine the model
m1 <- glmer(mention.outliers ~ year  + (1 | Journal) + (1 | article),
            data=master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1)
```

Data processing resulted in a total of `r nrow(master)` experiments being coded, `r nrow(master[ master$time.pulled == "2012" , ])` of which were from 2012 or prior, with the additional `r nrow(master[ master$time.pulled == "2017" , ])` being from 2017 or prior. Investigating reporting of outliers, we found that in 2012, `r printnum(overalloutpercent$percent[2], digits = 1)`% of experiments mentioned outliers, while in 2017, `r printnum(overalloutpercent$percent[4], digits = 1)`% of experiments mentioned outliers. Actual publication year was used to predict outlier mention (yes/no) with a random intercept for journal and article, as described above. We found that publication year predicted outlier mentions, *Z* = `r printnum(summary(m1)$coefficients[2,3], digits = 2)`, *p* = `r printp(summary(m1)$coefficients[2,4])`. Each year, experiments were `r printnum(exp(summary(m1)$coefficients[2,1])*100-100, digits = 1)`% more likely to report outliers as the previous year. 

## Fields

```{r fieldout, include = FALSE}

##create a data frame of the percentages by time and type
outsummary = table(master$mention.outliers, master$time.pulled, master$Type)
outsummary = as.data.frame(outsummary)
colnames(outsummary) = c("mention.outliers", "time.pulled", "Type", "Freq")

outpercent = as.data.frame(group_by(outsummary, time.pulled, Type) %>%
  mutate(percent = Freq/sum(Freq)*100))
```

```{r field-mlm, include = FALSE}
clin.master=master[which(master$Type=="Clinical"),]
cog.master=master[which(master$Type=="Cognitive"),]
coun.master=master[which(master$Type=="Counseling"),]
dev.master=master[which(master$Type=="Developmental"),]
edu.master=master[which(master$Type=="Educational"),]
envi.master=master[which(master$Type=="Environmental"),]
for.master=master[which(master$Type=="Forensics"),]
io.master=master[which(master$Type=="IO"),]
meth.master=master[which(master$Type=="Methods"),]
neu.master=master[which(master$Type=="Neuro"),]
over.master=master[which(master$Type=="Overview"),]
soc.master=master[which(master$Type=="Social"),]
sport.master=master[which(master$Type=="Sports"),]

m1cl <- glmer(mention.outliers ~ year  + (1 | Journal) + (1 | article),
            data=clin.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1cl)

m1cog <- glmer(mention.outliers ~ year  + (1 | Journal) + (1 | article),
            data=cog.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1cog)

m1cou <- glmer(mention.outliers ~ year  + (1 | Journal) + (1 | article),
            data=coun.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1cou)

m1dev <- glmer(mention.outliers ~ year  + (1 | Journal) + (1 | article),
            data=dev.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1dev)

m1edu <- glmer(mention.outliers ~ year  + (1 | Journal) + (1 | article),
            data=edu.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1edu)

m1envi <- glmer(mention.outliers ~ year  + (1 | Journal) + (1 | article),
            data=envi.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1envi)

m1for <- glmer(mention.outliers ~ year  + (1 | Journal) + (1 | article),
            data=for.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1for)

m1io <- glmer(mention.outliers ~ year  + (1 | Journal) + (1 | article),
            data=io.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1io)

m1meth <- glmer(mention.outliers ~ year  + (1 | Journal) + (1 | article),
            data=meth.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1meth)

m1neu <- glmer(mention.outliers ~ year  + (1 | Journal) + (1 | article),
            data=neu.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1neu)

m1over <- glmer(mention.outliers ~ year  + (1 | Journal) + (1 | article),
            data=over.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1over)

m1soc <- glmer(mention.outliers ~ year  + (1 | Journal) + (1 | article),
            data=soc.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1soc)

m1sport <- glmer(mention.outliers ~ year  + (1 | Journal) + (1 | article),
            data=sport.master, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1sport)
```

Further exploration reveals that differences in reporting between years arise between fields which can be seen in Table \@ref(tab:info-table). Figure \@ref(fig:type-graph) displays the percentage of outlier mentions of each field colored by year examined. A MLM was analyzed for each field using journal and article as a random intercept to determine the influence of year of publication on outlier reporting rates. Specifically, if we look at the change in reporting for each field analyzed at the level of the experiment, we find the largest changes in forensic (`r printnum(exp(summary(m1for)$coefficients[2,1])*100-100, digits = 1)`% more likely to report), social (`r printnum(exp(summary(m1soc)$coefficients[2,1])*100-100, digits = 1)`%), and I/O (`r printnum(exp(summary(m1io)$coefficients[2,1])*100-100, digits = 1)`%), followed by developmental (`r printnum(exp(summary(m1dev)$coefficients[2,1])*100-100, digits = 1)`%) and cognitive (`r printnum(exp(summary(m1cog)$coefficients[2,1])*100-100, digits = 1)`%). In support of our hypothesis, we found that both social and cognitive fields showed general increases in their outlier reporting; however, it was encouraging to see positive trends in other fields as well. These analyses show that in some fields, including overview and neurological fields, we found a decrease in reporting across years, although these changes were not significant. 

The analyses shown below were exploratory based on the findings when coding each experiment for outlier data. We explored the relationship of outlier reporting to the type of analysis used in each experiment, reasons for why outliers were excluded, as well as the type of outlier excluded from the study.

```{r type-graph, echo = FALSE, fig.cap = "Percent of outlier mentions by sub-domain field and year examined. Error bars represent 95% confidence interval.", fig.height = 6, fig.width = 8}
graphdata = subset(outpercent, mention.outliers == "Yes")

#calculate CIs to add to graph
graphdata$sample = as.data.frame(table(master$time.pulled, master$Type))$Freq
graphdata$SE = sqrt(graphdata$percent*(100-graphdata$percent)/graphdata$sample)

dotplot = ggplot(graphdata, aes(Type, percent, color = time.pulled))
dotplot = dotplot +
  geom_pointrange(aes(ymin=percent-1.96*SE, ymax=percent+1.96*SE))+ 
  cleanup +
  xlab("Field of Journal") +
  ylab("Percent of Outlier Mentions") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  scale_color_manual(name = "Year", 
                     values = c("black", "gray")) + 
  coord_cartesian(ylim = c(0,75))
dotplot
```

```{r info-table, echo = FALSE, results = 'asis'}
tableprint = matrix(NA, nrow = 13, ncol = 8)

tableprint[ ,1] = levels(master$Type)
tableprint[ ,2] = printnum(graphdata$percent[graphdata$time.pulled == "2012"], digits = 1)
tableprint[ ,3] = printnum(graphdata$sample[graphdata$time.pulled == "2012"], digits = 0)
tableprint[ ,4] = printnum(graphdata$percent[graphdata$time.pulled == "2017"], digits = 1)
tableprint[ ,5] = printnum(graphdata$sample[graphdata$time.pulled == "2017"], digits = 0)
tableprint[ , 6] = printnum(c(exp(summary(m1cl)$coefficients[2,1]), 
                     exp(summary(m1cog)$coefficients[2,1]),
                     exp(summary(m1cou)$coefficients[2,1]),
                     exp(summary(m1dev)$coefficients[2,1]),
                     exp(summary(m1edu)$coefficients[2,1]),
                     exp(summary(m1envi)$coefficients[2,1]),
                     exp(summary(m1for)$coefficients[2,1]),
                     exp(summary(m1io)$coefficients[2,1]),
                     exp(summary(m1meth)$coefficients[2,1]),
                     exp(summary(m1neu)$coefficients[2,1]),
                     exp(summary(m1over)$coefficients[2,1]),
                     exp(summary(m1soc)$coefficients[2,1]),
                     exp(summary(m1sport)$coefficients[2,1])), digits = 2)
tableprint[ , 7] = printnum(c(summary(m1cl)$coefficients[2,3], 
                     summary(m1cog)$coefficients[2,3],
                     summary(m1cou)$coefficients[2,3],
                     summary(m1dev)$coefficients[2,3],
                     summary(m1edu)$coefficients[2,3],
                     summary(m1envi)$coefficients[2,3],
                     summary(m1for)$coefficients[2,3],
                     summary(m1io)$coefficients[2,3],
                     summary(m1meth)$coefficients[2,3],
                     summary(m1neu)$coefficients[2,3],
                     summary(m1over)$coefficients[2,3],
                     summary(m1soc)$coefficients[2,3],
                     summary(m1sport)$coefficients[2,3]), digits = 2)
tableprint[ , 8] = printp(c(summary(m1cl)$coefficients[2,4], 
                     summary(m1cog)$coefficients[2,4],
                     summary(m1cou)$coefficients[2,4],
                     summary(m1dev)$coefficients[2,4],
                     summary(m1edu)$coefficients[2,4],
                     summary(m1envi)$coefficients[2,4],
                     summary(m1for)$coefficients[2,4],
                     summary(m1io)$coefficients[2,4],
                     summary(m1meth)$coefficients[2,4],
                     summary(m1neu)$coefficients[2,4],
                     summary(m1over)$coefficients[2,4],
                     summary(m1soc)$coefficients[2,4],
                     summary(m1sport)$coefficients[2,4]))

apa_table(as.data.frame(tableprint),
          format = "latex",
          caption = "Outlier Reporting by Field Across Years",
          align = c("l", rep("c",7)),
          escape = FALSE,
          note = "$N$ represents number of experiments for each category.",
          col.names = c("Field", "\\% 12", "$N$ 12", "\\% 17", 
                              "$N$ 17", "OR", "$Z$", "$p$"))
```

## Analyses Type

```{r analysisout, include=FALSE}
##create longdata set for analysis
long.test=master[,c("Journal","year", "article", "time.pulled", "Type", "mention.outliers", "peopleor.data.points",
                    "take.out.or.leave.in", "run.with.or.without", 
                    "Basics", "ANOVA" , "Regression", "ChiSquare", "Nonparametric", 
                    "Modeling", "BayesOther" )]

long.test.data=melt(long.test, 
                    id = c("Journal","year", "article", "time.pulled", "Type",
                           "mention.outliers", "peopleor.data.points", "take.out.or.leave.in", "run.with.or.without"),
                    measured = c("Basics", "ANOVA" , "Regression", "ChiSquare", "Nonparametric", 
                                 "Modeling", "BayesOther"))
colnames(long.test.data)=c("Journal","year", "article", "time.pulled", 
                           "Type","mention.outliers", "peopleor.data.points", "take.out.or.leave.in",
                           "run.with.or.without", "analysis", "used")
##only use times that they used it or we are doubling up 
long.test.data = subset(long.test.data, used == 1)

##create a data frame of the percentages
analysessummary = table(long.test.data$mention.outliers, long.test.data$time.pulled, long.test.data$analysis)
analysessummary = as.data.frame(analysessummary)
colnames(analysessummary) = c("mention.outliers", "time.pulled",  "analysis.type", "Freq")

analysespercent = as.data.frame(group_by(analysessummary, time.pulled, analysis.type) %>%
  mutate(percent = Freq/sum(Freq)*100))
```

```{r analyses-graph, echo = FALSE, fig.cap = "Percent of outlier mentions by analysis type and year examined. Error bars represent 95% confidence interval.", fig.height = 6, fig.width = 8}
##only pull mention outliers
graphdata = subset(analysespercent, mention.outliers == "Yes")

#calculate CIs to add to graph
graphdata$sample = as.data.frame(table(long.test.data$time.pulled, long.test.data$analysis))$Freq
graphdata$SE = sqrt(graphdata$percent*(100-graphdata$percent)/graphdata$sample)

dotplot2 = ggplot(graphdata, aes(analysis.type, percent, color = time.pulled))
dotplot2 = dotplot2 +
  geom_pointrange(aes(ymin=percent-1.96*SE, ymax=percent+1.96*SE))+ 
  cleanup +
  xlab("Type of Analysis") +
  ylab("Percent of Outlier Mentions") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  scale_color_manual(name = "Year", 
                     values = c("black", "gray")) + 
  coord_cartesian(ylim = c(0,50))
dotplot2
```

```{r analysis-mlm, include=FALSE}
#now need to see if they vary by analysis
basic.data=long.test.data[which(long.test.data$analysis=="Basics"),]
anova.data=long.test.data[which(long.test.data$analysis=="ANOVA"),]
reg.data=long.test.data[which(long.test.data$analysis=="Regression"),]
chi.data=long.test.data[which(long.test.data$analysis=="ChiSquare"),]
nonpar.data=long.test.data[which(long.test.data$analysis=="Nonparametric"),]
model.data=long.test.data[which(long.test.data$analysis=="Modeling"),]
bo.data=long.test.data[which(long.test.data$analysis=="BayesOther"),]

m1basic <- glmer(mention.outliers ~ year + (1 | Journal) + (1 | article),
            data=basic.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1basic)

m1anova <- glmer(mention.outliers ~ year + (1 | Journal) + (1 | article),
            data=anova.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1anova)

m1reg <- glmer(mention.outliers ~ year + (1 | Journal) + (1 | article),
            data=reg.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1reg)

m1chi <- glmer(mention.outliers ~ year + (1 | Journal) + (1 | article),
            data=chi.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1chi)

m1non <- glmer(mention.outliers ~ year + (1 | Journal) + (1 | article),
            data=nonpar.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1non)

m1model <- glmer(mention.outliers ~ year + (1 | Journal) + (1 | article),
            data=model.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1model)

m1bo <- glmer(mention.outliers ~ year + (1 | Journal) + (1 | article),
            data=bo.data, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1bo)
```

```{r analysis-table, echo = FALSE, results = 'asis'}
tableprint = matrix(NA, nrow = 7, ncol = 8)

tableprint[ , 1] = c("Basic Statistics", "ANOVA", "Regression", "Chi-Square",
                    "Non-Parametric", "Modeling", "Bayesian or Other")
tableprint[ ,2] = printnum(graphdata$percent[graphdata$time.pulled == "2012"], digits = 1)
tableprint[ ,3] = printnum(graphdata$sample[graphdata$time.pulled == "2012"], digits = 0)
tableprint[ ,4] = printnum(graphdata$percent[graphdata$time.pulled == "2017"], digits = 1)
tableprint[ ,5] = printnum(graphdata$sample[graphdata$time.pulled == "2017"], digits = 0)
tableprint[ , 6] = printnum(c(exp(summary(m1basic)$coefficients[2,1]), 
                     exp(summary(m1anova)$coefficients[2,1]),
                     exp(summary(m1reg)$coefficients[2,1]),
                     exp(summary(m1chi)$coefficients[2,1]),
                     exp(summary(m1non)$coefficients[2,1]),
                     exp(summary(m1model)$coefficients[2,1]),
                     exp(summary(m1bo)$coefficients[2,1])), digits = 2)
tableprint[ , 7] = printnum(c(summary(m1basic)$coefficients[2,3], 
                     summary(m1anova)$coefficients[2,3],
                     summary(m1reg)$coefficients[2,3],
                     summary(m1chi)$coefficients[2,3],
                     summary(m1non)$coefficients[2,3],
                     summary(m1model)$coefficients[2,3],
                     summary(m1bo)$coefficients[2,3]), digits = 2)
tableprint[ , 8] = printp(c(summary(m1basic)$coefficients[2,4], 
                     summary(m1anova)$coefficients[2,4],
                     summary(m1reg)$coefficients[2,4],
                     summary(m1chi)$coefficients[2,4],
                     summary(m1non)$coefficients[2,4],
                     summary(m1model)$coefficients[2,4],
                     summary(m1bo)$coefficients[2,4]))
apa_table(as.data.frame(tableprint),
                caption = "Outlier Reporting by Analysis Type Across Years",
                align = c("l", rep("c",7)),
                escape = F,
                format = "latex",
                note = "$N$ represents number of experiments for each category.",
                col.names = c("Analysis", "\\% 12", "$N$ 12", "\\% 17", 
                              "$N$ 17", "OR", "$Z$", "$p$"))

```

Table \@ref(tab:analysis-table) indicates the types of analyses across years that mention outliers, and Figure \@ref(fig:analyses-graph) visually depicts these findings. An increase in reporting was found for non-parametric statistics (`r printnum(exp(summary(m1non)$coefficients[2,1])*100-100, digits = 1)`%), basic statistics (`r printnum(exp(summary(m1basic)$coefficients[2,1])*100-100, digits = 1)`%), modeling (`r printnum(exp(summary(m1model)$coefficients[2,1])*100-100, digits = 1)`%), ANOVA (`r printnum(exp(summary(m1anova)$coefficients[2,1])*100-100, digits = 1)`%), and regression (`r printnum(exp(summary(m1reg)$coefficients[2,1])*100-100, digits = 1)`%). Bayesian and other statistics additionally showed a comparable increase, `r printnum(exp(summary(m1bo)$coefficients[2,1])*100-100, digits = 1)`%, which was not deemed a significant change over years. 

## Type of Outlier

```{r typeout, include=FALSE}
peep_data = subset(master, mention.outliers == "Yes")

##create a data frame of the percentages
peep_datasummary = table(peep_data$peopleor.data.points)

peep_datasummary = as.data.frame(peep_datasummary)
colnames(peep_datasummary) = c("code", "Freq")

peep_datapercent = as.data.frame(group_by(peep_datasummary) %>%
  mutate(percent = Freq/sum(Freq)*100))
```

```{r type-mlm, include=FALSE}
##people versus no outliers
people = subset(master, peopleor.data.points == "people" | peopleor.data.points == "")
datapoints = subset(master, peopleor.data.points == "data points" | peopleor.data.points == "")

m1people <- glmer(mention.outliers ~ year + (1 | Journal) + (1 | article),
            data=people, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1people)

m1dp <- glmer(mention.outliers ~ year + (1 | Journal) + (1 | article),
            data=datapoints, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1dp)

whatdid = table(master$run.with.or.without[master$run.with.or.without !=""])/sum(table(master$run.with.or.without[master$run.with.or.without !=""]))*100
```

In our review, the majority of outliers mentioned referred to people (`r printnum(peep_datapercent$percent[5], digits = 1)`%) as opposed to data points (`r printnum(peep_datapercent$percent[3], digits = 1)`%), or both people and data points (`r printnum(peep_datapercent$percent[2], digits = 1)`%), and a final small set (`r printnum(peep_datapercent$percent[4], digits = 1)`%) of experiments mentioned outliers but did not specify a type, just that they searched for outliers and found none. The trends across years were examined for mentioning outliers (yes/no) for both people and data points, dropping the both and none found categories due to small size. Therefore, the dependent variable was outlier mention where the "yes" category indicated either the people or data point categories separately. The mentions of excluding entire participants increased across years, `r printnum(abs(exp(summary(m1people)$coefficients[2,1])*100-100), digits = 1)`%, *Z* = `r printnum(summary(m1people)$coefficients[2,3], digits = 2)`, *p* = `r printp(summary(m1people)$coefficients[2,4])`, while the mention of data trial exclusion was consistent across years, `r printnum(abs(exp(summary(m1dp)$coefficients[2,1])*100-100), digits = 1)`%, *Z* = `r printnum(summary(m1dp)$coefficients[2,3], digits = 2)`, *p* = `r printp(summary(m1dp)$coefficients[2,4])`. Overall, when handling these data, few experiments chose to winsorize the data (`r printnum(whatdid["winsorized"], digits = 1)`%), most analyzed the data without the observations (`r printnum(whatdid["without"], digits = 1)`%), some analyzed the data with the observations (`r printnum(whatdid["with"], digits = 1)`%), and some conducted analyses both with and without the observations (`r printnum(whatdid["both"], digits = 1)`%). 

## Reason for Exclusion

```{r reasonsout, include=FALSE}
table(master$reason.code)
reasons = subset(master, mention.outliers == "Yes")

##create reason columns
reasons$part = 0
reasons$part[ grep("part", reasons$reason.code)] = 1
reasons$stat = 0
reasons$stat[ grep("stat", reasons$reason.code)] = 1
reasons$unus = 0
reasons$unus[ grep("unus", reasons$reason.code)] = 1

##create a data frame of the percentages
reasonssummary = table(reasons$part)
reasonssummary = as.data.frame(reasonssummary)
colnames(reasonssummary) = c("yesno", "part.freq")
reasonssummary$stat.freq = as.data.frame(table(reasons$stat))$Freq
reasonssummary$unus.freq = as.data.frame(table(reasons$unus))$Freq

reasonspercent = as.data.frame(group_by(reasonssummary) %>%
  mutate(part.percent = part.freq/sum(part.freq)*100) %>%
  mutate(stat.percent = stat.freq/sum(stat.freq)*100) %>%
  mutate(unus.percent = unus.freq/sum(unus.freq)*100))
```

```{r reason-mlm, include=FALSE}
reasondata = master[ master$mention.outliers == "Yes", c("Journal", "year", "reason.code", "article")]

##create reason columns
reasondata$part = 0
reasondata$part[ grep("part", reasondata$reason.code)] = 1
reasondata$stat = 0
reasondata$stat[ grep("stat", reasondata$reason.code)] = 1
reasondata$unus = 0
reasondata$unus[ grep("unus", reasondata$reason.code)] = 1

m1part <- glmer(part ~ year + (1 | Journal) + (1 | article),
            data=reasondata, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1part)

m1stat <- glmer(stat ~ year + (1 | Journal) + (1 | article),
            data=reasondata, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1stat)

m1unus <- glmer(unus ~ year + (1 | Journal) + (1 | article),
            data=reasondata, 
            family=binomial(link="logit"),
            control = glmerControl(optimizer = "bobyqa"),
            nAGQ = 0)
#summary(m1unus)
```

We found that researchers often used multiple criterion checks for outlier coding, as one study might exclude participants for exceeding a standard deviation cut-off, while also excluding participants for low effort data. Therefore, reason coding was not unique for each experiment, and each experiment could have one to three reasons for data exclusion. Statistical reasoning was the largest reported exclusion criteria of papers that mentioned outliers at `r printnum(reasonspercent$stat.percent[2], digits = 1)`%. Next, participant reasons followed with `r printnum(reasonspercent$part.percent[2], digits = 1)`% of outlier mentions, and unusable data was coded in `r printnum(reasonspercent$unus.percent[2], digits = 1)`% of experiments that mentioned outliers. To examine the trend over time, we used a similar MLM analysis as described in the data analytic plan, with journal and article as a random intercept, year as the independent variable, and the mention of type of outlier (yes/no for participant, statistical, or unusable data) as the dependent variables separately. Statistical reasons tended to decrease about `r printnum(abs(exp(summary(m1stat)$coefficients[2,1])*100-100), digits = 1)`% each year, *Z* = `r printnum(summary(m1stat)$coefficients[2,3], digits = 2)`, *p* = `r printp(summary(m1stat)$coefficients[2,4])`. Participant reasons increased  by `r printnum(abs(exp(summary(m1part)$coefficients[2,1])*100-100), digits = 1)`% each year, *Z* = `r printnum(summary(m1part)$coefficients[2,3], digits = 2)`, *p* = `r printp(summary(m1part)$coefficients[2,4])`. Unusable data increased by about `r printnum(abs(exp(summary(m1unus)$coefficients[2,1])*100-100), digits = 1)`% each year, *Z* = `r printnum(summary(m1unus)$coefficients[2,3], digits = 2)`, *p* = `r printp(summary(m1unus)$coefficients[2,4])`. None of these trends would be considered "significant"; however, their pattern is an interesting finding to see that traditional deviant data points for statistical reasons was decreasing, while there was increased reporting for other types of deviant data. 

# Discussion

We hypothesized that report rates for outliers would increase overall in experiments from 2012 to 2017, and we largely found support for this hypothesis. We additionally hypothesized larger increases in report rates of outliers for the domains of social and cognitive psychology because of the overwhelming response to the @ScienceCollaboration2015 publication. This hypothesis was supported, with increasing trends for both areas, along with most other sub-domains in our study. Social and cognitive psychology publications included the most experiments in their papers, and reporting outliers for each experiment and analysis will be crucial for future studies or meta-analyses. While improvements in reporting can be seen in almost all fields, it is worthwhile to note that in 2017 the average proportion of experiments reporting outliers was still only `r printnum(overalloutpercent$percent[4], digits = 1)`%, with some fields as low as approximately 12%. While the effort of many fields should not be overlooked, we suggest that there is still room for improvement overall.

All analytic techniques presented in these experiments showed increased reporting over time, ranging from `r printnum(exp(summary(m1model)$coefficients[2,1])*100-100, digits = 1)`% for modeling to `r printnum(exp(summary(m1non)$coefficients[2,1])*100-100, digits = 1)`% for nonparametric statistics. Of all outliers reported, we found that the majority discussed were people (`r printnum(peep_datapercent$percent[5], digits = 1)`%), and that while reporting of exclusion of people as outliers increased from 2012 to 2017, reporting of exclusion of outlying data points remained consistent across time. Most experiments cited outliers as those found through statistical means (e.g., Mahalanobis distance, leverage, or a standard deviation rule) and/or participant reasons (e.g., failed attention checks or failure to follow instructions), but a small subset were cited as unusable data (e.g., individuals who believed the procedure was staged or participants whose position in a room was never recorded by the experimenter). The trends across years indicate that potentially, the detection of outliers as only a statistical technique is decreasing, while transparently presenting information about the exclusion of other errant data is increasing. These findings suggest that not only is discussion of outliers important for the study at hand, but also for future studies. Given insight into ways data can become unusable, a researcher is better equipped to prepare for and deter unusable data from arising in future studies through knowledge of past failures that can improve their research design. 

Given the frequentist nature of most psychological work, and the impact those outliers can have on these statistics [@Cook1980; @Stevens1984], research would be well served if authors described outlier data analysis in their reports. One confounding issue may be journal word limits. The Open Science Framework provides the option to publish online supplemental materials that can be referenced in manuscripts with permanent  identifiers (i.e., weblinks and DOIs). Potentially, if journal or reviewer comments indicate shortening data analyses sections, the detailed specifics of these plans can be shifted to these online resources. While the best practice may be to include this information in the published article (as @Bakker2014 note that sample size and degrees of freedom are often inconsistent and difficult to follow in publications), online materials can be useful when that option is restricted. 

We recommend that those researchers who create reporting guidelines and checklists ensure that they address the need to discuss if outliers were identified, and if so, how outliers were identified, how these outliers were handled, and any differences that arose in conclusions when outliers were excluded/included. We believe this information should be included in every single publication that includes human subjects data (even simply to report that no outliers were found). We implore researchers not to overlook the importance of visualizing their data and identifying data that may not fall within the expected range or pattern of the sample. We suggest that all researchers implement relevant checklists or guidelines (a list of which can be found at https://osf.io/kgnva/wiki/home/) when handling, analyzing, and reporting their data as following these types of checklists has been shown to make a modest improvement in some reporting practices [@Caron2017; @Macleod2017]. Additionally, there are online tools that can assist even the most junior researcher in the cleaning of data, including outlier detection and handling, for almost any type of analysis. From online courses (e.g., Coursera.org, DataCamp.com), free software with plugins [e.g., JASP and jamovi; @JASP2018; @jamovi2018], and YouTube tutorials that detail the step by step procedures (available from the second author at StatsTools.com), inexperienced researchers can learn more and better their reporting and statistical practices. Further, we recommend that those who are reviewers and editors consider data screening procedures when assessing research articles and request this information be added when it is absent from a report. This article spotlights the positive changes that are occurring as researchers are actively reshaping reporting practices in response to the conversation around transparency. We believe that these results present a positive outlook for the future of the psychological sciences, especially when coupled with the training, reviewer feedback, and incentive structure change, that can only improve our science.

\newpage
#Open Practices Statement 

The data and materials for this manuscript are available at https://osf.io/52mqw/ and no hypotheses were preregistered.

# References

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}


