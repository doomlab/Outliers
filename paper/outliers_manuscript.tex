\documentclass[english,man]{apa6}

\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}

% Table formatting
\usepackage{longtable, booktabs}
\usepackage{lscape}
% \usepackage[counterclockwise]{rotating}   % Landscape page setup for large tables
\usepackage{multirow}		% Table styling
\usepackage{tabularx}		% Control Column width
\usepackage[flushleft]{threeparttable}	% Allows for three part tables with a specified notes section
\usepackage{threeparttablex}            % Lets threeparttable work with longtable

% Create new environments so endfloat can handle them
% \newenvironment{ltable}
%   {\begin{landscape}\begin{center}\begin{threeparttable}}
%   {\end{threeparttable}\end{center}\end{landscape}}

\newenvironment{lltable}
  {\begin{landscape}\begin{center}\begin{ThreePartTable}}
  {\end{ThreePartTable}\end{center}\end{landscape}}

  \usepackage{ifthen} % Only add declarations when endfloat package is loaded
  \ifthenelse{\equal{\string man}{\string man}}{%
   \DeclareDelayedFloatFlavor{ThreePartTable}{table} % Make endfloat play with longtable
   % \DeclareDelayedFloatFlavor{ltable}{table} % Make endfloat play with lscape
   \DeclareDelayedFloatFlavor{lltable}{table} % Make endfloat play with lscape & longtable
  }{}%



% The following enables adjusting longtable caption width to table width
% Solution found at http://golatex.de/longtable-mit-caption-so-breit-wie-die-tabelle-t15767.html
\makeatletter
\newcommand\LastLTentrywidth{1em}
\newlength\longtablewidth
\setlength{\longtablewidth}{1in}
\newcommand\getlongtablewidth{%
 \begingroup
  \ifcsname LT@\roman{LT@tables}\endcsname
  \global\longtablewidth=0pt
  \renewcommand\LT@entry[2]{\global\advance\longtablewidth by ##2\relax\gdef\LastLTentrywidth{##2}}%
  \@nameuse{LT@\roman{LT@tables}}%
  \fi
\endgroup}


  \usepackage{graphicx}
  \makeatletter
  \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
  \def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
  \makeatother
  % Scale images if necessary, so that they will not overflow the page
  % margins by default, and it is still possible to overwrite the defaults
  % using explicit options in \includegraphics[width, height, ...]{}
  \setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            pdfauthor={},
            pdftitle={Have researchers increased reporting of outliers in response to the reproducability crisis?},
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=black,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls

\setlength{\parindent}{0pt}
%\setlength{\parskip}{0pt plus 0pt minus 0pt}

\setlength{\emergencystretch}{3em}  % prevent overfull lines

\ifxetex
  \usepackage{polyglossia}
  \setmainlanguage{}
\else
  \usepackage[english]{babel}
\fi

% Manuscript styling
\captionsetup{font=singlespacing,justification=justified}
\usepackage{csquotes}
\usepackage{upgreek}

 % Line numbering
  \usepackage{lineno}
  \linenumbers


\usepackage{tikz} % Variable definition to generate author note

% fix for \tightlist problem in pandoc 1.14
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

% Essential manuscript parts
  \title{Have researchers increased reporting of outliers in response to the
reproducability crisis?}

  \shorttitle{Outlier reporting}


  \author{K. D. Valentine\textsuperscript{1}, Erin M. Buchanan\textsuperscript{2}, Arielle Cunningham\textsuperscript{2}, Tabetha Hopke\textsuperscript{2}, Addie Wikowsky\textsuperscript{2}, \& Haley Wilson\textsuperscript{2}}

  % \def\affdep{{"", "", "", "", "", ""}}%
  % \def\affcity{{"", "", "", "", "", ""}}%

  \affiliation{
    \vspace{0.5cm}
          \textsuperscript{1} University of Missouri\\
          \textsuperscript{2} Missouri State University  }

  \authornote{
    K. D. Valentine is a Ph.D.~candidate at the University of Missouri. Erin
    M. Buchanan is an Associate Professor of Quantitative Psychology at
    Missouri State University. ADD HERE STUFF AND THINGS.
    
    Correspondence concerning this article should be addressed to K. D.
    Valentine, 210 McAlester Hall, Columbia, MO, 65211. E-mail:
    \href{mailto:kdvdnf@mail.missouri.edu}{\nolinkurl{kdvdnf@mail.missouri.edu}}
  }


  \abstract{Psychology is currently experiencing a ``renaissance'' where replication
and reproducibility of published reports is at the forefront of the
field. While researchers have worked to discuss possible problems and
solutions, work has yet to uncover how this new culture may alter
reporting practices in Psychology. As outliers can bias both descriptive
and inferential statistics, the examination for these data points is
essential to any analysis using these parameters. We quantified the
rates of reporting of outliers within psychology at two time points:
2012 when the replication crisis was born, and 2017, after the
publication of reports concerning replication, questionable research
practices, and transparency. A total of 2234 experiments were identified
and analyzed, finding an increase in reporting of outliers from only
15.7\% of experiments mentioning outliers in 2012 to 25.1\% in 2017. We
further delve into differences across years given the psychological
field or statistical analysis that experiment employed. Further, we
inspect whether outliers mentioned are people or data points, and what
reasons authors gave for stating the observation was deviant. We
conclude that while report rates are improving overall, there is still
room for improvement in the reporting practices of psychological
scientists which can only aid in strengthening our science.}
  \keywords{outlier, influential observation, replication \\

    
  }





\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\theoremstyle{definition}
\newtheorem{example}{Example}[section]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}

\maketitle

\setcounter{secnumdepth}{0}



Psychology is undergoing a \enquote{renaissance} in which focus has
shifted to the replication and reproducibility of current published
reports (Etz \& Vandekerckhove, 2016; Lindsay, 2015; Nelson, Simmons, \&
Simonsohn, 2018; Open Science Collaboration, 2015; Van Elk et al.,
2015). A main concern has been the difficulty in replicating phenomena,
often attributed to publication bias (Ferguson \& Brannick, 2012), the
use and misuse of \emph{p}-values (Gigerenzer, 2004; Ioannidis, 2005),
and researcher degrees of freedom (Simmons, Nelson, \& Simonsohn, 2011).
In particular, this analysis focused on one facet of questionable
research practices that affect potential replication (QPRs),
specifically, the selective removal or inclusion of data points.

As outlined by Nelson et al. (2018), the social sciences turned inward
to examine its practices due to the publication of unbelievable data
(Wagenmakers, Wetzels, Borsboom, \& Maas, 2011), academic fraud
(Simonsohn, 2013), failures to replicate important findings (Doyen,
Klein, Pichon, \& Cleeremans, 2012), and the beginning of the Open
Science Framework (Center for Open Science, n.d.). These combined forces
led to the current focus on QRPs and \emph{p}-hacking and the
investigation potential solutions to these problems. Recommendations
included integrating effect sizes into results (Cumming, 2008; Lakens,
2013), implementing full disclosure and encouraging researchers to be
transparent about their research practices, including not only the
design and execution of their experiments, but especially the data
preparation and resulting analyses (Simmons et al., 2011), attempting
and interpreting well thought out replication studies (Asendorpf et al.,
2012; Maxwell, Lau, \& Howard, 2015), altering the way we think about
\emph{p}-values (Benjamin et al., 2018; Lakens, Adolfi, Albers, \& al.,
2018; Valentine, Buchanan, \& Scofield, n.d.), and restructuring
incentives (Nosek, Spies, \& Motyl, 2012). Additionally, Klein et al.
(2014) developed the Many Labs project to aid in data collection for
increased power, while the Open Science Collaboration (2015) published
their findings from a combined many labs approach about the replication
of phenomena in psychology.

While we have seen vast discussion of the problems and proposed
solutions, research has yet to determine how this new culture may have
impacted reporting practices of researchers. Herein, we aim specifically
to quantify the rates of reporting of outliers within psychology at two
time points: 2012 when the replication crisis was born (Pashler \&
Wagenmakers, 2012), and 2017, after the publication of reports
concerning QPRs, replication, and transparency (Miguel et al., 2014).

\subsection{Outliers}\label{outliers}

Bernoulli first mentioned outliers in 1777 starting the long history of
examining for discrepant observations (Bernoulli, 1977), which can bias
both descriptive and inferential statistics (Cook \& Weisberg, 1980;
Stevens, 1984). Therefore, the examination for these data points is
essential to any analysis using these parameters, as outliers can impact
study results. Outliers have been defined as influential observations or
fringliers but specifically we use the definition of \enquote{an
observation which being atypical and/or erroneous deviates decidedly
from the general behavior of experimental data with respect to the
criteria which is to be analyzed on it} (Muñoz-garcia, Moreno-Rebollo,
\& Pascual-Acosta, 1990, pg 217). However, the definition of outliers
can vary from researcher to researcher, as a wide range of graphical and
statistical options are available for outlier detection (Beckman \&
Cook, 1983; Hodge \& Austin, 2004; Orr, Sackett, \& Dubois, 1991;
Osborne \& Overbay, 2004). For example, Tabachnick and Fidell (2012)
outline several of the most popular detection methods including visual
data inspection, residual statistics, a set number of standard
deviations, Mahalanobis distance, Leverage, and Cook's distances. Before
the serious focus on QRPs, the information regarding outlier detection
as part of data screening was often excluded from publication,
particularly if a journal page limit requirement needed to be
considered. Consider, for example, THIS WAS THE WRONG STUDY/FIND THE
RIGHT STUDY, who inspected 100 Industrial/Organizational Psychology
personnel studies and found no mention of outliers.

However, outlier detection and removal is likely part of a researchers
data screening procedure, even if it does not make the research
publication. LeBel et al. (2013) that 11\% of psychology researchers
stated that they had not reported excluding participants for being
outliers in their papers. Fiedler and Schwarz (2016) suggested that more
than a quarter of researchers decide whether to exclude data only after
looking at the impact of doing so. Bakker and Wicherts (2014)
investigated the effects of outliers on published analyses, and while
they did not find that they affected the surveyed results, they do
report that these findings are likely biased by the non-reporting of
data screening procedures, as sample sizes and degrees of freedom often
did not match. The lack of transparency in data manipulation and
reporting is problematic.

By keeping outliers in a dataset, analyses are more likely to have
increased error variance (depending on sample size, Orr et al., 1991),
biased estimates {[}Osborne2004{]}, and reduced effect size and power
(Orr et al., 1991; Osborne \& Overbay, 2004), which can alter the
results of the analysis and lead to falsely supporting (Type I error),
or denying a claim (Type II error). Inconsistencies in the treatment and
publication of outliers could also lead to the failures to replicate
previous work, as it would be difficult to replicate analyses that have
been \emph{p}-hacked into \enquote{just-significant} results (Leggett,
Thomas, Loetscher, \& Nicholls, n.d.; Nelson et al., 2018). The
influence of this practice can be wide spread, as non-reporting of data
manipulation can negatively affect meta-analyses, effect size, and
sample size estimates for study planning. On the other hand, outliers do
not always need to be seen as nuisance, as they will often be
informative to researchers as they can encourage the diagnosis, change,
and evolution of a research model (Beckman \& Cook, 1983). Taken
together, a lack of reporting of outlier practices can lead to
furthering unwarranted avenues of research, ignoring important
information, creating erroneous theories, and failure to replicate, all
of which serve to weaken the sciences. Clarifying the presence or
absence of outliers, how they were assessed, and how they were handled,
can improve our transparency and replicability, and ultimately help to
strengthen our science.

The current zeitgeist of increased transparency and reproducibility
applies not only to the manner in which data is collected, but also the
various ways the data is transformed, cleaned, pared down, and analyzed.
Therefore, it can be argued that it is just as important for a
researcher to state how they identified outliers within their data, how
the outliers were handled, and how this choice of handling impacted the
estimates and conclusions of their analyses, as it is for them to report
their sample size. Given the timing of the renaissance, we expected to
find a positive change in reporting ratings for outliers in 2017, as
compared to 2012. This report spans a wide range of psychological
sub-domains, however, we also expected the impact of the Open Science
Collaboration (2015) publication to affect social and cognitive
psychology more than other fields.

\section{Method}\label{method}

\subsection{Fields}\label{fields}

A list of psychological sub-domains was created to begin the search for
appropriate journals to include. The authors brainstormed the list of
topics (shown in Table \ref{tab:info-table}) by first listing major
research areas in psychology (i.e., cognitive, clinical, social, etc.).
Second, a list of common courses offered at large universities was
consulted to add to the list of fields. Lastly, the American
Psychological Association's list of divisions was examined for any
potential missed fields. The topic list was created to capture large
fields of psychology with small overlap (i.e., cognition and
neuropsychology) while avoiding specific sub-fields of topics (i.e.,
cognition, perception, and memory). Sixteen fields were initially
identified, however only thirteen were included in final analysis due to
limitations noted below.

\subsection{Journals}\label{journals}

Once these fields were agreed upon, researchers used various search
sources (Google, EBSCO host databases) to find journals that were
dedicated to each broad topic. Journals were included if they appeared
to publish a wide range of articles within the selected fields. A list
of journals, publishers, and impact factors (as noted by each journals
website in Spring of 2013 and 2018) were identified for each field. Two
journals from each field were selected based on the following criteria:
1) high impact factors over one at minimum, 2) a mix of publishers, if
possible, and 3) availability due to university resources. These
journals are shown in the online supplemental materials at
\url{https://osf.io/52mqw/}.

\subsection{Articles}\label{articles}

Fifty articles from each journal were examined for data analysis: 25
articles were collected beginning in Spring 2013 for 2012 and in Fall
2017. Data collection of articles started at the last volume publication
from the given year (2012 or 2017) and progressed backwards until 25
articles had been found. We excluded online first publications and
started in 2012 to ensure time for errata and retraction of articles.
Articles were included if they met the following criteria: 1) included
data analyses, 2) included multiple participants or data-points, and 3)
analyses were based on human subjects or stimuli. Therefore, we excluded
theory articles, animal populations, and single subject designs. Based
on review for the 2012 articles, three fields were excluded. Applied
Behavior Analysis articles predominantly included single-subject
designs, evolutionary psychology articles were primarily theory
articles, and statistics related journal articles were based on user
created data with specific set characteristics. Since none of these
themes fit into our analysis of understanding data screening with human
subject samples, we excluded those three fields from analyses.

\subsection{Data Processing}\label{data-processing}

Each article was then reviewed for key components of data analysis. Each
experiment in an article was coded separately. For each experiment, the
type of analysis conducted, number of participants/stimuli analyzed, and
whether or not they made any mention of outliers were coded.

\subsubsection{Analysis types}\label{analysis-types}

Types of analyses were broadly defined as basic statistics (descriptive
statistics, \emph{z}-scores, \emph{t}-tests, and correlations), ANOVAs,
regressions, chi-squares, non-parametric statistics, modeling, and
Bayesian/other analyses.

\subsubsection{Outlier coding}\label{outlier-coding}

For outliers, we used a dichotomous yes/no for if they were mentioned in
an article. Outliers were not limited to simple statistical analysis of
discrepant responses, but we also checked for specific exclusion
criteria that were not related to missing data or study characteristics
(i.e., we did not consider it an outlier if they were only looking for
older adults). If so, we coded information about outliers into four
types: 1) people, 2) data points, 3) both, or 4) none found. The
distinction between people and data points was if individual trials were
eliminated or if entire participant data was eliminated. We found that a
unique code for data points was important for analyses with response
time studies where individual participants were not omitted but rather
specific data trials were eliminated.

Then, for those articles that mentioned outliers, the author's decision
for how to handle the outliers was coded into whether they removed
participants/stimuli, left these outliers in the analysis, or winsorized
the data points. Experiments were coded for whether they tested the
analyses with, without, or both for determination of their effect on the
study. If they removed outliers, a new sample size was recorded;
although, this data was not analyzed, as we determined it was conflated
with removal of other types of data unrelated to the interest of this
paper (i.e., missing data). Lastly, we coded the reasoning for outlier
detection as one or more of the following: 1) Statistical reason (i.e.,
used numbers to define odd or deviant behavior in responding, such as
\emph{z}-score or Mahalanobis distance scores), 2) Participant error
(i.e., failed attention checks, did not follow instructions, or low
quality data because of participant problems), and 3) Unusable data
(i.e., inside knowledge of the study or experimenter/technological
problems).

\section{Results}\label{results}

\subsection{Data Analytic Plan}\label{data-analytic-plan}

Because each article constituted multiple data points within the dataset
which were each nested within a particular journal, a multilevel model
(MLM) was used to control for correlated error (Gelman, 2006). Pinheiro,
Bates, Debroy, Sarkar, and Team's (2017) \emph{nlme} package in \emph{R}
was used to calculate these analyses. A maximum likelihood logistic
multilevel model was used to examine how the year in which the
experiment was published predicted the likelihood of mentioning outliers
(yes/no) while including a random intercept for journal. This model was
run over all of the data, as well as broken down for each field, as well
as each type of analysis in order to glean a more detailed account of
the effect of year on outlier reporting. Additionally, 3 MLMs were
analyzed attempting to individually predict each outlier reason
(i.e.~statistical reason yes/no; unusable data yes/no; participant
reason yes/no) given the year while including a random intercept for
journal. All code and data can be viewed at osf.io/52mqw. We futher
explored whether these outliers were people or data points, how outliers
were handled, and the reasons data were named outliers with descriptive
statistics.

\subsection{Overall Outliers}\label{overall-outliers}

Data processing resulted in a total of 2235 experiments being coded,
1085 of which were from 2012 or prior, with the additional 1150 being
from 2017 or prior. Investigating reporting of outliers, we found that
in 2012, 15.7\% of experiments mentioned outliers, while in 2017 25.0\%
of experiments mentioned outliers. Actual publication year was used to
predict outlier mention (yes/no) with a random intercept for journal, as
described above. We found that publication year predicted outlier
mentions, \emph{Z} = 5.78, \emph{p} \textless{} .001. Each year,
experiments were 13.5\% more likely to report outliers as the previous
year.

\subsection{Fields}\label{fields-1}

Further exploration reveals that differences in reporting between years
arise between fields which can be seen in Table \ref{tab:info-table}.
Figure \ref{fig:type-graph} displays the percentage of outlier mentions
of each field colored by year examined. A MLM was analyzed for each
field using journal as a random intercept to determine the influence of
year of publication on outlier reporting rates. Specifically, if we look
at the change in reporting for each field analyzed at the level of the
experiment, we find the largest changes in forensic (44.9\% more likely
to report), social (33.7\%), and I/O (33.4\%), followed by developmental
(19.6\%), counseling (20.2\%), and cognitive (15.2\%). In support of our
hypothesis, we found that social and cognitive fields showed increases
in their outlier reporting; however, it was encouraging to see positive
trends in other fields as well. These analyses show that in some fields,
including our overall and neurological fields, we found a decrease in
reporting across years, although these changes were not significant.

The analyses shown below were exploratory based on the findings when
coding each experiment for outlier data. We explored the relationship of
outlier reporting to the type of analysis used to support research
hypotheses, reasons for why outliers were excluded, as well as the type
of outlier excluded from the study.

\begin{figure}
\centering
\includegraphics{outliers_manuscript_files/figure-latex/type-graph-1.pdf}
\caption{\label{fig:type-graph}Percent of outlier mentions by sub-domain
field and year examined. Error bars represent 95\% confidence interval.}
\end{figure}

\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{\label{tab:info-table}Outlier Reporting by Field Across Years}
\begin{tabular}{lccccccc}
\toprule
Field & \% 12 & $N$ 12 & \% 17 & $N$ 17 & OR & $Z$ & $p$\\
\midrule
Clinical & 9.3 & 54 & 12.0 & 50 & 1.06 & 0.43 & .665\\
Cognitive & 31.1 & 164 & 49.6 & 135 & 1.15 & 3.05 & .002\\
Counseling & 14.3 & 56 & 28.1 & 57 & 1.20 & 1.93 & .053\\
Developmental & 20.0 & 70 & 34.4 & 61 & 1.20 & 2.20 & .028\\
Educational & 8.9 & 56 & 12.1 & 58 & 1.07 & 0.54 & .586\\
Environmental & 12.1 & 58 & 12.1 & 58 & 1.01 & 0.05 & .957\\
Forensics & 3.2 & 62 & 18.6 & 70 & 1.45 & 2.50 & .012\\
IO & 5.8 & 104 & 18.5 & 124 & 1.33 & 2.92 & .003\\
Methods & 13.6 & 66 & 11.5 & 61 & 1.01 & 0.06 & .948\\
Neuro & 30.5 & 59 & 17.9 & 56 & 0.87 & -1.55 & .121\\
Overview & 21.9 & 114 & 18.9 & 132 & 0.96 & -0.64 & .523\\
Social & 9.8 & 164 & 33.8 & 231 & 1.34 & 5.08 & < .001\\
Sports & 6.9 & 58 & 12.3 & 57 & 1.08 & 0.63 & .526\\
\bottomrule
\end{tabular}
\end{threeparttable}
\end{center}
\end{table}

\subsection{Analyses Type}\label{analyses-type}

\begin{figure}
\centering
\includegraphics{outliers_manuscript_files/figure-latex/analyses-graph-1.pdf}
\caption{\label{fig:analyses-graph}Percent of outlier mentions by analysis
type and year examined. Error bars represent 95\% confidence interval.}
\end{figure}

\begin{table}[tbp]
\begin{center}
\begin{threeparttable}
\caption{\label{tab:analysis-table}Outlier Reporting by Analysis Type Across Years}
\begin{tabular}{lccccccc}
\toprule
Analysis & \% 12 & $N$ 12 & \% 17 & $N$ 17 & OR & $Z$ & $p$\\
\midrule
Basic Statistics & 15.0 & 406 & 31.0 & 507 & 1.23 & 5.86 & < .001\\
ANOVA & 19.6 & 469 & 31.5 & 466 & 1.14 & 4.35 & < .001\\
Regression & 12.0 & 208 & 22.1 & 244 & 1.15 & 2.60 & .009\\
Chi-Square & 19.6 & 112 & 23.8 & 172 & 1.04 & 0.59 & .557\\
Non-Parametric & 6.2 & 64 & 25.5 & 47 & 1.38 & 2.67 & .008\\
Modeling & 12.0 & 217 & 21.8 & 408 & 1.12 & 2.20 & .028\\
Bayesian or Other & 13.2 & 53 & 25.9 & 143 & 1.23 & 1.61 & .107\\
\bottomrule
\end{tabular}
\end{threeparttable}
\end{center}
\end{table}

Table \ref{tab:analysis-table} indicates the types of analyses across
years that mention outliers, and Figure \ref{fig:analyses-graph}
visually depicts these findings. An increase in reporting was found for
non-parametric statistics (38.2\%), basic statistics (22.6\%),
regression (15.0\%), ANOVA (14.3\%), and modeling (11.8\%). Bayesian and
other statistics additionally showed a comparable increase, 23.4\%,
which was not a significant change over years.

\subsection{Type of Outlier}\label{type-of-outlier}

In our review, the majority of outliers mentioned referred to people
(65.9\%) as opposed to data points (25.4\%), or both people and data
points (5.7\%), and a final (3.1\%) of experiments mentioned outliers
but did not specify a type, just that they found none. The trends across
years were examined for mentioning outliers (yes/no) for both people and
data points, dropping the both and none found categories due to small
size. Therefore, the dependent variable was outlier mention where the
\enquote{yes} category indicated either the people or data point
categories separately. The mentions of excluding participants increased
across years, 17.1\%, \emph{Z} = 5.99, \emph{p} \textless{} .001, while
the mention of data point exclusion was consistent across years, 4.5\%,
\emph{Z} = 1.11, \emph{p} = .268. When handling these data, some
experiments chose to winzorize the data (0.7\%), most analyzed the data
without the observations (88.6\%), some analyzed the data with the
observations (7.4\%), and some conducted analyses both with and without
the observations (3.4\%).

\subsection{Reasons}\label{reasons}

We found that researchers often used multiple criterion checks for
outlier coding, as one study might exclude participants for exceeding a
standard deviation cut-off, while also excluding participants for low
effort data. Therefore, reason coding was not unique for each
experiment, and each experiment could have one to three reasons for data
exclusion. Statistical reasoning was the largest reported exclusion
criteria of papers that mentioned outliers at 58.0\%. Next, participant
reasons followed with 50.3\% of outlier mentions, and unusable data was
coded in 6.3\% of experiments that mentioned outliers. To examine the
trend over time, we used a similar MLM analysis as described in the our
data analytic plan, with Journal as a random intercept, year as the
independent variable, and the mention of type of outlier (yes/no for
participant, statistical, and unusable data) as the dependent variables
separately. Statistical reasons decreased by 8.4\%, \emph{Z} = -1.91,
\emph{p} = .056. Participant reasons increased over time by 13.7\%,
\emph{Z} = 2.92, \emph{p} = .004. Unusable data increased by about
5.3\%, \emph{Z} = 0.59, \emph{p} = .554.

\section{Discussion}\label{discussion}

We hypothesized that report rates for outliers would increase overall in
experiments from 2012 to 2017, and we largely we found support for this
hypothesis. We additionally hypothesized larger increases in report
rates of outliers for the domains of social and cognitive psychology.
This, too, bore out within our results. While modest improvements in
reporting can be seen in almost all fields, it is worthwhile to note
that in 2017 the average proportion of experiments reporting outliers
was still only x\%, with some fields as low as x\%. While the effort of
many fields should not be overlooked, we suggest that there is still a
large amount of room for improvement overall.

Further investigation into the rates of reporting over time broken down
by analysis revealed that all analytic techniques showed increased
reporting over time, ranging from 11.7\% for modeling to 38.2\% for
nonparametric statistics. Of all outliers reported, we found that the
majority discussed were people (65.9\%), and that while exclusion of
people as outliers increased from 2012 to 2017, exclusion of outlying
data points remained consistent across time. Interestingly, most
experiments cited outliers as those found through statistical means
(e.g.~mahalanobis distance, leverage, or the 3 standard deviation rule)
and/or participant reasons (e.g.~failed attention checks or failure to
follow instructions), but a small subset were cited as unusable data
(e.g.~individuals who believed the procedure was staged or participants
whose position in a room was never recorded by the experimenter). These
findings suggest that not only is discussion of outliers important for
the study at hand, but also for future studies. Given insight into ways
data can become unusable, a researcher is better equipped to prepare for
and deter unusable data from arising in future studies through knowledge
of past failures that can improve their research design.

While there may be many reasons an individual experiment does not speak
to outliers (ERIN PUT AN EXAMPLE HERE), we believe that given the
frequentist nature of most psychological work there are many more
reasons that an experiment should take the time and word length to
express the presence or absence of these data. Some may argue that use
of the precious word limit dictated by journals to describe such choices
as identification and handling of outliers may be irrational. However,
we contest that given the current availability and use of online
supplements and appendixes, as well as the invent of the Open Science
Framework (Center for Open Science, n.d.) researchers now have the
ability to upload any number of additional resources and supplements
that can be easily referenced in manuscripts. This should assist
researchers in moving detail-laden data cleaning procedures into a
supplemental document, allowing for a simple statement that data
cleaning was completed (leaving the details in the referenced
documentation), and reporting the resultant sample size. This can help
to alleviate a problem noted by Bakker and Wicherts (2014), that in many
articles where no data removal was reported, the reported sample size
was inconsistent with the reported degrees of freedom.

We implore researchers not to overlook the importance of visualizing
your data and identifying data---statically or otherwise---that may not
fall within the expected range or pattern of the sample. Currently,
there are a plethora of online tools that can assist even the most
junior researcher in the cleaning of data, including outlier detection
and handling, for almost any type of analysis. From online courses
(cite) to YouTube videos that walk you through the process step-by-step
(cite), we believe that if the failure to report these outliers is due
to a lack of learning, that this could quickly be fixed on a
case-by-case basis. Further, we implore those who are reviewers and
editors to think critically about this idea. If no mention is made of
outlier inspection within an article, perhaps it is worthwhile to ask
the researchers why this was so.

While these findings assist in showing the improvements made by this
renaissance in psychology, we believe there is still much work to be
done in this area. Future studies should inquire about other poor
researcher behaviors (such as QRPs) and neglected hallmarks of
psychology (such as replication). We believe that quantifying the
changes within our science is worthwhile to show researchers that we are
in a renaissance and are actively reshaping our practices for the
better. More importantly, we believe that spotlighting those fields and
researchers that are improving and taking new recommendations into
consideration is one way to encourage future researchers to make strong,
ethical research decisions.

\newpage

\section{References}\label{references}

\setlength{\parindent}{-0.5in} \setlength{\leftskip}{0.5in}

\hypertarget{refs}{}
\hypertarget{ref-Asendorpf2012}{}
Asendorpf, J. B., Conner, M., Fruyt, F. D., Houwer, J. D., Denissen, J.
J. A., Perugini, M., \ldots{} Aken, M. A. G. V. (2012). Recommendations
for increasing replicability in psychology. \emph{European Journal of
Personality}.

\hypertarget{ref-Bakker2014}{}
Bakker, M., \& Wicherts, J. M. (2014). Outlier removal and the relation
with reporting errors and quality of psychological research. \emph{PLoS
ONE}, \emph{9}(7), 1--9.
doi:\href{https://doi.org/10.1371/journal.pone.0103360}{10.1371/journal.pone.0103360}

\hypertarget{ref-Beckman1983}{}
Beckman, R. J., \& Cook, R. D. (1983). {[}Outlier..........s{]}:
Response. \emph{Technometrics}, \emph{25}(2), 161.
doi:\href{https://doi.org/10.2307/1268548}{10.2307/1268548}

\hypertarget{ref-Benjamin2018}{}
Benjamin, D. J., Berger, J. O., Johannesson, M., Nosek, B. A.,
Wagenmakers, E. J., Berk, R., \ldots{} Johnson, V. E. (2018). Redefine
statistical significance. \emph{Nature Human Behaviour}, \emph{2}(1),
6--10.
doi:\href{https://doi.org/10.1038/s41562-017-0189-z}{10.1038/s41562-017-0189-z}

\hypertarget{ref-Bernoulli1777}{}
Bernoulli, D. (1977). ``The most probable choice between several
discrepant observations and the formation therefrom of the most likely
induction'' in C. G. Allen (1961).

\hypertarget{ref-CenterforOpenScience}{}
Center for Open Science. (n.d.). Business Plan.

\hypertarget{ref-Cook1980}{}
Cook, R. D., \& Weisberg, S. (1980). Characterizations of an Empirical
Influence Function for Detecting Influential Cases in Regression.
\emph{Technometrics}, \emph{22}(1), 495--508.

\hypertarget{ref-Cumming2008}{}
Cumming, G. (2008). Replication and p intervals. \emph{Perspectives on
Psychological Science}, \emph{3}(4), 286--300.
doi:\href{https://doi.org/10.1111/j.1745-6924.2008.00079.x}{10.1111/j.1745-6924.2008.00079.x}

\hypertarget{ref-Doyen2012}{}
Doyen, S., Klein, O., Pichon, C. L., \& Cleeremans, A. (2012).
Behavioral priming: It's all in the mind, but whose mind? \emph{PLoS
ONE}, \emph{7}(1).
doi:\href{https://doi.org/10.1371/journal.pone.0029081}{10.1371/journal.pone.0029081}

\hypertarget{ref-Etz2016}{}
Etz, A., \& Vandekerckhove, J. (2016). A Bayesian perspective on the
reproducibility project: Psychology. \emph{PLoS ONE}, \emph{11}(2),
1--12.
doi:\href{https://doi.org/10.1371/journal.pone.0149794}{10.1371/journal.pone.0149794}

\hypertarget{ref-Ferguson2012a}{}
Ferguson, C. J., \& Brannick, M. T. (2012). Publication bias in
psychological science: prevalence, methods for identifying and
controlling, and implications for the use of meta-analyses.
\emph{Psychological Methods}, \emph{17}(1), 120.

\hypertarget{ref-Fiedler2016}{}
Fiedler, K., \& Schwarz, N. (2016). Questionable Research Practices
Revisited. \emph{Social Psychological and Personality Science},
\emph{7}(1), 45--52.
doi:\href{https://doi.org/10.1177/1948550615612150}{10.1177/1948550615612150}

\hypertarget{ref-Gigerenzer2004}{}
Gigerenzer, G. (2004). Mindless statistics. \emph{The Journal of
Socio-Economics}, \emph{33}(5), 587--606.
doi:\href{https://doi.org/10.1016/j.socec.2004.09.033}{10.1016/j.socec.2004.09.033}

\hypertarget{ref-Hodge2004}{}
Hodge, V. J., \& Austin, J. (2004). A survey of outlier detection
methodologies. \emph{Artificial Intelligence Review}, \emph{22}(2),
85--126.
doi:\href{https://doi.org/10.1007/s10462-004-4304-y}{10.1007/s10462-004-4304-y}

\hypertarget{ref-Ioannidis2005}{}
Ioannidis, J. P. A. (2005). Why most published research findings are
false. \emph{PLoS Medicine}, \emph{2}(8), e124.
doi:\href{https://doi.org/10.1371/journal.pmed.0020124}{10.1371/journal.pmed.0020124}

\hypertarget{ref-Klein2014c}{}
Klein, R. A., Ratliff, K. A., Vianello, M., Adams, R. B., Bahník, Š.,
Bernstein, M. J., \ldots{} Nosek, B. A. (2014). Investigating variation
in replicability: A ``many labs'' replication project. \emph{Social
Psychology}, \emph{45}(3), 142--152.
doi:\href{https://doi.org/10.1027/1864-9335/a000178}{10.1027/1864-9335/a000178}

\hypertarget{ref-Lakens2013}{}
Lakens, D. (2013). Calculating and reporting effect sizes to facilitate
cumulative science: A practical primer for t-tests and ANOVAs.
\emph{Front Psychol}, \emph{4}.
doi:\href{https://doi.org/10.3389/fpsyg.2013.00863}{10.3389/fpsyg.2013.00863}

\hypertarget{ref-Lakens2018}{}
Lakens, D., Adolfi, F. G., Albers, C. J., \& al., E. (2018). Justify
your alpha. \emph{Nat Hum Behav}, \emph{2}(3), 68--171.
doi:\href{https://doi.org/10.1038/s41562-018-0311-x}{10.1038/s41562-018-0311-x}

\hypertarget{ref-LeBel2013}{}
LeBel, E. P., Borsboom, D., Giner-Sorolla, R., Hasselman, F., Peters, K.
R., Ratliff, K. A., \& Smith, C. T. (2013). PsychDisclosure.org:
Grassroots Support for Reforming Reporting Standards in Psychology.
\emph{Perspectives on Psychological Science}, \emph{8}(4), 424--432.
doi:\href{https://doi.org/10.1177/1745691613491437}{10.1177/1745691613491437}

\hypertarget{ref-Leggett}{}
Leggett, N. C., Thomas, N. A., Loetscher, T., \& Nicholls, M. E. (n.d.).
The life of p:``Just significant'' results are on the rise. \emph{The
Quarterly Journal of Experimental Psychology}, \emph{66}(12),
2303--2309.

\hypertarget{ref-Lindsay2015}{}
Lindsay, D. S. (2015). Replication in Psychological Science.
\emph{Psychological Science}, \emph{26}(12), 1827--1832.
doi:\href{https://doi.org/10.1177/0956797615616374}{10.1177/0956797615616374}

\hypertarget{ref-Maxwell2015}{}
Maxwell, S. E., Lau, M. Y., \& Howard, G. S. (2015). Is psychology
suffering from a replication crisis? What does ``failure to replicate''
really mean? \emph{American Psychologist}, \emph{70}(6), 487.

\hypertarget{ref-Miguel2014}{}
Miguel, E., Camerer, C., Casey, K., Cohen, J., Esterling, K. M., Gerber,
A., \ldots{} Neumark, D. (2014). Social science. Promoting transparency
in social science research. \emph{Science (New York, N.Y.)},
\emph{343}(6166), 30--1.
doi:\href{https://doi.org/10.1126/science.1245317}{10.1126/science.1245317}

\hypertarget{ref-Munoz-garcia1990}{}
Muñoz-garcia, A. J., Moreno-Rebollo, J. L., \& Pascual-Acosta, A.
(1990). Outliers : A Formal Approach. \emph{International Statistical
Review}, \emph{58}(3), 215--226.

\hypertarget{ref-Nelson2018}{}
Nelson, L. D., Simmons, J., \& Simonsohn, U. (2018). Psychology's
renaissance. \emph{Annual Review of Psychology}, \emph{69}(1), 511--534.
doi:\href{https://doi.org/10.1146/annurev-psych-122216-011836}{10.1146/annurev-psych-122216-011836}

\hypertarget{ref-Nosek2012c}{}
Nosek, B. A., Spies, J. R., \& Motyl, M. (2012). Scientific Utopia: II.
Restructuring Incentives and Practices to Promote Truth Over
Publishability. \emph{Perspectives on Psychological Science},
\emph{7}(6), 615--631.
doi:\href{https://doi.org/10.1177/1745691612459058}{10.1177/1745691612459058}

\hypertarget{ref-ScienceCollaboration2015}{}
Open Science Collaboration. (2015). Estimating the reproducibility of
psychological science. \emph{Science}, \emph{349}, 6251.

\hypertarget{ref-Orr1991}{}
Orr, Sackett, \& Dubois. (1991). Outlier Detection and Treatment in I /
O Psychology : A Survey. \emph{Personnel Psychology}, 473--486.

\hypertarget{ref-Osborne2004}{}
Osborne, J. W., \& Overbay, A. (2004). The power of outliers (and why
researchers should always check for them). \emph{Practical Assessment,
Research \& Evaluation}, \emph{9}(6), 1--12.

\hypertarget{ref-Pashler2012a}{}
Pashler, H., \& Wagenmakers, E. J. (2012). Editors' Introduction to the
Special Section on Replicability in Psychological Science: A Crisis of
Confidence? \emph{Perspectives on Psychological Science}, \emph{7}(6),
528--530.
doi:\href{https://doi.org/10.1177/1745691612465253}{10.1177/1745691612465253}

\hypertarget{ref-Simmons2011}{}
Simmons, J. P., Nelson, L. D., \& Simonsohn, U. (2011). False-positive
psychology: Undisclosed flexibility in data collection and analysis
allows presenting anything as significant. \emph{Psychological Science},
\emph{22}(11), 1359--1366.
doi:\href{https://doi.org/10.1177/0956797611417632}{10.1177/0956797611417632}

\hypertarget{ref-Simonsohn2013}{}
Simonsohn, U. (2013). Just Post It: The Lesson From Two Cases of
Fabricated Data Detected by Statistics Alone. \emph{Psychological
Science}, \emph{24}(10), 1875--1888.
doi:\href{https://doi.org/10.1177/0956797613480366}{10.1177/0956797613480366}

\hypertarget{ref-Stevens1984}{}
Stevens, J. P. (1984). Outliers and influential data points in
regression analysis. \emph{Psychological Bulletin}, \emph{95}(2),
334--344.
doi:\href{https://doi.org/10.1037/0033-2909.95.2.334}{10.1037/0033-2909.95.2.334}

\hypertarget{ref-Tabachnick2012}{}
Tabachnick, B. G., \& Fidell, L. S. (2012). \emph{Using multivariate
statistics} (Sixth.). Boston, MA: Pearson.

\hypertarget{ref-Valentine}{}
Valentine, K., Buchanan, E. M., \& Scofield, J. E. (n.d.). Beyond
p-values: Utilizing Multiple Methods to Evaluate Evidence.
doi:\href{https://doi.org/10.17605/OSF.IO/9HP7Y}{10.17605/OSF.IO/9HP7Y}

\hypertarget{ref-VanElk2015}{}
Van Elk, M., Matzke, D., Gronau, Q. F., Guan, M., Vandekerckhove, J., \&
Wagenmakers, E.-J. (2015). Meta-analyses are no substitute for
registered replications: A skeptical perspective on religious priming.
\emph{Frontiers in Psychology}, \emph{6}, 1365.
doi:\href{https://doi.org/10.3389/fpsyg.2015.01365}{10.3389/fpsyg.2015.01365}

\hypertarget{ref-Wagenmakers2011a}{}
Wagenmakers, E. J., Wetzels, R., Borsboom, D., \& Maas, H. L. van der.
(2011). Why Psychologists Must Change the Way They Analyze Their Data:
The Case of Psi: Comment on Bem (2011). \emph{Journal of Personality and
Social Psychology}, \emph{100}(3), 426--432.
doi:\href{https://doi.org/10.1037/a0022790}{10.1037/a0022790}






\end{document}
